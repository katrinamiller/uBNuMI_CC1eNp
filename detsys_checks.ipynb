{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'BDT_models')\n",
    "sys.path.insert(0, 'backend_functions')\n",
    "\n",
    "import selection_functions as sf\n",
    "\n",
    "import importlib\n",
    "\n",
    "import uproot\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import awkward\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import ROOT\n",
    "from ROOT import TH1F, TDirectory, TH1D\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import top \n",
    "importlib.reload(top)\n",
    "from top import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uncertainty_functions\n",
    "from uncertainty_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ISRUN3: \n",
    "    standard_dict = detvar_run1_fhc\n",
    "    intrinsic_dict = intrinsic_detvar_run1_fhc\n",
    "    \n",
    "    run = 'run1'\n",
    "    detvar = detvar_run1_fhc\n",
    "    \n",
    "else: \n",
    "    standard_dict = detvar_run3_rhc\n",
    "    intrinsic_dict = intrinsic_detvar_run3_rhc\n",
    "    \n",
    "    run = 'run3b'\n",
    "    detvar = detvar_run3_rhc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "variables = [\n",
    "    \"nu_pdg\", \"ccnc\",\n",
    "    \"true_nu_vtx_x\", \"true_nu_vtx_y\" , \"true_nu_vtx_z\", \n",
    "    \"swtrig_pre\", \"npion\", \"nproton\", \"npi0\",\n",
    "    \"reco_nu_vtx_sce_x\",\"reco_nu_vtx_sce_y\",\"reco_nu_vtx_sce_z\",\n",
    "    \"n_tracks_contained\", \n",
    "    \"shr_tkfit_dedx_Y\",  \n",
    "    \"shrsubclusters0\",\"shrsubclusters1\",\"shrsubclusters2\", # number of sub-clusters in shower\n",
    "    \"trkshrhitdist2\",\n",
    "    \"n_showers_contained\",\n",
    "    \"shr_score\", \"tksh_angle\", \n",
    "    \"trk_energy\", \n",
    "    \"tksh_distance\",\n",
    "    \"shr_energy_tot_cali\",  \n",
    "    \"nslice\", \n",
    "    \"contained_fraction\", \n",
    "    \"shrmoliereavg\", \n",
    "    \"ppfx_cv\", \"weightSplineTimesTune\", \"NeutrinoEnergy2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/uboone/data/users/kmiller/uBNuMI_CCNp/ntuples/\"+run+\"/systematics/detvar/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detvar.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = \"nuselection\"\n",
    "tree = \"NeutrinoSelectionFilter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beamon_pot = parameters(ISRUN3)['beamon_pot']\n",
    "beamon_pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xvar = 'shr_tkfit_dedx_Y'\n",
    "#bins = [x*0.5 for x in range(15)]\n",
    "\n",
    "xvar = \"reco_nu_vtx_sce_x\"\n",
    "bins = [0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250]\n",
    "\n",
    "q = BDT_LOOSE_CUTS + ' and BDT_score>'+str(parameters(ISRUN3)['bdt_score_cut'])\n",
    "print(\"BDT score >\", parameters(ISRUN3)['bdt_score_cut'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_model = xgb.Booster({'nthread': 4})\n",
    "bdt_model.load_model(parameters(ISRUN3)['bdt_model'])\n",
    "\n",
    "useBDT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detvar_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variation in detvar.keys(): \n",
    "    \n",
    "    print(\"Plotting\", variation)\n",
    "    \n",
    "    #standard_input_file = input_path + \"standard_overlay/neutrinoselection_filt_run1_overlay_\"+variation+\".root\"\n",
    "    intrinsic_input_file = input_path + \"intrinsic/neutrinoselection_filt_\"+run+\"_overlay_\"+variation+\"_intrinsic.root\"\n",
    "    \n",
    "    #f_standard = uproot.open(standard_input_file)[fold][tree]\n",
    "    f_intrinsic = uproot.open(intrinsic_input_file)[fold][tree]\n",
    "    \n",
    "    uproot_v = [f_intrinsic] #[f_standard, f_intrinsic]\n",
    "    \n",
    "    #df_standard = f_standard.pandas.df(variables, flatten=False)\n",
    "    df_intrinsic = f_intrinsic.pandas.df(variables, flatten=False)\n",
    "    \n",
    "    for i, df in enumerate([df_intrinsic]): #([df_standard, df_intrinsic]):\n",
    "            \n",
    "        up = uproot_v[i]\n",
    "        trk_llr_pid_v = up.array('trk_llr_pid_score_v')\n",
    "        trk_id = up.array('trk_id')-1 \n",
    "        trk_llr_pid_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_llr_pid_v,trk_id)])\n",
    "        df['trkpid'] = trk_llr_pid_v_sel\n",
    "        df['subcluster'] = df['shrsubclusters0'] + df['shrsubclusters1'] + df['shrsubclusters2']\n",
    "        \n",
    "        df['NeutrinoEnergy2_GeV'] = df['NeutrinoEnergy2']/1000\n",
    "\n",
    "        df.loc[ df['weightSplineTimesTune'] <= 0, 'weightSplineTimesTune' ] = 1.\n",
    "        df.loc[ df['weightSplineTimesTune'] == np.inf, 'weightSplineTimesTune' ] = 1.\n",
    "        df.loc[ df['weightSplineTimesTune'] > 100, 'weightSplineTimesTune' ] = 1.\n",
    "        df.loc[ np.isnan(df['weightSplineTimesTune']) == True, 'weightSplineTimesTune' ] = 1.\n",
    "            \n",
    "        # bool for is signal vs is not signal \n",
    "        df['is_signal'] = np.where((df.swtrig_pre == 1) \n",
    "                            & (df.nu_pdg==12) & (df.ccnc==0) & (df.nproton>0) & (df.npion==0) & (df.npi0==0)\n",
    "                            & (10 <= df.true_nu_vtx_x) & (df.true_nu_vtx_x <= 246)\n",
    "                            & (-106 <= df.true_nu_vtx_y) & (df.true_nu_vtx_y <= 106)\n",
    "                            & (10 <= df.true_nu_vtx_z) & (df.true_nu_vtx_z <= 1026), True, False)\n",
    "            \n",
    "        print('is_signal check:', len(df) == len(df.query('is_signal==True')) + len(df.query('is_signal==False')))\n",
    "        \n",
    "    # APPLY SW TRIGGER\n",
    "    #df_standard = df_standard.query('swtrig_pre==1')\n",
    "    df_intrinsic = df_intrinsic.query('swtrig_pre==1')\n",
    "    \n",
    "    # SPLIT DATAFRAME \n",
    "    n_rows = len(df_intrinsic) \n",
    "    df1 = df_intrinsic.iloc[:round(n_rows/3),:]\n",
    "    df2 = df_intrinsic.iloc[round(n_rows/3):2*round(n_rows/3),:]\n",
    "    df3 = df_intrinsic.iloc[2*round(n_rows/3):,:]\n",
    "    \n",
    "    print('split check:', len(df_intrinsic) == len(df1) + len(df2) + len(df3))\n",
    "    \n",
    "    # POT SCALING\n",
    "    #df_standard['pot_scale'] = beamon_pot/standard_dict.get(variation)    \n",
    "    df_intrinsic['pot_scale'] = beamon_pot/intrinsic_dict.get(variation+'_intrinsic')\n",
    "    \n",
    "    #df1['pot_scale'] = beamon_pot/((1/3)*intrinsic_dict.get(variation+'_intrinsic'))\n",
    "    #df2['pot_scale'] = beamon_pot/((1/3)*intrinsic_dict.get(variation+'_intrinsic'))\n",
    "    #df3['pot_scale'] = beamon_pot/((1/3)*intrinsic_dict.get(variation+'_intrinsic'))\n",
    "    \n",
    "    # remove nue CC events \n",
    "    #print(\"# nueCC in AV in standard overlay det. sys. sample = \"+str(len(df_standard.query(nueCC_query))))\n",
    "    #len1 = len(df_standard)\n",
    "\n",
    "    #idx = df_standard.query(nueCC_query).index\n",
    "    #df_standard.drop(idx, inplace=True)\n",
    "    #len2 = len(df_standard) \n",
    "\n",
    "    #print(\"# of nueCC in AV removed = \"+str(len1-len2)) # should be same as above\n",
    "    \n",
    "    if useBDT: \n",
    "            \n",
    "        print('Using BDT')\n",
    "            \n",
    "        #df_bdt_standard = df_standard.copy()\n",
    "        df_bdt_intrinsic = df_intrinsic.copy()\n",
    "\n",
    "        # clean datasets \n",
    "        for column in training_parameters:\n",
    "            #df_bdt_standard.loc[(df_bdt_standard[column] < -1.0e37) | (df_bdt_standard[column] > 1.0e37), column] = np.nan\n",
    "            df_bdt_intrinsic.loc[(df_bdt_intrinsic[column] < -1.0e37) | (df_bdt_intrinsic[column] > 1.0e37), column] = np.nan\n",
    "    \n",
    "        # apply BDT model  \n",
    "        #df_test_standard = xgb.DMatrix(data=df_bdt_standard[training_parameters])\n",
    "        #preds_standard = bdt_model.predict(df_test_standard)\n",
    "        #df_bdt_standard['BDT_score'] = preds_standard\n",
    "        \n",
    "        df_test_intrinsic = xgb.DMatrix(data=df_bdt_intrinsic[training_parameters])\n",
    "        preds_intrinsic = bdt_model.predict(df_test_intrinsic)\n",
    "        df_bdt_intrinsic['BDT_score'] = preds_intrinsic\n",
    "            \n",
    "        #df_standard_sel = df_bdt_standard.query(q).copy()\n",
    "        df_intrinsic_sel = df_bdt_intrinsic.query(q).copy()\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        #df_standard_sel = df_standard.query(q).copy()\n",
    "        df_intrinsic_sel = df_intrinsic.query(q).copy()\n",
    "        #df1_sel = df1.query(q).copy()\n",
    "        #df2_sel = df2.query(q).copy()\n",
    "        #df3_sel = df3.query(q).copy()\n",
    "\n",
    "    \n",
    "    #standard_counts = plt.hist(df_standard_sel[xvar], bins, range=[bins[0], bins[-1]], \n",
    "    #        weights=df_standard_sel['ppfx_cv']*df_standard_sel['weightSplineTimesTune']*df_standard_sel['pot_scale'])[0]\n",
    "    #plt.close()\n",
    "    \n",
    "    #print(\"non-nueCC events = \", sum(standard_counts))\n",
    "    \n",
    "    intrinsic_counts = plt.hist(df_intrinsic_sel[xvar], bins, range=[bins[0], bins[-1]], \n",
    "            weights=df_intrinsic_sel['ppfx_cv']*df_intrinsic_sel['weightSplineTimesTune'])[0] #*df_intrinsic_sel['pot_scale'])[0]\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    print(\"nue CC events = \", sum(intrinsic_counts))\n",
    "\n",
    "    \n",
    "    # store counts in a dictionary \n",
    "    #detvar_dict[variation] = standard_counts\n",
    "    detvar_dict[variation+'_intrinsic'] = list(intrinsic_counts)\n",
    "    #detvar_dict[variation+'_total'] = [a+b for a,b in zip(standard_counts, intrinsic_counts)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_err = [ round(abs(bins[x+1]-bins[x])/2, 3) for x in range(len(bins)-1) ]\n",
    "bin_centers = []\n",
    "for a in range(len(bins)-1): \n",
    "    bin_centers.append(round(bins[a] + (bins[a+1]-bins[a])/2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt CV \n",
    "\n",
    "plt.errorbar(bin_centers, detvar_dict['CV_intrinsic'], xerr=x_err, fmt='none', color='black', linewidth=2, label='CV')\n",
    "plt.xlabel(xvar, fontsize=14)\n",
    "plt.ylabel('$\\\\nu$ / '+str(beamon_pot)+' POT', fontsize=14)\n",
    "plt.title(variation+' (nue CC events)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "detvar_dict['CV_intrinsic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intrinsic sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variation in detvar: \n",
    "    \n",
    "    if variation=='CV': \n",
    "        continue\n",
    "    \n",
    "    plt.hist(bin_centers, bins, histtype='step', \n",
    "                        range=[bins[0], bins[-1]], weights=detvar_dict[variation+'_intrinsic'], \n",
    "                        color='cornflowerblue', linewidth=0.5, label='UV')\n",
    "    \n",
    "    plt.errorbar(bin_centers, detvar_dict['CV_intrinsic'], xerr=x_err, fmt='none', color='black', linewidth=2, label='CV')\n",
    "    plt.xlabel(xvar, fontsize=14)\n",
    "    plt.ylabel('$\\\\nu$ / '+str(beamon_pot)+' POT', fontsize=14)\n",
    "    plt.title(variation+' (nue CC events)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detsys_cov_nueCC = {}\n",
    "\n",
    "\n",
    "\n",
    "for variation in detvar.keys(): \n",
    "    \n",
    "    if variation=='CV': \n",
    "        continue\n",
    "   \n",
    "    detsys_cov_nueCC[variation+\"_to_nueCC\"] = calcCov(xvar, bins, detvar_dict['CV_intrinsic'], \n",
    "                                              detvar_dict['CV_intrinsic'], \n",
    "                                              [detvar_dict[variation+'_intrinsic']])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTAL cov, corr, fractional uncertainty -- nue CC \n",
    "\n",
    "\n",
    "cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "cor = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for i in range(len(bins)-1): \n",
    "        for j in range(len(bins)-1):\n",
    "            \n",
    "            cov[i][j] = sum([detsys_cov_nueCC[x+\"_to_nueCC\"]['cov'][i][j] for x in detvar.keys() if x is not 'CV'])\n",
    "            frac_cov[i][j] = sum([detsys_cov_nueCC[x+\"_to_nueCC\"]['frac_cov'][i][j] for x in detvar.keys() if x is not 'CV'])\n",
    "\n",
    "for i in range(len(bins)-1): \n",
    "    for j in range(len(bins)-1):\n",
    "        \n",
    "        if np.sqrt(cov[i][i])*np.sqrt(cov[j][j]) != 0: \n",
    "                cor[i][j] = cov[i][j] / (np.sqrt(cov[i][i])*np.sqrt(cov[j][j]))\n",
    "            \n",
    "nueCC_detsys_dict = {\n",
    "    'cov' : cov, \n",
    "    'frac_cov' : frac_cov,\n",
    "    'cor' : cor,\n",
    "    'fractional_uncertainty' : np.sqrt(np.diag(frac_cov))\n",
    "} \n",
    "\n",
    "nueCC_detsys_dict['fractional_uncertainty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "        \n",
    "plt.pcolor(bins, bins, nueCC_detsys_dict['cor'], cmap='OrRd', edgecolors='k', vmin=-1, vmax=1)\n",
    "            \n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "        \n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "\n",
    "plt.xlabel(xvar, fontsize=15)\n",
    "plt.ylabel(xvar, fontsize=15)\n",
    "\n",
    "plt.title('Correlation (nueCC)', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "        \n",
    "plt.pcolor(bins, bins, nueCC_detsys_dict['frac_cov'], cmap='OrRd', edgecolors='k', vmin=-0.1, vmax=0.1)\n",
    "            \n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "        \n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "\n",
    "plt.xlabel(xvar, fontsize=15)\n",
    "plt.ylabel(xvar, fontsize=15)\n",
    "\n",
    "plt.title('Fractional Covariance (nueCC)', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(detvar_dict['CV_intrinsic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bincenters = 0.5*(np.array(bins)[1:]+np.array(bins)[:-1])\n",
    "x_err = [ round(abs(bins[x+1]-bins[x])/2, 3) for x in range(len(bins)-1) ]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "cv_bdtcut = detvar_dict['CV_intrinsic']\n",
    "cv_bdtcut_err = [np.sqrt(x)/x for x in cv_bdtcut]\n",
    "\n",
    "plt.errorbar(bincenters, nueCC_detsys_dict['fractional_uncertainty'], fmt='none', \n",
    "             yerr=cv_bdtcut_err, color='blue', alpha=0.7)\n",
    "plt.hist(bincenters, bins, weights=nueCC_detsys_dict['fractional_uncertainty'], histtype='step', \n",
    "         label='After BDT Cut (from 8767.0 GENIE/PPFX-tuned CV evts)', color='blue',alpha=0.7)\n",
    "\n",
    "plt.hlines(np.average(nueCC_detsys_dict['fractional_uncertainty']), 0, 250, linestyle='--', \n",
    "           label='Avg = '+str(round(np.average(nueCC_detsys_dict['fractional_uncertainty']), 3)))\n",
    "\n",
    "plt.xlim(0, 250)\n",
    "plt.ylim(0, 0.33)\n",
    "plt.legend(fontsize=13)\n",
    "\n",
    "plt.xlabel(\"Reconstructed X Position [cm]\", fontsize=14)\n",
    "plt.ylabel(\"Fractional Uncertainty\", fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.title(\"FHC Run 1 Det. Sys. ($\\\\nu_{e}$ CC Intrinsic Sample)\", fontsize=15)\n",
    "#plt.savefig(parameters(ISRUN3)['plots_path']+\"RHCRUN3DetSys.pdf\", transparent=True, bbox_inches='tight') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_variations = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_variations: \n",
    "    \n",
    "    import json\n",
    "    \n",
    "    with open('variations/FHCDetSysVariations_July8.json', 'w') as f:\n",
    "        json.dump(detvar_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standard sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variation in detvar: \n",
    "    \n",
    "    if variation=='CV': \n",
    "        continue\n",
    "    \n",
    "    plt.hist(bin_centers, bins, histtype='step', \n",
    "                        range=[bins[0], bins[-1]], weights=detvar_dict[variation], \n",
    "                        color='cornflowerblue', linewidth=0.5, label='UV')\n",
    "    \n",
    "    plt.errorbar(bin_centers, detvar_dict['CV'], \n",
    "                 xerr=x_err, fmt='none', color='black', linewidth=2, label='CV')\n",
    "    plt.xlabel(xvar, fontsize=14)\n",
    "    plt.ylabel('$\\\\nu$ / '+str(beamon_pot)+' POT', fontsize=14)\n",
    "    plt.title(variation+' (non-nue CC events)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detsys_cov_non_nueCC = {}\n",
    "\n",
    "\n",
    "for variation in detvar.keys(): \n",
    "    \n",
    "    if variation=='CV': \n",
    "        continue\n",
    "    \n",
    "    # calc covariance for each unisim - nonnueCC \n",
    "    detsys_cov_non_nueCC[variation+\"_to_nonnueCC\"] = calcCov(xvar, bins, detvar_dict['CV'], \n",
    "                                              detvar_dict['CV'], # taken from uncertainty.ipynb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  cov, corr, fractional uncertainty -- non nue CC\n",
    "\n",
    "\n",
    "cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "cor = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for i in range(len(bins)-1): \n",
    "        for j in range(len(bins)-1):\n",
    "            \n",
    "            cov[i][j] = sum([detsys_cov_non_nueCC[x+\"_tofull\"]['cov'][i][j] for x in detvar.keys() if x is not 'CV'])\n",
    "            frac_cov[i][j] = sum([detsys_cov_non_nueCC[x+\"_tofull\"]['frac_cov'][i][j] for x in detvar.keys() if x is not 'CV'])\n",
    "\n",
    "for i in range(len(bins)-1): \n",
    "    for j in range(len(bins)-1):\n",
    "        \n",
    "        if np.sqrt(cov[i][i])*np.sqrt(cov[j][j]) != 0: \n",
    "                cor[i][j] = cov[i][j] / (np.sqrt(cov[i][i])*np.sqrt(cov[j][j]))\n",
    "            \n",
    "non_nueCC_detsys_dict = {\n",
    "    'cov' : cov, \n",
    "    'frac_cov' : frac_cov,\n",
    "    'cor' : cor,\n",
    "    'fractional_uncertainty' : np.sqrt(np.diag(frac_cov))\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nueCC_detsys_dict['fractional_uncertainty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "        \n",
    "plt.pcolor(bins, bins, non_nueCC_detsys_dict['cor'], cmap='OrRd', edgecolors='k', vmin=-1, vmax=1)\n",
    "            \n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "        \n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "\n",
    "plt.xlabel(xvar, fontsize=15)\n",
    "plt.ylabel(xvar, fontsize=15)\n",
    "\n",
    "plt.title('Correlation (non-nueCC)', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "        \n",
    "plt.pcolor(bins, bins, non_nueCC_detsys_dict['frac_cov'], cmap='OrRd', edgecolors='k', vmin=-0.1, vmax=0.1)\n",
    "            \n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "        \n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "\n",
    "plt.xlabel(xvar, fontsize=15)\n",
    "plt.ylabel(xvar, fontsize=15)\n",
    "\n",
    "plt.title('Fractional Covariance (non-nueCC)', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparisons between intrinsic vs. standard overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bin_centers, bins, weights=nueCC_detsys_dict['fractional_uncertainty'], histtype='step', label='nueCC')\n",
    "plt.hist(bin_centers, bins, weights=non_nueCC_detsys_dict['fractional_uncertainty'], histtype='step', label='non-nueCC')\n",
    "plt.hist(bin_centers, bins, weights=tot_frac, histtype='step', label='total', color='black', linestyle='--')\n",
    "\n",
    "#plt.hist(bin_centers, bins, weights=tot_detsys_dict['fractional_uncertainty'], histtype='step', label='together', color='red')\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(xvar, fontsize=14)\n",
    "plt.ylim(0, 1.2)\n",
    "#plt.xlim(0, 7)\n",
    "plt.ylabel('Fractional Uncertainty', fontsize=14)\n",
    "plt.title(\"Detector Uncertainties\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(nueCC_detsys_dict['fractional_uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bin_centers, bins, weights=nueCC_detsys_dict['fractional_uncertainty'], histtype='step', label='nueCC')\n",
    "plt.hlines(np.average(nueCC_detsys_dict['fractional_uncertainty']), 0, 250, linestyle='--', \n",
    "           label='Average = '+str(round(np.average(nueCC_detsys_dict['fractional_uncertainty']), 3)))\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(xvar, fontsize=14)\n",
    "plt.ylim(0, 0.65)\n",
    "plt.xlim(0, bins[-1])\n",
    "plt.ylabel('Fractional Uncertainty', fontsize=14)\n",
    "plt.title(\"Detector Uncertainties\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(nueCC_detsys_dict['fractional_uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTDATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = overlay.query(BDT_PRE_QUERY).copy()\n",
    "df_loose = overlay.query(BDT_LOOSE_CUTS).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_pre = list(df_pre['ppfx_cv']*df_pre['weightSplineTimesTune']*df_pre['pot_scale'])\n",
    "w_loose = list(df_loose['ppfx_cv']*df_loose['weightSplineTimesTune']*df_loose['pot_scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvar = 'shr_tkfit_dedx_Y'\n",
    "xlow = 0\n",
    "xhigh = 7\n",
    "bins = [x*0.5 for x in range(15)]\n",
    "x_label = \"dE/dx on the Y plane [MeV/cm]\"\n",
    "\n",
    "\n",
    "plt.hist(df_pre[xvar], bins=bins, range=[bins[0], bins[-1]], weights=w_pre, \n",
    "         histtype='step', label=\"pre\")\n",
    "plt.hist(df_loose[xvar], bins=bins, range=[bins[0], bins[-1]], weights=w_loose, \n",
    "         histtype='step', label='loose')\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(xvar, fontsize=14)\n",
    "plt.title('LYRayleigh', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beamon_pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bincenters = 0.5*(np.array(bins)[1:]+np.array(bins)[:-1])\n",
    "x_err = [ round(abs(bins[x+1]-bins[x])/2, 3) for x in range(len(bins)-1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evolution -- total \n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "presel = np.array([0.08919758, 0.03710655, 0.07212393, 0.0740782 , 0.05647801, 0.06390213, 0.03153526, 0.09894273, 0.06314468, 0.08524242])\n",
    "loosecuts = np.array([0.15597447, 0.13987804, 0.07696078, 0.07240133, 0.10087724, 0.10312506, 0.13383267, 0.05475324, 0.10502673, 0.17370385])\n",
    "bdt_cut = np.array([0.16095705, 0.29032498, 0.23506311, 0.15685235, 0.33354353, 0.15485916, 0.2284225 , 0.12265416, 0.08450959, 0.21393108])\n",
    "\n",
    "\n",
    "plt.hist(bincenters, weights=presel, histtype='step', label='After preselection')\n",
    "plt.hist(bincenters, weights=loosecuts, histtype='step', label='After loose cuts')\n",
    "plt.hist(bincenters, weights=bdt_cut, histtype='step', label='After BDT cut')\n",
    "\n",
    "plt.ylim(0, 0.5)\n",
    "plt.legend(fontsize=13)\n",
    "\n",
    "plt.xlabel(\"Reco X [cm]\", fontsize=14)\n",
    "plt.ylabel(\"Fractional Uncertainty\", fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.title(\"DetSys Uncertainty Evolution (Total)\", fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bincenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evolution -- nue CC\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "#######################################\n",
    "\n",
    "presel = np.array([0.06870054, 0.02259937, 0.0497535 , 0.02650766, 0.05251311, 0.01690655, 0.06395052, 0.06882898, 0.07349036, 0.11812196])\n",
    "cv_presel = np.array([1950.3359375 , 3777.09765625, 3911.71777344, 3808.6484375 , 3780.08691406, 3747.08398438, 3643.8046875 , 3765.2890625 , 3746.96875 , 1781.91210938])\n",
    "cv_presel_err = [np.sqrt(x)/x for x in cv_presel]\n",
    "\n",
    "plt.errorbar(bincenters, presel, fmt='none', yerr=cv_presel_err, color='cornflowerblue')\n",
    "plt.hist(bincenters, bins, weights=presel,  histtype='step', color='cornflowerblue', \n",
    "         label='After preselection (33913.0 CV evts, Avg='+str(round(np.average(presel), 3))+')')\n",
    "#plt.hlines(np.average(presel), 0, 250, linestyle='--', label='Average = '+str(round(np.average(presel), 3)))\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "#plt.figure(figsize=(8, 5))\n",
    "\n",
    "loosecuts = np.array([0.0749319 , 0.10707654, 0.05782089, 0.04730627, 0.05607244, 0.06158666, 0.09371712, 0.08221844, 0.04345454, 0.08520043])\n",
    "cv_loosecuts = np.array([ 997.53582764, 1834.1015625 , 1922.6262207 , 1901.54052734, 1795.18994141, 1851.43164062, 1814.76269531, 1974.08789062, 2011.9609375 , 983.98144531])\n",
    "cv_loosecuts_err = [np.sqrt(x)/x for x in cv_loosecuts]\n",
    "\n",
    "plt.errorbar(bincenters, loosecuts, fmt='none', yerr=cv_loosecuts_err, color='darkorange')\n",
    "plt.hist(bincenters, bins, weights=loosecuts,  histtype='step', color='darkorange', \n",
    "         label='After loose cuts (17087.2 CV evts, Avg='+str(round(np.average(loosecuts), 3))+')')\n",
    "#plt.hlines(np.average(loosecuts), 0, 250, linestyle='--', label='Average = '+str(round(np.average(loosecuts), 3)))\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "#plt.figure(figsize=(8, 5))\n",
    "\n",
    "bdt_cut = np.array([0.11615794, 0.09695565, 0.08512074, 0.10490526, 0.16026189, 0.10672332, 0.11354116, 0.14747448, 0.05915643, 0.10043502])\n",
    "cv_bdtcut = [552.42785645, 1081.7479248, 1195.08251953, 1152.29296875, 1061.40185547, 1154.8359375, 1089.49169922, 1122.58740234, 1207.57714844, 472.75 ]\n",
    "cv_bdtcut_err = [np.sqrt(x)/x for x in cv_bdtcut]\n",
    "\n",
    "plt.errorbar(bincenters, bdt_cut, fmt='none', yerr=cv_bdtcut_err, color='forestgreen')\n",
    "plt.hist(bincenters, bins, weights=bdt_cut, histtype='step', \n",
    "         label='After BDT cut (10090.2 CV evts, Avg='+str(round(np.average(bdt_cut), 3))+')', color='forestgreen')\n",
    "#plt.hlines(np.average(bdt_cut), 0, 250, linestyle='--', label='Average = '+str(round(np.average(bdt_cut), 3)))\n",
    "\n",
    "plt.xlim(0, 250)\n",
    "plt.ylim(0, 0.3)\n",
    "plt.legend(fontsize=13)\n",
    "\n",
    "plt.xlabel(\"Reco X [cm]\", fontsize=14)\n",
    "plt.ylabel(\"Fractional Uncertainty\", fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.title(\"DetSys Uncertainty Evolution (nueCC WRT sample)\", fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## half the samples\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "presel = np.array([0.06870054, 0.02259937, 0.0497535 , 0.02650766, 0.05251311, 0.01690655, 0.06395052, 0.06882898, 0.07349036, 0.11812196])\n",
    "cv_presel = np.array([1950.3359375 , 3777.09765625, 3911.71777344, 3808.6484375 , 3780.08691406, 3747.08398438, 3643.8046875 , 3765.2890625 , 3746.96875 , 1781.91210938])\n",
    "cv_presel_err = [np.sqrt(x)/x for x in cv_presel]\n",
    "\n",
    "plt.errorbar(bincenters, presel, fmt='none', yerr=cv_presel_err, color='cornflowerblue',linewidth=2)\n",
    "plt.hist(bincenters, bins, weights=presel,  histtype='step', color='cornflowerblue', \n",
    "         label='Full (33913.0 CV evts, Avg='+str(round(np.average(presel), 3))+')', linewidth=2)\n",
    "\n",
    "\n",
    "presel_1 = np.array([0.08276135, 0.13338453, 0.08900892, 0.14611306, 0.12927223, 0.07762727, 0.09594148, 0.05464803, 0.12272057, 0.12233051])\n",
    "cv_presel_1 = np.array([ 986.69923139, 1991.25876022, 1979.06098178, 1850.39915699, 1959.67234071, 1883.90602839, 1812.72918274, 1870.95998444, 1902.79260546, 901.90942413])\n",
    "cv_presel_1_err = [np.sqrt(x)/x for x in cv_presel_1]\n",
    "\n",
    "plt.errorbar(bincenters, presel_1, fmt='none', yerr=cv_presel_1_err, color='red')\n",
    "plt.hist(bincenters, bins, weights=presel_1,  histtype='step', \n",
    "         label='Half 1 (17139.4 CV evts, Avg='+str(round(np.average(presel_1), 3))+')', color='red')\n",
    "\n",
    "\n",
    "presel_2 = np.array([0.13550844, 0.15860746, 0.0508528 , 0.11228756, 0.07343269, 0.0638385 , 0.09993951, 0.13137824, 0.08834187, 0.19772075])\n",
    "cv_presel_2 = np.array([ 963.6359203 , 1785.84381734, 1932.65090755, 1958.23858006, 1820.39336017, 1863.19111967, 1831.11549528, 1894.37794724, 1844.161984 , 880.00029449])\n",
    "cv_presel_2_err = [np.sqrt(x)/x for x in cv_presel_2]\n",
    "\n",
    "plt.errorbar(bincenters, presel_2, fmt='none', yerr=cv_presel_2_err, color='goldenrod')\n",
    "plt.hist(bincenters, bins, weights=presel_2,  histtype='step', \n",
    "         label='Half 2 (16773.6 CV evts, Avg='+str(round(np.average(presel_2), 3))+')', color='goldenrod')\n",
    "\n",
    "\n",
    "plt.xlim(0, 250)\n",
    "plt.ylim(0, 0.3)\n",
    "plt.legend(fontsize=13, loc='upper left')\n",
    "\n",
    "plt.xlabel(\"Reco X [cm]\", fontsize=14)\n",
    "plt.ylabel(\"Fractional Uncertainty\", fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.title(\"DetSys Uncertainty After Preselection (nueCC WRT sample)\", fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/3 of the samples \n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "loosecuts = np.array([0.0749319 , 0.10707654, 0.05782089, 0.04730627, 0.05607244, 0.06158666, 0.09371712, 0.08221844, 0.04345454, 0.08520043])\n",
    "cv_loosecuts = np.array([ 997.53582764, 1834.1015625 , 1922.6262207 , 1901.54052734, 1795.18994141, 1851.43164062, 1814.76269531, 1974.08789062, 2011.9609375 , 983.98144531])\n",
    "cv_loosecuts_err = [np.sqrt(x)/x for x in cv_loosecuts]\n",
    "\n",
    "plt.errorbar(bincenters, loosecuts, fmt='none', yerr=cv_loosecuts_err, color='darkorange', linewidth=2)\n",
    "plt.hist(bincenters, bins, weights=loosecuts,  histtype='step', color='darkorange', \n",
    "         label='Full (17087.2 CV evts, Avg='+str(round(np.average(loosecuts), 3))+')', linewidth=2)\n",
    "\n",
    "#######################################\n",
    "\n",
    "loosecuts_1 = np.array([0.21443809, 0.11683203, 0.25497575, 0.22126091, 0.10338683, 0.1914447 , 0.29447061, 0.10896304, 0.11700523, 0.17420665])\n",
    "cv_loosecuts_1 = np.array([326.09517148, 621.23741241, 613.16593185, 612.67984031, 625.49892382, 633.92312074, 576.34828676, 661.85773279, 675.02504981, 330.61486563])\n",
    "cv_loosecuts_1_err = [np.sqrt(x)/x for x in cv_loosecuts_1]\n",
    "\n",
    "plt.errorbar(bincenters, loosecuts_1, fmt='none', yerr=cv_loosecuts_1_err, color='yellowgreen')\n",
    "plt.hist(bincenters, bins, weights=loosecuts_1,  histtype='step', color='yellowgreen', \n",
    "         label='Third 1 (5676.4 CV evts, Avg='+str(round(np.average(loosecuts_1), 3))+')')\n",
    "\n",
    "#######################################\n",
    "\n",
    "loosecuts_2 = np.array([0.18178328, 0.18413507, 0.12833944, 0.10598469, 0.21716196, 0.11239651, 0.18992073, 0.20723299, 0.31169509, 0.30546998])\n",
    "cv_loosecuts_2 = np.array([330.31648308, 622.28601108, 643.09801325, 621.57186267, 573.27452433, 616.69670582, 597.5434631 , 626.24670479, 616.45876978, 290.32520257])\n",
    "cv_loosecuts_2_err = [np.sqrt(x)/x for x in cv_loosecuts_2]\n",
    "\n",
    "plt.errorbar(bincenters, loosecuts_2, fmt='none', yerr=cv_loosecuts_2_err, color='plum')\n",
    "plt.hist(bincenters, bins, weights=loosecuts_2,  histtype='step', color='plum', \n",
    "         label='Third 2 (5537.8 CV evts, Avg='+str(round(np.average(loosecuts_2), 3))+')')\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "loosecuts_3 = np.array([0.19695213, 0.19679284, 0.1427169 , 0.17903722, 0.10891259, 0.17262879, 0.16445115, 0.26874798, 0.20139965, 0.35580272])\n",
    "cv_loosecuts_3 = np.array([341.12370765, 590.57644886, 666.3615749 , 667.28486967, 596.41434205, 600.79566863, 640.87313415, 685.9799037 , 720.47157948, 363.04109147])\n",
    "cv_loosecuts_3_err = [np.sqrt(x)/x for x in cv_loosecuts_3]\n",
    "\n",
    "plt.errorbar(bincenters, loosecuts_3, fmt='none', yerr=cv_loosecuts_3_err, color='peru')\n",
    "plt.hist(bincenters, bins, weights=loosecuts_3,  histtype='step', color='peru', \n",
    "         label='Third 3 (5872.9 CV evts, Avg='+str(round(np.average(loosecuts_3), 3))+')')\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(0, 250)\n",
    "plt.ylim(0, 0.5)\n",
    "plt.legend(fontsize=13, loc='upper left')\n",
    "\n",
    "plt.xlabel(\"Reco X [cm]\", fontsize=14)\n",
    "plt.ylabel(\"Fractional Uncertainty\", fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.title(\"DetSys Uncertainty After Loose Cuts (nueCC WRT sample)\", fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(997.53582764)/997.53582764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.sqrt(3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evolution -- non nue CC\n",
    "\n",
    "###########################################\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "presel = np.array([0.18320706, 0.07047145, 0.15389311, 0.13855863, 0.10447263, 0.13108198, 0.07348303, 0.16887075, 0.11046296, 0.17272322])\n",
    "cv_presel = [170.870224, 320.18408203, 321.68411255, 344.71813965, 329.53295898, 337.1027832, 332.45605469, 359.81982422, 308.86206055, 168.31860352]\n",
    "cv_presel_err = [np.sqrt(x)/x for x in cv_presel]\n",
    "\n",
    "plt.errorbar(bincenters, presel, fmt='none', yerr=cv_presel_err, color='cornflowerblue')\n",
    "plt.hist(bincenters, bins, weights=presel, histtype='step', label='After preselection (2993.5 CV evts, Avg=0.13)', color='cornflowerblue')\n",
    "#plt.hlines(np.average(presel), 0, 250, linestyle='--', label='Average = '+str(round(np.average(presel), 3)))\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "loosecuts = np.array([0.36642883, 0.40933568, 0.30770769, 0.2274781 , 0.27710819, 0.45294262, 0.37625798, 0.18602785, 0.3058598 , 0.36586612])\n",
    "cv_loosecuts = np.array([33.15947342, 37.07825851, 35.81521606, 41.98361206, 35.39056396, 29.81181335, 50.02552795, 45.67678833, 43.27896118, 37.53433228])\n",
    "cv_loosecuts_err = [np.sqrt(x)/x for x in cv_loosecuts]\n",
    "\n",
    "plt.errorbar(bincenters, loosecuts, fmt='none', yerr=cv_loosecuts_err, color='darkorange')\n",
    "plt.hist(bincenters, bins, weights=loosecuts, histtype='step', label='After loose cuts (389.8 CV evts, Avg=0.33)', color='darkorange')\n",
    "#plt.hlines(np.average(loosecuts), 0, 250, linestyle='--', label='Average = '+str(round(np.average(loosecuts), 3)))\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "bdt_cut = np.array([1.08089404, 1.41431277, 1.16890459, 0.53734153, 1.73935168, 0.82604116, 1.87221604, 0.4192925 , 0.61276501, 0. ])\n",
    "cv_bdtcut = [2.75313497, 7.39452553, 6.95109463, 9.85165405, 4.49909401, 4.84739876, 3.12324142, 6.37028122, 4.52698135, 0. ]\n",
    "cv_bdtcut_err = [np.sqrt(x)/x for x in cv_bdtcut]\n",
    "\n",
    "plt.errorbar(bincenters, bdt_cut, fmt='none', yerr=cv_bdtcut_err, color='forestgreen')\n",
    "plt.hist(bincenters, bins, weights=bdt_cut, histtype='step', label='After BDT cut (50.3 CV evts, Avg=0.97)', color='forestgreen')\n",
    "#plt.hlines(np.average(bdt_cut), 0, 250, linestyle='--', label='Average = '+str(round(np.average(bdt_cut), 3)))\n",
    "\n",
    "\n",
    "plt.legend(fontsize=13)\n",
    "\n",
    "plt.xlim(0, 250)\n",
    "plt.ylim(0, 3)\n",
    "plt.xlabel(\"Reco X [cm]\", fontsize=14)\n",
    "plt.ylabel(\"Fractional Uncertainty\", fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.title(\"DetSys Uncertainty Evolution (non-nueCC WRT sample)\", fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nue CC vs non nue CC\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "presel = np.array([0.18320706, 0.07047145, 0.15389311, 0.13855863, 0.10447263, 0.13108198, 0.07348303, 0.16887075, 0.11046296, 0.17272322])\n",
    "cv_presel = [170.870224, 320.18408203, 321.68411255, 344.71813965, 329.53295898, 337.1027832, 332.45605469, 359.81982422, 308.86206055, 168.31860352]\n",
    "cv_presel_err = [np.sqrt(x)/x for x in cv_presel]\n",
    "\n",
    "plt.errorbar(bincenters, presel, fmt='none', yerr=cv_presel_err, color='cornflowerblue')\n",
    "plt.hist(bincenters, bins, weights=presel, histtype='step', label='non-nueCC after preselection (2993.5 CV evts, Avg=0.13)', color='cornflowerblue')\n",
    "#plt.hlines(np.average(presel), 0, 250, linestyle='--', label='Average = '+str(round(np.average(presel), 3)))\n",
    "\n",
    "\n",
    "presel = np.array([0.06870054, 0.02259937, 0.0497535 , 0.02650766, 0.05251311, 0.01690655, 0.06395052, 0.06882898, 0.07349036, 0.11812196])\n",
    "cv_presel = np.array([1950.3359375 , 3777.09765625, 3911.71777344, 3808.6484375 , 3780.08691406, 3747.08398438, 3643.8046875 , 3765.2890625 , 3746.96875 , 1781.91210938])\n",
    "cv_presel_err = [np.sqrt(x)/x for x in cv_presel]\n",
    "\n",
    "plt.errorbar(bincenters, presel, fmt='none', yerr=cv_presel_err, color='darkblue')\n",
    "plt.hist(bincenters, bins, weights=presel,  histtype='step', color='darkblue', \n",
    "         label='nueCC after preselection (33913.0 CV evts, Avg='+str(round(np.average(presel), 3))+')')\n",
    "#plt.hlines(np.average(presel), 0, 250, linestyle='--', label='Average = '+str(round(np.average(presel), 3)))\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(fontsize=13)\n",
    "\n",
    "plt.xlim(0, 250)\n",
    "plt.ylim(0, .4)\n",
    "plt.xlabel(\"Reco X [cm]\", fontsize=14)\n",
    "plt.ylabel(\"Fractional Uncertainty\", fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.title(\"DetSys Uncertainty Evolution (fractional WRT sample)\", fontsize=15)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df_standard['pot_scale'] = beamon_pot/standard_dict.get('CV')    \n",
    "cv_df_intrinsic['pot_scale'] = beamon_pot/intrinsic_dict.get('CV_intrinsic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nue CC events \n",
    "print(\"# nueCC in AV in standard overlay det. sys. sample = \"+str(len(cv_df_standard.query(nueCC_query))))\n",
    "len1 = len(cv_df_standard)\n",
    "    \n",
    "idx = cv_df_standard.query(nueCC_query).index\n",
    "cv_df_standard.drop(idx, inplace=True)\n",
    "len2 = len(cv_df_standard) \n",
    "    \n",
    "print(\"# of nueCC in AV removed = \"+str(len1-len2)) # should be same as above\n",
    "        \n",
    "cv_overlay = pd.concat([cv_df_standard,cv_df_intrinsic], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df_pre = cv_overlay.query(BDT_PRE_QUERY).copy()\n",
    "cv_df_loose = cv_overlay.query(BDT_LOOSE_CUTS).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_w_pre = list(cv_df_pre['ppfx_cv']*cv_df_pre['weightSplineTimesTune']*cv_df_pre['pot_scale'])\n",
    "cv_w_loose = list(cv_df_loose['ppfx_cv']*cv_df_loose['weightSplineTimesTune']*cv_df_loose['pot_scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(cv_df_pre[xvar], bins=bins, range=[bins[0], bins[-1]], weights=cv_w_pre, \n",
    "         histtype='step', label=\"pre\")\n",
    "plt.hist(cv_df_loose[xvar], bins=bins, range=[bins[0], bins[-1]], weights=cv_w_loose, \n",
    "         histtype='step', label='loose')\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(xvar, fontsize=14)\n",
    "plt.title('CV', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_loose[xvar], bins=bins, range=[bins[0], bins[-1]], weights=w_loose, \n",
    "         histtype='step', label=\"LYRayleigh\")\n",
    "\n",
    "plt.hist(cv_df_loose[xvar], bins=bins, range=[bins[0], bins[-1]], weights=cv_w_loose, \n",
    "         histtype='step', label='CV')\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(xvar, fontsize=14)\n",
    "plt.ylabel('$\\\\nu$ / '+str(beamon_pot)+' POT', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beamon_pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv_overlay_counts = plt.hist(cv_overlay[xvar], #bins=bins, range=[bins[0], bins[-1]], \n",
    "         weights=cv_overlay['ppfx_cv']*cv_overlay['weightSplineTimesTune']*cv_overlay['pot_scale'], \n",
    "         histtype='step', label=\"CV full\")[0]\n",
    "overlay_counts = plt.hist(overlay[xvar], #bins=bins, range=[bins[0], bins[-1]], \n",
    "         weights=overlay['ppfx_cv']*overlay['weightSplineTimesTune']*overlay['pot_scale'], \n",
    "         histtype='step', label='LYRayleigh full')[0]\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(xvar, fontsize=14)\n",
    "plt.ylabel('$\\\\nu$ / '+str(beamon_pot)+' POT', fontsize=14)\n",
    "plt.title('Before any selection', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(overlay_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cv_overlay_counts = plt.hist(cv_overlay.query(BDT_LOOSE_CUTS)[xvar], #bins=bins, range=[bins[0], bins[-1]], \n",
    "         weights=cv_overlay.query(BDT_LOOSE_CUTS)['ppfx_cv']*cv_overlay.query(BDT_LOOSE_CUTS)['weightSplineTimesTune']*cv_overlay.query(BDT_LOOSE_CUTS)['pot_scale'], \n",
    "         histtype='step', label=\"CV full\")[0]\n",
    "overlay_counts = plt.hist(overlay.query(BDT_LOOSE_CUTS)[xvar], #bins=bins, range=[bins[0], bins[-1]], \n",
    "         weights=overlay.query(BDT_LOOSE_CUTS)['ppfx_cv']*overlay.query(BDT_LOOSE_CUTS)['weightSplineTimesTune']*overlay.query(BDT_LOOSE_CUTS)['pot_scale'], \n",
    "         histtype='step', label='LYRayleigh full')[0]\n",
    "\n",
    "plt.legend(fontsize=14, loc='upper left')\n",
    "plt.xlabel(xvar, fontsize=14)\n",
    "plt.ylabel('$\\\\nu$ / '+str(beamon_pot)+' POT', fontsize=14)\n",
    "plt.title('After loose cuts', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare only the intrinsic samples \n",
    "\n",
    "cv_intrinsic_counts = plt.hist(cv_df_intrinsic[xvar], #bins=bins, range=[bins[0], bins[-1]], \n",
    "         weights=cv_df_intrinsic['ppfx_cv']*cv_df_intrinsic['weightSplineTimesTune']*cv_df_intrinsic['pot_scale'], \n",
    "         histtype='step', label=\"CV intrinsic\")[0]\n",
    "intrinsic_counts = plt.hist(df_intrinsic[xvar], #bins=bins, range=[bins[0], bins[-1]], \n",
    "         weights=df_intrinsic['ppfx_cv']*df_intrinsic['weightSplineTimesTune']*df_intrinsic['pot_scale'], \n",
    "         histtype='step', label='LYRayleigh intrinsic')[0]\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(xvar, fontsize=14)\n",
    "plt.ylim(0, 1000)\n",
    "plt.ylabel('$\\\\nu$ / '+str(beamon_pot)+' POT', fontsize=14)\n",
    "plt.title('Before any selection', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cv intrinsic counts', sum(cv_intrinsic_counts))\n",
    "print('LYRayleigh intrinsic counts', sum(intrinsic_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare only the intrinsic samples \n",
    "\n",
    "cv_counts = plt.hist(cv_df_standard[xvar], #bins=bins, range=[bins[0], bins[-1]], \n",
    "         weights=cv_df_standard['ppfx_cv']*cv_df_standard['weightSplineTimesTune']*cv_df_standard['pot_scale'], \n",
    "         histtype='step', label=\"CV standard (non-nueCC)\")[0]\n",
    "counts = plt.hist(df_standard[xvar], #bins=bins, range=[bins[0], bins[-1]], \n",
    "         weights=df_standard['ppfx_cv']*df_standard['weightSplineTimesTune']*df_standard['pot_scale'], \n",
    "         histtype='step', label='LYRayleigh standard (non-nueCC)')[0]\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(xvar, fontsize=14)\n",
    "plt.ylabel('$\\\\nu$ / '+str(beamon_pot)+' POT', fontsize=14)\n",
    "plt.title('Before any selection', fontsize=15)\n",
    "#plt.ylim(0, 1300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cv  counts', sum(cv_counts))\n",
    "print('LYRayleigh  counts', sum(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
