{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot event rate variations, fractional uncertainties, & data/MC comparisons \n",
    "# for all sources of systematic error\n",
    "# also consider potential NuMI oscillations on the event rate \n",
    "# make sure to update the plots_path here & in backend function scripts before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import selection_functions as sf\n",
    "\n",
    "import importlib\n",
    "\n",
    "import uproot\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import awkward\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import ROOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"date and time:\",date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NuMIGeoWeights\n",
    "importlib.reload(NuMIGeoWeights)\n",
    "\n",
    "# the default option is FHC, RHC needs different arguments\n",
    "numiBeamlineGeoWeights = NuMIGeoWeights.NuMIGeoWeights() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NuMIDetSys\n",
    "importlib.reload(NuMIDetSys)\n",
    "\n",
    "NuMIDetSysWeights = NuMIDetSys.NuMIDetSys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sf)\n",
    "from selection_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing Run3??\n",
    "ISRUN3 = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nue intrinsic? \n",
    "NUE_INTRINSIC = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ISRUN3: \n",
    "    plots_path = \"/uboone/data/users/kmiller/searchingfornues_v33/v08_00_00_33/plots/rhc/\"\n",
    "\n",
    "else: \n",
    "    plots_path = \"/uboone/data/users/kmiller/searchingfornues_v33/v08_00_00_33/plots/fhc/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POT normalization factors\n",
    "\n",
    "# FHC\n",
    "if not ISRUN3:  \n",
    "    overlay_pot =  2.33652E21  # v7       \n",
    "    dirt_pot = 1.67392E21 # david's file\n",
    "    beamon_pot = 2.0E20 # v5\n",
    "\n",
    "    beamon_ntrig =  5268051.0 # v5 (EA9CNT_wcut)\n",
    "    beamoff_ntrig = 9199232.74  # v5 (EXT_NUMIwin_FEMBeamTriggerAlgo)\n",
    "    \n",
    "    if NUE_INTRINSIC: \n",
    "        nue_intrinsic_pot = 2.37838E22 # v7\n",
    "    \n",
    "\n",
    "# RHC \n",
    "else: \n",
    "    overlay_pot =  1.98937E21 # v7\n",
    "    dirt_pot = 1.03226E21 # v6\n",
    "    beamon_pot = 5.0E20 # v5\n",
    "    \n",
    "    beamon_ntrig = 10363728.0 # v5\n",
    "    beamoff_ntrig =  32878305.25 # v5\n",
    "        \n",
    "    if NUE_INTRINSIC: \n",
    "        nue_intrinsic_pot = 2.5345E22 # v7\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = \"nuselection\"\n",
    "tree = \"NeutrinoSelectionFilter\"\n",
    "\n",
    "DATA = \"\"\n",
    "EXT = \"\"\n",
    "OVRLY  = \"\"\n",
    "DRT = \"\"\n",
    "NUE = \"\"\n",
    "\n",
    "\n",
    "# reduced with opening angle \n",
    "path = ''\n",
    "\n",
    "if not ISRUN3: \n",
    "    \n",
    "    path = '/uboone/data/users/kmiller/ntuples/run1/qualcuts/'#nuepresel/'\n",
    "    \n",
    "    # Run 1 FHC \n",
    "    OVRLY = 'neutrinoselection_filt_run1_overlay_v7'\n",
    "    EXT = 'neutrinoselection_filt_run1_beamoff_v5'\n",
    "    DATA = 'neutrinoselection_filt_run1_beamon_beamgood_v5'\n",
    "    DRT = 'prodgenie_numi_uboone_overlay_dirt_fhc_mcc9_run1_v28_all_snapshot'\n",
    "    \n",
    "    if NUE_INTRINSIC: \n",
    "        NUE = 'neutrinoselection_filt_run1_overlay_intrinsic_v7'\n",
    "\n",
    "else: \n",
    "    \n",
    "    path = '/uboone/data/users/kmiller/ntuples/run3b/nuepresel/'\n",
    "    \n",
    "    # Run 3 RHC\n",
    "    OVRLY = 'neutrinoselection_filt_run3b_overlay_v7'\n",
    "    DATA = 'neutrinoselection_filt_run3b_beamon_beamgood_v5'\n",
    "    EXT = 'neutrinoselection_filt_run3b_beamoff_v5'\n",
    "    DRT = 'neutrinoselection_filt_run3b_dirt_overlay_v6'\n",
    "    \n",
    "    if NUE_INTRINSIC: \n",
    "        NUE = 'neutrinoselection_filt_run3b_overlay_intrinsic_v7'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = uproot.open(path+OVRLY+\".root\")[fold][tree]\n",
    "data = uproot.open(path+DATA+\".root\")[fold][tree]\n",
    "ext = uproot.open(path+EXT+\".root\")[fold][tree]\n",
    "dirt = uproot.open(path+DRT+\".root\")[fold][tree]  \n",
    "\n",
    "uproot_v = [overlay,data,ext,dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue = uproot.open(path+NUE+\".root\")[fold][tree]\n",
    "    uproot_v.append(nue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    \"selected\", \"nu_pdg\", \"shr_theta\", \"true_e_visible\", \n",
    "    \"trk_score_v\", \n",
    "    \"shr_tkfit_dedx_Y\", \"ccnc\", \"n_tracks_contained\", \n",
    "    \"NeutrinoEnergy2\",\n",
    "    \"reco_nu_vtx_sce_x\",\"reco_nu_vtx_sce_y\",\"reco_nu_vtx_sce_z\",\n",
    "    \"shrsubclusters0\",\"shrsubclusters1\",\"shrsubclusters2\", # number of sub-clusters in shower\n",
    "    \"trkshrhitdist2\",\n",
    "    \"nproton\", \"nu_e\", \"n_showers_contained\", \"nu_purity_from_pfp\", \n",
    "    \"shr_phi\", \"trk_phi\", \"trk_theta\",\n",
    "    \"shr_score\", \n",
    "    \"trk_energy\", \"tksh_distance\", \"tksh_angle\",\n",
    "    \"npi0\",\n",
    "    \"shr_energy_tot_cali\",  \n",
    "    \"nslice\", \n",
    "    \"contained_fraction\",\n",
    "    \"true_nu_vtx_x\", \"true_nu_vtx_y\" , \"true_nu_vtx_z\", \n",
    "    \"npion\", \"shr_energy_cali\", \n",
    "    \"shrmoliereavg\", \"shr_px\", \"shr_py\", \"shr_pz\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC only variables\n",
    "mc_var = [\"weightTune\", \"weightSpline\", \"weightSplineTimesTune\", \"true_nu_px\", \"true_nu_py\", \"true_nu_pz\", \n",
    "            \"elec_e\", \"proton_e\", \"mc_px\", \"mc_py\", \"mc_pz\", \"elec_px\", \"elec_py\", \"elec_pz\", \n",
    "            \"swtrig_pre\", \"ppfx_cv\", \"mc_pdg\"]#, \"opening_angle\"]\n",
    "\n",
    "sys_genie = [\"weightsGenie\", \"weightsReint\"]\n",
    "sys_flux = ['weightsPPFX']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = overlay.pandas.df(variables + mc_var + sys_genie + sys_flux, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay['weightsPPFX'] = overlay['weightsPPFX']/1000\n",
    "overlay['weightsReint'] = overlay['weightsReint']/1000\n",
    "overlay['weightsGenie'] = overlay['weightsGenie']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt = dirt.pandas.df(variables + mc_var + sys_genie, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no flux weights exist for dirt (yet?)\n",
    "dirt['weightsPPFX'] = [[1 for x in range(len(overlay['weightsPPFX'].iloc[0]))] for y in range(len(dirt))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUE_INTRINSIC: \n",
    "    nue = nue.pandas.df(variables + mc_var + sys_genie + sys_flux, flatten=False)\n",
    "    nue['weightsPPFX'] = nue['weightsPPFX']/1000\n",
    "    nue['weightsGenie'] = nue['weightsGenie']/1000\n",
    "    nue['weightsReint'] = nue['weightsReint']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.pandas.df(variables, flatten=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = ext.pandas.df(variables, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframes equal # of columns \n",
    "\n",
    "for var in mc_var+sys_genie+sys_flux: \n",
    "    data[var] = np.nan\n",
    "    ext[var] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to get the LLR-PID value for the \"track candidate\" \n",
    "# (proton for nue selection, muon for numu)\n",
    "# can be done for any variable\n",
    "# code from Giuseppe!\n",
    "#LLR-PID : log likelihood ratio particle ID \n",
    "\n",
    "df_v = [overlay,data,ext,dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    df_v.append(nue)\n",
    "    \n",
    "for i,df in enumerate(df_v):\n",
    "    up = uproot_v[i]\n",
    "    trk_llr_pid_v = up.array('trk_llr_pid_score_v')\n",
    "    trk_id = up.array('trk_id')-1 # I think we need this -1 to get the right result\n",
    "    trk_llr_pid_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_llr_pid_v,trk_id)])\n",
    "    df['trkpid'] = trk_llr_pid_v_sel\n",
    "    df['subcluster'] = df['shrsubclusters0'] + df['shrsubclusters1'] + df['shrsubclusters2']\n",
    "    \n",
    "    df['NeutrinoEnergy2_GeV'] = df['NeutrinoEnergy2']/1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add truth level theta & phi angles (detector & beam coordinates)\n",
    "overlay = addAngles(overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt = addAngles(dirt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUE_INTRINSIC: \n",
    "    nue = addAngles(nue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_df = [overlay, dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    mc_df.append(nue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add beamline geometry weights \n",
    "\n",
    "for i,df in enumerate(mc_df):\n",
    "    df['weightsNuMIGeo'] = df.apply( lambda x: numiBeamlineGeoWeights.calculateGeoWeight(x['nu_pdg'],x['nu_e'],x['thbeam']) , axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframes equal # of columns \n",
    "\n",
    "nan_var = ['thdet', 'phidet', 'true_nu_px_beam', 'true_nu_py_beam', 'true_nu_pz_beam', \n",
    "           'thbeam', 'phibeam','weightsNuMIGeo']\n",
    "\n",
    "for var in nan_var: \n",
    "    data[var] = np.nan\n",
    "    ext[var] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.setdiff1d(ext.columns,overlay.columns)\n",
    "# ext.columns == overlay.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuts applied for bad weights \n",
    "for i,df in enumerate(mc_df):\n",
    "    df.loc[ df['weightSplineTimesTune'] <= 0, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] == np.inf, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] > 100, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightSplineTimesTune']) == True, 'weightSplineTimesTune' ] = 1.\n",
    "    \n",
    "    df.loc[ df['weightTune'] <= 0, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] == np.inf, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] > 100, 'weightTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightTune']) == True, 'weightTune' ] = 1.  \n",
    "    \n",
    "    for ievt in range(df.shape[0]):\n",
    "        reweightCondition = ((df['weightsGenie'].iloc[ievt] > 60) | (df['weightsGenie'].iloc[ievt] < 0)  | \n",
    "                             (df['weightsGenie'].iloc[ievt] == np.inf) | (df['weightsGenie'].iloc[ievt] == np.nan))\n",
    "        df['weightsGenie'].iloc[ievt][ reweightCondition ] = 1.\n",
    "        \n",
    "        # if no variations exist for the event\n",
    "        if not list(df['weightsGenie'].iloc[ievt]): \n",
    "            df['weightsGenie'].iloc[ievt] = [1.0 for k in range(600)]\n",
    "        \n",
    "        reweightCondition2 = ((df['weightsReint'].iloc[ievt] > 60) | (df['weightsReint'].iloc[ievt] < 0)  | \n",
    "                             (df['weightsReint'].iloc[ievt] == np.inf) | (df['weightsReint'].iloc[ievt] == np.nan))\n",
    "        df['weightsReint'].iloc[ievt][ reweightCondition2 ] = 1.\n",
    "        \n",
    "        # if no variations exist for the event\n",
    "        if not list(df['weightsReint'].iloc[ievt]): \n",
    "            df['weightsReint'].iloc[ievt] = [1.0 for k in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pot scaling weights \n",
    "\n",
    "# applied tunes \n",
    "dirt_tune = 0.35\n",
    "ext_tune = 1\n",
    "\n",
    "# Run 1\n",
    "if not ISRUN3: \n",
    "    ext_tune = .98\n",
    "\n",
    "# Run 3\n",
    "else: \n",
    "    ext_tune = .94\n",
    "    \n",
    "    \n",
    "##############################################\n",
    "# SCALE  TO BEAM ON POT\n",
    "overlay_scale_to_data = beamon_pot/overlay_pot\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue_scale_to_data = beamon_pot/nue_intrinsic_pot\n",
    "\n",
    "dirt_scale_to_data = dirt_tune*(beamon_pot/dirt_pot)\n",
    "beamoff_scale_to_data = ext_tune*(beamon_ntrig/beamoff_ntrig) # scale factor to beam on POT\n",
    "\n",
    "overlay['pot_scale'] = overlay_scale_to_data\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue['pot_scale'] = nue_scale_to_data\n",
    "    \n",
    "dirt['pot_scale'] = dirt_scale_to_data\n",
    "ext['pot_scale'] = beamoff_scale_to_data\n",
    "data['pot_scale'] = [1 for x in range(len(data))]\n",
    "##############################################\n",
    "# SCALE TO OVERLAY\n",
    "\n",
    "dirt_scale_to_overlay = dirt_tune*(overlay_pot/dirt_pot)\n",
    "beamoff_scale_to_overlay = ext_tune*((overlay_pot/beamon_pot)*(beamon_ntrig/beamoff_ntrig))\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue_scale_to_overlay = overlay_pot/nue_intrinsic_pot\n",
    "\n",
    "overlay['pot_scale_overlay'] = [1 for x in range(len(overlay))]\n",
    "if NUE_INTRINSIC: \n",
    "    nue['pot_scale_overlay'] = nue_scale_to_overlay\n",
    "    \n",
    "dirt['pot_scale_overlay'] = dirt_scale_to_overlay\n",
    "ext['pot_scale_overlay'] = beamoff_scale_to_overlay\n",
    "data['pot_scale_overlay'] = [1 for x in range(len(data))]\n",
    "##############################################\n",
    "# SCALE TO PROJECTED \n",
    "proj_pot = 0.0\n",
    "\n",
    "if not ISRUN3: \n",
    "    proj_pot = 9.23E20 # FHC\n",
    "else: \n",
    "    proj_pot = 11.95E20 # RHC\n",
    "\n",
    "overlay_scale_to_proj = proj_pot/overlay_pot\n",
    "dirt_scale_to_proj = dirt_tune*(proj_pot/dirt_pot)\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue_scale_to_proj = proj_pot/nue_intrinsic_pot\n",
    "\n",
    "# first scale to beamon, then scale to projected\n",
    "beamoff_scale_to_proj = (ext_tune*(beamon_ntrig/beamoff_ntrig)) * (proj_pot/beamon_pot)\n",
    "\n",
    "overlay['pot_scale_proj'] = overlay_scale_to_proj\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue['pot_scale_proj'] = nue_scale_to_proj\n",
    "    \n",
    "dirt['pot_scale_proj'] = dirt_scale_to_proj\n",
    "ext['pot_scale_proj'] = beamoff_scale_to_proj\n",
    "data['pot_scale_proj'] = [1 for x in range(len(data))]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total weights \n",
    "\n",
    "# combined genie * POT weight * flux weight (now using online)\n",
    "# ext gets POT weight only \n",
    "\n",
    "flux_weight = 'ppfx_cv'\n",
    "\n",
    "################################################################\n",
    "# totweight scales to BEAMON\n",
    "\n",
    "# tuned\n",
    "overlay['totweight'] = overlay['pot_scale']*overlay[flux_weight]*overlay['weightSplineTimesTune']\n",
    "dirt['totweight'] = dirt['pot_scale']*dirt[flux_weight]*dirt['weightSplineTimesTune']\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue['totweight'] = nue['pot_scale']*nue[flux_weight]*nue['weightSplineTimesTune']\n",
    "\n",
    "\n",
    "################################################################\n",
    "# totweight_overlay scales to STANDARD OVERLAY\n",
    "\n",
    "# tuned\n",
    "overlay['totweight_overlay'] = overlay[flux_weight]*overlay['weightSplineTimesTune']\n",
    "dirt['totweight_overlay'] = dirt['pot_scale_overlay']*dirt[flux_weight]*dirt['weightSplineTimesTune']\n",
    "\n",
    "if NUE_INTRINSIC:\n",
    "    nue['totweight_overlay'] = nue['pot_scale_overlay']*nue[flux_weight]*nue['weightSplineTimesTune']\n",
    "\n",
    "################################################################\n",
    "# totweight_proj scales to TOTAL PROJECTED BEAM ON \n",
    "\n",
    "overlay['totweight_proj'] = overlay['pot_scale_proj']*overlay[flux_weight]*overlay['weightSplineTimesTune']\n",
    "dirt['totweight_proj'] = dirt['pot_scale_proj']*dirt[flux_weight]*dirt['weightSplineTimesTune']\n",
    "\n",
    "if NUE_INTRINSIC:\n",
    "    nue['totweight_proj'] = nue['pot_scale_proj']*nue[flux_weight]*nue['weightSplineTimesTune']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to keep the number of columns the same \n",
    "new_var = ['totweight', 'totweight_overlay', 'totweight_proj']\n",
    "\n",
    "for var in new_var: \n",
    "    for df in [data, ext]: \n",
    "        df[var] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_fv_query = \"10<=true_nu_vtx_x<=246 and -106<=true_nu_vtx_y<=106 and 10<=true_nu_vtx_z<=1026\"\n",
    "reco_in_fv_query = \"10<=reco_nu_vtx_sce_x<=246 and -106<=reco_nu_vtx_sce_y<=106 and 10<=reco_nu_vtx_sce_z<=1026\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace overlay nue CC events with nue intrinsic sample\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    # intrinsic sample contains in AV TPC events ONLY, & only CC events (overlay is entire cryo)\n",
    "    in_AV_query = \"-1.55<=true_nu_vtx_x<=254.8 and -116.5<=true_nu_vtx_y<=116.5 and 0<=true_nu_vtx_z<=1036.8\"\n",
    "    \n",
    "    nueCC_query = 'abs(nu_pdg)==12 and ccnc==0 and '+in_AV_query\n",
    "    print(\"# of nueCC in AV in overlay sample = \"+str(len(overlay.query(nueCC_query))))\n",
    "    len1 = len(overlay)\n",
    "    \n",
    "    idx = overlay.query(nueCC_query).index\n",
    "    overlay.drop(idx, inplace=True)\n",
    "    len2 = len(overlay) \n",
    "    print(\"# of nueCC in AV dropped in overlay = \"+str(len1-len2))\n",
    "    \n",
    "    overlay = pd.concat([overlay,nue], ignore_index=True)\n",
    "\n",
    "    # from here on out everything else should be the same. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SW trigger, combine overlay + dirt as MC \n",
    "mc = pd.concat([overlay.query('swtrig_pre==1'),dirt.query('swtrig_pre==1')], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate by in/out FV & cosmic\n",
    "infv = mc.query(in_fv_query+' and nu_purity_from_pfp>0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmic = mc.query(in_fv_query+' and nu_purity_from_pfp<=0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfv = mc.query(out_fv_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check that everything is accounted for \n",
    "print(len(mc)==len(infv)+len(cosmic)+len(outfv))\n",
    "\n",
    "if not (len(mc)==len(infv)+len(cosmic)+len(outfv)): \n",
    "    d = len(mc) - (len(infv)+len(cosmic)+len(outfv))\n",
    "    print(d)\n",
    "    \n",
    "     \n",
    "    m = pd.concat([infv, cosmic, outfv])\n",
    "    diff = np.setdiff1d(list(mc.index),list(m.index))\n",
    "\n",
    "    #for i in range(d):\n",
    "        #print(mc.loc[diff[i], 'nu_purity_from_pfp'])\n",
    "        #print(mc.loc[diff[i], 'nslice'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 main categories: \n",
    "\n",
    "# infv - overlay & dirt events with truth vtx in FV \n",
    "# outfv - overlay & dirt events with truth vtx in FV that are classified as neutrinos\n",
    "# cosmic - overlay & dirt events with true vtx in FV that get misclassified as cosmic \n",
    "# ext - beam OFF data\n",
    "# data - beam ON data \n",
    "\n",
    "datasets = [infv, outfv, cosmic, ext, data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply BDT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT_PRE_QUERY = 'nslice==1'\n",
    "BDT_PRE_QUERY += ' and ' + reco_in_fv_query\n",
    "BDT_PRE_QUERY +=' and contained_fraction>0.9'\n",
    "BDT_PRE_QUERY += ' and n_tracks_contained>0'\n",
    "BDT_PRE_QUERY += ' and n_showers_contained==1'\n",
    "BDT_PRE_QUERY += ' and shr_energy_tot_cali>0.07'\n",
    "#BDT_PRE_QUERY += ' and trk_energy>0.04' # 40 MeV reco pion/proton cut on leading track - what about non-leading tracks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT_LOOSE_CUTS = BDT_PRE_QUERY\n",
    "BDT_LOOSE_CUTS += ' and shr_score<0.3'\n",
    "BDT_LOOSE_CUTS += ' and trkpid<0.35'\n",
    "BDT_LOOSE_CUTS += ' and shrmoliereavg<15'\n",
    "BDT_LOOSE_CUTS += ' and shr_tkfit_dedx_Y<7'\n",
    "BDT_LOOSE_CUTS += ' and tksh_distance<12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bdt model \n",
    "bdt_model = xgb.Booster({'nthread': 4})\n",
    "bdt_model.load_model('bdt_model_feb2021.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the saved model to our df \n",
    "\n",
    "datasets_bdt = []\n",
    "\n",
    "varlist = [\n",
    "    \"shr_score\", \"shrmoliereavg\", \"trkpid\",\n",
    "    \"n_showers_contained\", \"shr_tkfit_dedx_Y\", \"tksh_distance\",\n",
    "    \"tksh_angle\", \"subcluster\", \"trkshrhitdist2\"]\n",
    "\n",
    "for df in datasets: \n",
    "\n",
    "    # apply cuts\n",
    "    df = df.copy()\n",
    "    df = df.query(BDT_LOOSE_CUTS)\n",
    "\n",
    "    # clean datasets \n",
    "    for column in varlist:\n",
    "        df.loc[(df[column] < -1.0e37) | (df[column] > 1.0e37), column] = np.nan\n",
    "    \n",
    "    # create testing dmatrix \n",
    "    df_test = xgb.DMatrix(data=df[varlist])\n",
    "    \n",
    "    # apply the bdt selection\n",
    "    preds = bdt_model.predict(df_test)\n",
    "\n",
    "    # add columns for plotting \n",
    "    df['BDT_score'] = preds\n",
    "    \n",
    "    datasets_bdt.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic Variations & Event Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ROOT file with BDT-selected detector variations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this step if it is already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations = {\n",
    "    \"LYAttenuation\": 7.51336E20,\n",
    "    \"LYRayleigh\": 7.60573E20, \n",
    "    \"LYDown\": 7.43109E20, \n",
    "    \"SCE\": 7.39875E20, \n",
    "    \"Recomb2\": 7.59105E20, \n",
    "    \"WireModX\": 7.64918E20, \n",
    "    \"WireModYZ\": 7.532E20, \n",
    "    \"WireModThetaXZ\": 7.64282E20,\n",
    "    \"WireModThetaYZ_withSigmaSplines\": 7.64543E20, \n",
    "    \"WireModThetaYZ_withoutSigmaSplines\": 7.5783E20, \n",
    "    \"CV\": 7.59732E20\n",
    "}\n",
    "\n",
    "intrinsic_variations = {\n",
    "    \"LYAttenuation_intrinsic\": 2.3837E22, \n",
    "    \"LYRayleigh_intrinsic\": 2.38081E22, \n",
    "    \"LYDown_intrinsic\": 2.24505E22, \n",
    "    \"SCE_intrinsic\": 2.39023E22, \n",
    "    \"Recomb2_intrinsic\": 2.38193E22, \n",
    "    \"WireModX_intrinsic\": 2.38318E22, \n",
    "    \"WireModYZ_intrinsic\": 2.38416E22,\n",
    "    \"WireModThetaXZ_intrinsic\": 2.31518E22, \n",
    "    \"WireModThetaYZ_withSigmaSplines_intrinsic\": 2.31421E22, \n",
    "    \"WireModThetaYZ_withoutSigmaSplines_intrinsic\": 2.31755E22, \n",
    "    \"CV_intrinsic\": 2.37261E22   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in list(variations.keys()): \n",
    "    NuMIDetSysWeights.makehist_detsys(v, \"BDT_score>0.575\", intrinsic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in list(intrinsic_variations.keys()): \n",
    "    NuMIDetSysWeights.makehist_detsys(v, \"BDT_score>0.575\", intrinsic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1, -0.6, -0.2, 0.2, 0.6, 1]\n",
    "xvar = \"tksh_angle\"\n",
    "x_label = \"Opening Angle (cos $\\\\theta_{ep}$)\"\n",
    "data_pot = \"$2.0\\\\times10^{20}$ POT\"\n",
    "\n",
    "xlow = -1\n",
    "xhigh = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0.19, .4, .65, .85, 1.15, 1.5, 4]\n",
    "xvar = \"NeutrinoEnergy2_GeV\"\n",
    "x_label = \"Total Deposited Energy [GeV]\"\n",
    "data_pot = \"$2.0\\\\times10^{20}$ POT\"\n",
    "xlow = 0\n",
    "xhigh = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0.09, 0.4, 0.65, 1, 3]\n",
    "xvar = \"shr_energy_cali\"\n",
    "x_label = \"Shower Energy [GeV]\"\n",
    "data_pot = \"$2.0\\\\times10^{20}$ POT\"\n",
    "xlow = 0.09\n",
    "xhigh = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [1, 2, 3, 7]\n",
    "xvar = \"n_tracks_contained\"\n",
    "x_label = \"Track Multiplicity\"\n",
    "data_pot = \"$2.0\\\\times10^{20}$ POT\"\n",
    "xlow = 1\n",
    "xhigh = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = plot_mc(xvar, bins, xlow, xhigh, 'BDT_score>0.575', datasets_bdt, ISRUN3, \n",
    "            plt_norm='proj', pot='$9.23\\\\times10^{20}$', ymax=450, sys=False, x_label=x_label, \n",
    "            save=False, save_label=\"proj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flux Systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = calcSysError(xvar, bins, xlow, xhigh, 'BDT_score>0.575', datasets_bdt, 'weightsPPFX', 600, \n",
    "                 plot=True, save=False, axis_label=x_label, pot=data_pot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = calcSysError(xvar, bins, xlow, xhigh, 'BDT_score>0.575', datasets_bdt, 'weightsNuMIGeo', 20, \n",
    "                 plot=True, save=False, axis_label=x_label, pot=data_pot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GENIE systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = calcSysError(xvar, bins, xlow, xhigh, \n",
    "             'BDT_score>0.575', datasets_bdt, 'weightsGenie', 600, plot=True, save=False, \n",
    "                axis_label=x_label, pot=data_pot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEED GENIE UNISIMS HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GEANT4 systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = calcSysError(xvar, bins, xlow, xhigh, \n",
    "             'BDT_score>0.575', datasets_bdt, 'weightsReint', 1000, plot=True, save=False, \n",
    "                axis_label=x_label, pot=data_pot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector Systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nueCC \n",
    "x = calcDetSysError(xvar, bins, plot=True, plot_cov=False, save=False, axis_label=x_label, \n",
    "                    intrinsic=True, pot=data_pot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break down of the detector systematics \n",
    "# x[4]\n",
    "\n",
    "keys = list(intrinsic_variations.keys())\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for v in x[4]: # for each list of fractional uncertainties\n",
    "    plt.step(bins, [0]+v, label=keys[count][:-10])\n",
    "    count += 1\n",
    "    \n",
    "plt.xlim(xlow,xhigh)\n",
    "plt.ylim(0, 0.3)\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.xlabel(x_label, fontsize=14)\n",
    "plt.ylabel(\"Fractional Uncertainty\", fontsize=14)\n",
    "\n",
    "plt.legend(frameon=False, ncol=2, loc='upper left')\n",
    "plt.title(\"Detector Variations ($\\\\nu_{e}$ CC events)\", fontsize=14)\n",
    "#plt.savefig(plots_path+xvar+\"_DetFracUncertainty_Intrinsic.pdf\", transparent=True, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non nueCC backgrounds\n",
    "x = calcDetSysError(xvar, bins, plot=True, plot_cov=False, save=False, axis_label=x_label, \n",
    "                    intrinsic=False, pot=data_pot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break down of the detector systematics \n",
    "# x[4]\n",
    "\n",
    "keys = list(variations.keys())\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for v in x[4]: # for each list of fractional uncertainties\n",
    "    plt.step(bins, [0]+v, label=keys[count])\n",
    "    count += 1\n",
    "    \n",
    "plt.xlim(xlow,xhigh)\n",
    "plt.ylim(0)\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.xlabel(x_label, fontsize=14)\n",
    "plt.ylabel(\"Fractional Uncertainty\", fontsize=14)\n",
    "\n",
    "plt.legend(frameon=False, ncol=2)\n",
    "plt.title(\"Detector Variations (non-$\\\\nu_{e}$ CC events)\", fontsize=14)\n",
    "#plt.savefig(plots_path+xvar+\"_DetFracUncertainty.pdf\", transparent=True, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_errors, tot_percent_error = plot_data(xvar, bins, bins[0], xhigh, 'BDT_score>0.575', \n",
    "          datasets_bdt, ISRUN3, plt_norm='pot', bdt_scale=None, ymax=85, \n",
    "          x_label=x_label, sys=True, save=False, save_label=\"Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the fractional systematic uncertainty \n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))  \n",
    "\n",
    "plt.step(bins, [0]+percent_errors[0], label=\"Statistical\", color='darkgreen')\n",
    "plt.step(bins, [0]+percent_errors[1], label=\"PPFX\", color='tab:red')\n",
    "plt.step(bins, [0]+percent_errors[2], label=\"Beamline Geometry\", color='orange')\n",
    "plt.step(bins, [0]+percent_errors[3], label=\"GENIE\", color='peru')\n",
    "plt.step(bins, [0]+percent_errors[4], label=\"Re-Interaction\", color='violet')\n",
    "plt.step(bins, [0]+percent_errors[5], label=\"Detector ($\\\\nu_{e}$ CC events)\", color='lightskyblue')\n",
    "plt.step(bins, [0]+tot_percent_error, label=\"Total\", color='black',linewidth=2)\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.xlabel(x_label, fontsize=14)\n",
    "plt.ylabel(\"Fractional Uncertainty\", fontsize=14)\n",
    "\n",
    "plt.xlim(bins[0], xhigh)\n",
    "plt.ylim(0, .5)\n",
    "\n",
    "plt.legend(fontsize=12, frameon=False, ncol=2)\n",
    "\n",
    "#plt.savefig(plots_path+xvar+\"_FracUncertainty.pdf\", transparent=True, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NuMI Oscillations (3+1 Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in list(variations.keys()): \n",
    "    NuMIDetSysWeights.makehist_detsys(v, \"BDT_score>0.575\", [\"nu_e\"], intrinsic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0.19, .4, .65, .85, 1.15, 1.5, 4]\n",
    "xvar = \"nu_e\"\n",
    "x_label = \"True Neutrino Energy [GeV]\"\n",
    "data_pot = \"$2.0\\\\times10^{20}$ POT\"\n",
    "xlow = 0\n",
    "xhigh = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = plot_mc(xvar, [round(0.01*x, 2) for x in range(0, 75, 5)], 0, 0.7, \n",
    "        'BDT_score>0.575', datasets_bdt, ISRUN3, \n",
    "        plt_norm='proj', pot='$9.23\\\\times10^{20}$', ymax=30,\n",
    " sys=True, x_label='True Neutrino Energy [GeV]', save=False, save_label='bestfit',\n",
    "       osc='machado_bestfit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = plot_mc(xvar, [round(0.01*x, 2) for x in range(0, 75, 5)], 0, 0.7, \n",
    "        'BDT_score>0.575', datasets_bdt, ISRUN3, \n",
    "        plt_norm='proj', pot='$9.23\\\\times10^{20}$', ymax=30,\n",
    " sys=True, x_label='True Neutrino Energy [GeV]', save=False, save_label='biggest',\n",
    "       osc='biggest_variation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full range \n",
    "# smaller binning [round(0.01*x, 2) for x in range(0, 455, 5)]\n",
    "\n",
    "x = plot_mc(xvar, [0, 0.5, 1, 1.50, 2, 2.5, 3, 3.5, 4, 4.5], 0, 4.5, \n",
    "        'BDT_score>0.575', datasets_bdt, ISRUN3, \n",
    "        plt_norm='proj', pot='$9.23\\\\times10^{20}$', #ymax=30,\n",
    "        sys=True, x_label='True Neutrino Energy [GeV]', save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to save the dictionary from plot_MC output\n",
    "\n",
    "import json\n",
    "\n",
    "with open('Insert_File_Name_Here.json', 'w') as f:\n",
    "    json.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load a stored dictionary \n",
    "with open('FHC_Projected_TrueNeutrinoEnergy.json') as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
