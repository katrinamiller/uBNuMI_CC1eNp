{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python notebook for producing upstream data/mc comparisons\n",
    "# by creating slimmed versions of the pandas dataframes\n",
    "\n",
    "# Doing Run3??\n",
    "ISRUN3 = True\n",
    "\n",
    "# load systematic universes? \n",
    "isPPFX = False\n",
    "isGENIE = False\n",
    "isGEANT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, 'backend_functions')\n",
    "\n",
    "import importlib\n",
    "\n",
    "import uproot\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import top \n",
    "importlib.reload(top)\n",
    "from top import *\n",
    "\n",
    "\n",
    "\n",
    "import selection_functions as sf\n",
    "importlib.reload(sf)\n",
    "from selection_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ISRUN3: \n",
    "    chi2_label = \"RHC RUN 3\"\n",
    "    save_label = \"rhcrun3\"\n",
    "    beamon_pot_str = \"5.0 $\\\\times 10^{20}$\"\n",
    "    \n",
    "else: \n",
    "    chi2_label = \"FHC RUN 1\"\n",
    "    save_label = \"fhcrun1\"\n",
    "    beamon_pot_str = \"2.0 $\\\\times 10^{20}$\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NuMIGeoWeights\n",
    "importlib.reload(NuMIGeoWeights)\n",
    "\n",
    "if ISRUN3: \n",
    "    current = \"RHC\"\n",
    "    \n",
    "else: \n",
    "    current = \"FHC\"\n",
    "\n",
    "numiBeamlineGeoWeights = NuMIGeoWeights.NuMIGeoWeights(current=current) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nue intrinsic? \n",
    "NUE_INTRINSIC = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"date and time:\",date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_path = parameters(ISRUN3)['plots_path']\n",
    "plots_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = \"nuselection\"\n",
    "tree = \"NeutrinoSelectionFilter\"\n",
    "\n",
    "DATA = \"\"\n",
    "EXT = \"\"\n",
    "OVRLY  = \"\"\n",
    "DRT = \"\"\n",
    "NUE = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ISRUN3: \n",
    "    path = \"/uboone/data/users/kmiller/uBNuMI_CCNp/ntuples/run3b/cv/\"\n",
    "    print('path = ', path)\n",
    "    \n",
    "        \n",
    "    # Run 3 RHC\n",
    "    OVRLY = 'neutrinoselection_filt_run3b_overlay_v7'\n",
    "    DATA = 'neutrinoselection_filt_run3b_beamon_beamgood_v5'\n",
    "    EXT = 'neutrinoselection_filt_run3b_beamoff_v5'\n",
    "    DRT = 'neutrinoselection_filt_run3b_dirt_overlay_v6'\n",
    "    \n",
    "    if NUE_INTRINSIC: \n",
    "        NUE = 'neutrinoselection_filt_run3b_overlay_intrinsic_v7'\n",
    "    \n",
    "else: \n",
    "    \n",
    "    path = \"/uboone/data/users/kmiller/uBNuMI_CCNp/ntuples/run1/cv/\"\n",
    "    print('path = ', path)\n",
    "    \n",
    "    # Run 1 FHC \n",
    "    OVRLY = 'neutrinoselection_filt_run1_overlay_v7'\n",
    "    EXT = 'neutrinoselection_filt_run1_beamoff_v5'\n",
    "    DATA = 'neutrinoselection_filt_run1_beamon_beamgood_v5'\n",
    "    DRT = 'prodgenie_numi_uboone_overlay_dirt_fhc_mcc9_run1_v28_all_snapshot'\n",
    "    \n",
    "    if NUE_INTRINSIC: \n",
    "        NUE = 'neutrinoselection_filt_run1_overlay_intrinsic_v7'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = uproot.open(path+OVRLY+\".root\")[fold][tree]\n",
    "data = uproot.open(path+DATA+\".root\")[fold][tree]\n",
    "ext = uproot.open(path+EXT+\".root\")[fold][tree]\n",
    "dirt = uproot.open(path+DRT+\".root\")[fold][tree]  \n",
    "\n",
    "uproot_v = [overlay,data,ext,dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue = uproot.open(path+NUE+\".root\")[fold][tree]\n",
    "    uproot_v.append(nue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    \"reco_nu_vtx_sce_x\",\"reco_nu_vtx_sce_y\",\"reco_nu_vtx_sce_z\",\n",
    "    #\"nslice\", \n",
    "    #\"contained_fraction\", \n",
    "    \"run\", \"flash_time\"\n",
    "]\n",
    "\n",
    "# MC only variables\n",
    "mc_var = [\"nu_pdg\", \"ccnc\", \n",
    "          \"nproton\",  \"npi0\", \"npion\",\n",
    "          \"true_nu_vtx_x\", \"true_nu_vtx_y\" , \"true_nu_vtx_z\", \n",
    "          \"weightSplineTimesTune\", \"weightTune\",\"ppfx_cv\", \"swtrig_pre\",\n",
    "          'nu_e', \n",
    "          \"true_nu_px\", \"true_nu_py\", \"true_nu_pz\"]#, 'weightsGenie', 'weightsReint']\n",
    "\n",
    "if isPPFX: \n",
    "    mc_var.append('weightsPPFX')\n",
    "if isGENIE: \n",
    "    mc_var.append('weightsGenie')\n",
    "if isGEANT: \n",
    "    mc_var.append('weightsReint')\n",
    "    \n",
    "\n",
    "sys_genie_unisim = [\n",
    "             \"knobRPAup\", \"knobRPAdn\", \n",
    "             \"knobCCMECup\", \n",
    "             \"knobAxFFCCQEup\", \n",
    "             \"knobVecFFCCQEup\", \n",
    "             \"knobDecayAngMECup\", \n",
    "             \"knobThetaDelta2Npiup\", \n",
    "             \"knobThetaDelta2NRadup\", \n",
    "             \"knobNormCCCOHup\", \n",
    "             \"knobNormNCCOHup\",   \n",
    "             \"knobxsr_scc_Fv3up\",  # these are supposed to be multisims - 10 universes each -- map to pull out\n",
    "             \"knobxsr_scc_Fa3up\" ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create slim pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start:\",datetime.now().strftime(\"%H:%M:%S\"))\n",
    "overlay = overlay.pandas.df(variables+mc_var+sys_genie_unisim, flatten=False)\n",
    "print(\"end:\",datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt = dirt.pandas.df(variables+mc_var+sys_genie_unisim[:-2], flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt['knobxsr_scc_Fv3up'] = 1\n",
    "dirt['knobxsr_scc_Fa3up'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isGEANT: \n",
    "    dirt['weightsReint'] = [np.array([1000 for k in range(1000)]) for x in range(len(dirt['weightsReint']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUE_INTRINSIC: \n",
    "    nue = nue.pandas.df(variables+mc_var+sys_genie_unisim, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.pandas.df(variables, flatten=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = ext.pandas.df(variables, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in mc_var: \n",
    "    data[var] = np.nan\n",
    "    ext[var] = np.nan\n",
    "    \n",
    "for var in sys_genie_unisim: \n",
    "    data[var] = np.nan\n",
    "    ext[var] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay['isDirt'] = False\n",
    "dirt['isDirt'] = True\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue['isDirt'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['isDirt'] = np.nan\n",
    "ext['isDirt'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_df = [overlay, dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    mc_df.append(nue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(mc_df):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    # is signal bool \n",
    "    #df['is_signal'] = np.where((df.swtrig_pre == 1) \n",
    "   #                          & (df.nu_pdg==12) & (df.ccnc==0) & (df.nproton>0) & (df.npion==0) & (df.npi0==0)\n",
    "   #                          & (10 <= df.true_nu_vtx_x) & (df.true_nu_vtx_x <= 246)\n",
    "   #                          & (-106 <= df.true_nu_vtx_y) & (df.true_nu_vtx_y <= 106)\n",
    "   #                          & (10 <= df.true_nu_vtx_z) & (df.true_nu_vtx_z <= 1026), True, False)\n",
    "    \n",
    "    # add beamline geometry weights\n",
    "    df = addAngles(df)\n",
    "    df['weightsNuMIGeo'] = df.apply( lambda x: numiBeamlineGeoWeights.calculateGeoWeight(x['nu_pdg'],x['nu_e'],x['thbeam']) , axis=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(nue.query('is_signal==True'))==len(nue.query(signal)))\n",
    "#print(len(nue.query('is_signal==False'))==len(nue.query(not_signal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(nue.query('is_signal==False'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(nue.query(not_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(mc_df):\n",
    "    \n",
    "    # bad weights \n",
    "    df.loc[ df['weightSplineTimesTune'] <= 0, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] == np.inf, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] > 60, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightSplineTimesTune']) == True, 'weightSplineTimesTune' ] = 1.\n",
    "    \n",
    "    # bad weights \n",
    "    df.loc[ df['weightTune'] <= 0, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] == np.inf, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] > 60, 'weightTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightTune']) == True, 'weightTune' ] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isPPFX: \n",
    "    for i,df in enumerate(mc_df):\n",
    "    \n",
    "        for ievt in range(df.shape[0]): \n",
    "\n",
    "\n",
    "            # check for NaNs separately        \n",
    "            if np.isnan(df['weightsPPFX'].iloc[ievt]).any() == True: \n",
    "                df['weightsPPFX'].iloc[ievt][ np.isnan(df['weightsPPFX'].iloc[ievt]) ] = 1000.\n",
    "\n",
    "            reweightCondition2 = ((df['weightsPPFX'].iloc[ievt] > 60000) | (df['weightsPPFX'].iloc[ievt] < 0)   |\n",
    "                                 (df['weightsPPFX'].iloc[ievt] == np.inf))\n",
    "            df['weightsPPFX'].iloc[ievt][ reweightCondition2 ] = 1000.\n",
    "\n",
    "            # if no variations exist for the event\n",
    "            if not list(df['weightsPPFX'].iloc[ievt]): \n",
    "                df['weightsPPFX'].iloc[ievt] = [1000 for k in range(600)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(mc_df):\n",
    "    universes = []\n",
    "\n",
    "    for evt in df[sys_genie_unisim].values: \n",
    "        universes.append( evt )\n",
    "            \n",
    "    # CLEAN GENIE UNISIM WEIGHTS & CREATE WEIGHTSGENIEUNISIM LIST \n",
    "    for v in sys_genie_unisim: \n",
    "        df.loc[ df[v] <= 0, v ] = 1.\n",
    "        df.loc[ df[v] == np.inf, v ] = 1.\n",
    "        df.loc[ df[v] > 60, v ] = 1.\n",
    "        df.loc[ np.isnan(df[v]) == True, v ] = 1.\n",
    "        \n",
    "    df['weightsGenieUnisim'] = universes\n",
    "    \n",
    "    for ievt in range(df.shape[0]):      \n",
    "        if np.isnan(df['weightsGenieUnisim'].iloc[ievt]).any() == True: \n",
    "            df['weightsGenieUnisim'].iloc[ievt][ np.isnan(df['weightsGenieUnisim'].iloc[ievt]) ] = 1.\n",
    "\n",
    "        reweightCondition = ((df['weightsGenieUnisim'].iloc[ievt] > 60) | (df['weightsGenieUnisim'].iloc[ievt] < 0)  | \n",
    "                                 (df['weightsGenieUnisim'].iloc[ievt] == np.inf) | (df['weightsGenieUnisim'].iloc[ievt] == np.nan))\n",
    "        df['weightsGenieUnisim'].iloc[ievt][ reweightCondition ] = 1.\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isGEANT: \n",
    "    for i,df in enumerate(mc_df):\n",
    "\n",
    "        print(i)\n",
    "        print(\"start:\",datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "        for ievt in range(df.shape[0]): #loop over the rows (events)\n",
    "\n",
    "            # RE-INTERACTION WEIGHTS\n",
    "\n",
    "            # turn everything into an array \n",
    "            if type(df['weightsReint'].iloc[ievt]) == list: \n",
    "                df['weightsReint'].iloc[ievt] = np.array(df['weightsReint'].iloc[ievt])\n",
    "\n",
    "            # check for NaNs separately \n",
    "            if np.isnan(df['weightsReint'].iloc[ievt]).any() == True: # if any of the weights are NaN\n",
    "                df['weightsReint'].iloc[ievt][ np.isnan(df['weightsReint'].iloc[ievt]) ] = 1000. \n",
    "\n",
    "\n",
    "            reweightCondition2 = ((df['weightsReint'].iloc[ievt] > 60000) | (df['weightsReint'].iloc[ievt] < 0)   |\n",
    "                                 (df['weightsReint'].iloc[ievt] == np.inf))\n",
    "            df['weightsReint'].iloc[ievt][ reweightCondition2 ] = 1000.\n",
    "\n",
    "\n",
    "            # if no variations exist for the event\n",
    "            if not list(df['weightsReint'].iloc[ievt]): \n",
    "                df['weightsReint'].loc[ievt] = np.array([1000 for k in range(1000)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isGENIE: \n",
    "    for i,df in enumerate(mc_df):\n",
    "\n",
    "        for ievt in range(df.shape[0]): \n",
    "\n",
    "            # check for NaNs separately        \n",
    "            if np.isnan(df['weightsGenie'].iloc[ievt]).any() == True: \n",
    "                df['weightsGenie'].iloc[ievt][ np.isnan(df['weightsGenie'].iloc[ievt]) ] = 1000.\n",
    "\n",
    "            reweightCondition2 = ((df['weightsGenie'].iloc[ievt] > 60000) | (df['weightsGenie'].iloc[ievt] < 0)   |\n",
    "                                 (df['weightsGenie'].iloc[ievt] == np.inf))\n",
    "            df['weightsGenie'].iloc[ievt][ reweightCondition2 ] = 1000.\n",
    "\n",
    "            # if no variations exist for the event\n",
    "            if not list(df['weightsGenie'].iloc[ievt]): \n",
    "                df['weightsGenie'].iloc[ievt] = [1000 for k in range(600)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(mc_df):\n",
    "\n",
    "    # get right order of magnitude for multiverses\n",
    "    if isPPFX: \n",
    "        df['weightsPPFX'] = df['weightsPPFX']/1000\n",
    "    if isGENIE: \n",
    "        df['weightsGenie'] = df['weightsGenie']/1000\n",
    "    if isGEANT: \n",
    "        df['weightsReint'] = df['weightsReint']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = overlay.query('swtrig_pre==1')\n",
    "dirt = dirt.query('swtrig_pre==1')\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue = nue.query('swtrig_pre==1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(nue.query('is_signal==True'))==len(nue.query(signal)))\n",
    "#print(len(nue.query('is_signal==False'))==len(nue.query(not_signal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POT Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(top)\n",
    "from top import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = pot_scale(ext, 'ext', ISRUN3, tune=tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pot_scale'] = [1 for x in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beamon_pot = parameters(ISRUN3)['beamon_pot'] \n",
    "\n",
    "overlay = pot_scale(overlay, 'overlay', ISRUN3, tune=tune)\n",
    "dirt = pot_scale(dirt, 'dirt', ISRUN3, tune=tune)\n",
    "\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue = pot_scale(nue, 'intrinsic', ISRUN3, tune=tune)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totweight_data scales to BEAMON\n",
    "\n",
    "overlay['totweight_data'] = overlay['pot_scale']*overlay['ppfx_cv']*overlay['weightSplineTimesTune']\n",
    "dirt['totweight_data'] = dirt['pot_scale']*dirt['ppfx_cv']*dirt['weightSplineTimesTune']\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue['totweight_data'] = nue['pot_scale']*nue['ppfx_cv']*nue['weightSplineTimesTune']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['totweight_data'] = np.nan\n",
    "ext['totweight_data'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nueCC_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nueCC events \n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    \n",
    "    print(\"# of nueCC in AV in overlay sample = \"+str(len(overlay.query(nueCC_query))))\n",
    "    len1 = len(overlay)\n",
    "    \n",
    "    idx = overlay.query(nueCC_query).index\n",
    "    overlay.drop(idx, inplace=True)\n",
    "    len2 = len(overlay) \n",
    "    print(\"# of nueCC in AV dropped in overlay = \"+str(len1-len2))\n",
    "    \n",
    "    overlay = pd.concat([overlay,nue], ignore_index=True)\n",
    "\n",
    "    # from here on out everything else should be the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SW trigger, combine overlay + dirt as MC \n",
    "mc = pd.concat([overlay.query('swtrig_pre==1'),dirt.query('swtrig_pre==1')], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infv = mc.query(in_fv_query)\n",
    "outfv = mc.query(out_fv_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that everything is accounted for \n",
    "print(len(mc)==len(infv)+len(outfv))\n",
    "\n",
    "if not (len(mc)==len(infv)+len(outfv)):\n",
    "    d = len(mc) - (len(infv)+len(outfv))\n",
    "    print(d)\n",
    "    \n",
    "     \n",
    "    m = pd.concat([infv, outfv]) \n",
    "    diff = np.setdiff1d(list(mc.index),list(m.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tot_signal_weighted = np.nansum(mc.query('is_signal==True')['totweight_data'])\n",
    "#print('total signal events in FV = '+ str(tot_signal_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"infv\": infv, \n",
    "    \"outfv\": outfv,\n",
    "    \"ext\": ext,\n",
    "    \"data\": data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flash time plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0.359 us shift between beam on & beam off hardware trigger \n",
    "\n",
    "overlay['flash_time'] = overlay['flash_time']  - 0.359\n",
    "dirt['flash_time'] = dirt['flash_time']  - 0.359\n",
    "ext['flash_time'] = ext['flash_time'] - 0.359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data, b_data, p_data = plt.hist(data['flash_time'], 50, range=[0, 25])\n",
    "data_bins = 0.5*(b_data[1:]+b_data[:-1])\n",
    "plt.close()\n",
    "\n",
    "x_err = [ (b_data[i+1]-b_data[i])/2 for i in range(len(b_data)-1) ]\n",
    "\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[2, 1])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 7))\n",
    "\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "    \n",
    "ax1.tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "ax2.tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "\n",
    "n = ax1.hist([ext['flash_time'], \n",
    "              overlay['flash_time'], \n",
    "              dirt['flash_time']], 50, range=[0, 25], stacked=True, \n",
    "            weights=[ ext['pot_scale'], \n",
    "                 overlay['ppfx_cv']*overlay['weightSplineTimesTune']*overlay['pot_scale'], \n",
    "                 dirt['ppfx_cv']*dirt['weightSplineTimesTune']*dirt['pot_scale']], \n",
    "         color=['navajowhite', 'limegreen', 'peru'], \n",
    "         label=['EXT', 'In Cryo MC', 'Dirt'])[0]\n",
    "\n",
    "ax1.errorbar(data_bins, n_data, yerr=np.sqrt(n_data), xerr=x_err, \n",
    "             color=\"black\", fmt='o', markersize=3, label='DATA')\n",
    "\n",
    "\n",
    "ax1.legend(fontsize=12)\n",
    "\n",
    "#ax2.yaxis.grid(linestyle=\"--\", color='black', alpha=0.7)\n",
    "ax2.axhline(1, color='black', lw=1, linestyle='--')\n",
    "ax2.set_ylim(0.8, 1.2)\n",
    "\n",
    "ax2.errorbar(data_bins, n_data/n[-1], \n",
    "             yerr=get_ratio_err(n_data, n[-1]), xerr=x_err, \n",
    "             color=\"black\", fmt='.')\n",
    "\n",
    "\n",
    "if ISRUN3: \n",
    "    ax1.set_ylabel('Events / 5$\\\\times10^{20}$ POT', fontsize=15)\n",
    "else: \n",
    "    ax1.set_ylabel('Events / 2$\\\\times10^{20}$ POT', fontsize=15)\n",
    "\n",
    "ax2.set_xlabel('Flash Time [$\\\\mu$s]', fontsize=15)\n",
    "\n",
    "ax1.set_xlim(0, 25)\n",
    "ax2.set_xlim(0, 25)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "ax1.set_title(\"\", fontsize=15)\n",
    "\n",
    "if ISRUN3: \n",
    "    if tune: \n",
    "        ax1.set_title(\"RHC Run 3: Flash Time (98% EXT Tune and 45% Dirt Tune)\", fontsize=15)\n",
    "        plt.savefig(parameters(ISRUN3)['plots_path']+\"RHCRUN3_flashtime_full_tune.pdf\", transparent=True, bbox_inches='tight')\n",
    "    else: \n",
    "        ax1.set_title(\"RHC Run 3: Flash Time (No Dirt/EXT Tune)\", fontsize=15)\n",
    "        plt.savefig(parameters(ISRUN3)['plots_path']+\"RHCRUN3_flashtime_full_notune.pdf\", transparent=True, bbox_inches='tight')       \n",
    "\n",
    "else: \n",
    "    if tune: \n",
    "        ax1.set_title(\"FHC Run 1: Flash Time (98% EXT Tune and 65% Dirt Tune)\", fontsize=15)\n",
    "        plt.savefig(parameters(ISRUN3)['plots_path']+\"FHCRUN1_flashtime_full_tune.pdf\", transparent=True, bbox_inches='tight')\n",
    "    else: \n",
    "        ax1.set_title(\"FHC Run 1: Flash Time (No Dirt/EXT Tune)\", fontsize=15)\n",
    "        plt.savefig(parameters(ISRUN3)['plots_path']+\"FHCRUN1_flashtime_full_notune.pdf\", transparent=True, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters(ISRUN3)['plots_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data, b_data, p_data = plt.hist(data['flash_time'], 9, range=[1, 5.5])\n",
    "data_bins = 0.5*(b_data[1:]+b_data[:-1])\n",
    "plt.close()\n",
    "\n",
    "x_err = [ (b_data[i+1]-b_data[i])/2 for i in range(len(b_data)-1) ]\n",
    "\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[2, 1])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 7))\n",
    "\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "    \n",
    "ax1.tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "ax2.tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "\n",
    "n = ax1.hist([ext['flash_time'], \n",
    "              overlay['flash_time'], \n",
    "              dirt['flash_time']], 9, range=[1, 5.5], stacked=True, \n",
    "            weights=[ ext['pot_scale'], \n",
    "                 overlay['ppfx_cv']*overlay['weightSplineTimesTune']*overlay['pot_scale'], \n",
    "                 dirt['ppfx_cv']*dirt['weightSplineTimesTune']*dirt['pot_scale']], \n",
    "         color=['navajowhite', 'limegreen', 'peru'], \n",
    "         label=['EXT', 'In Cryo MC', 'Dirt'])[0]\n",
    "\n",
    "\n",
    "\n",
    "ax1.errorbar(data_bins, n_data, yerr=np.sqrt(n_data), xerr=x_err, \n",
    "             color=\"black\", fmt='o', markersize=3, label='DATA')\n",
    "\n",
    "ax1.legend(fontsize=13)\n",
    "\n",
    "#ax2.yaxis.grid(linestyle=\"--\", color='black', alpha=0.7)\n",
    "ax2.axhline(1.0, color='black', lw=1, linestyle='--')\n",
    "#ax2.axhline(np.average(n_data/n[-1]), color='black', lw=1, linestyle='--', \n",
    "#            label='Average = '+str( round(np.average((n_data/n[-1])), 3) ))\n",
    "ax2.set_ylim(0.8, 1.2)\n",
    "\n",
    "ax2.errorbar(data_bins, n_data/n[-1], \n",
    "             yerr=get_ratio_err(n_data, n[-1]), xerr=x_err, \n",
    "             color=\"black\", fmt='.', label='Average = '+str( round(np.average((n_data/n[-1])), 2)))\n",
    "\n",
    "ax2.legend(fontsize=13)\n",
    "\n",
    "if ISRUN3: \n",
    "    ax1.set_ylabel('Events / 5$\\\\times10^{20}$ POT', fontsize=15)\n",
    "else: \n",
    "    ax1.set_ylabel('Events / 2$\\\\times10^{20}$ POT', fontsize=15)\n",
    "\n",
    "ax2.set_xlabel('Flash Time [$\\\\mu$s]', fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "ax1.set_xlim(1, 5.5)\n",
    "ax2.set_xlim(1, 5.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "       \n",
    "\n",
    "if ISRUN3: \n",
    "    if tune: \n",
    "        ax1.set_title(\"RHC Run 3: Flash Time (98% EXT Tune and 45% Dirt Tune)\", fontsize=15)\n",
    "        plt.savefig(parameters(ISRUN3)['plots_path']+\"RHCRUN3_flashtime_ext_tune.pdf\", transparent=True, bbox_inches='tight')\n",
    "    else: \n",
    "        ax1.set_title(\"RHC Run 3: Flash Time (No Dirt/EXT Tune)\", fontsize=15)\n",
    "        plt.savefig(parameters(ISRUN3)['plots_path']+\"RHCRUN3_flashtime_ext_notune.pdf\", transparent=True, bbox_inches='tight')       \n",
    "\n",
    "else: \n",
    "    if tune: \n",
    "        ax1.set_title(\"FHC Run 1: Flash Time (98% EXT Tune and 65% Dirt Tune)\", fontsize=15)\n",
    "        plt.savefig(parameters(ISRUN3)['plots_path']+\"FHCRUN1_flashtime_ext_tune.pdf\", transparent=True, bbox_inches='tight')\n",
    "    else: \n",
    "        ax1.set_title(\"FHC Run 1: Flash Time (No Dirt/EXT Tune)\", fontsize=15)\n",
    "        plt.savefig(parameters(ISRUN3)['plots_path']+\"FHCRUN1_flashtime_ext_notune.pdf\", transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data, b_data, p_data = plt.hist(data['flash_time'], 1, range=[5.64, 15.44])\n",
    "data_bins = 0.5*(b_data[1:]+b_data[:-1])\n",
    "plt.close()\n",
    "\n",
    "x_err = [ (b_data[i+1]-b_data[i])/2 for i in range(len(b_data)-1) ]\n",
    "\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[2, 1])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 7))\n",
    "\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "    \n",
    "ax1.tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "ax2.tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "\n",
    "n = ax1.hist([ext['flash_time'], overlay['flash_time'], dirt['flash_time']], 1, range=[5.64, 15.44], stacked=True, \n",
    "            weights=[ ext['pot_scale'], \n",
    "                 overlay['ppfx_cv']*overlay['weightSplineTimesTune']*overlay['pot_scale'], \n",
    "                 dirt['ppfx_cv']*dirt['weightSplineTimesTune']*dirt['pot_scale']], \n",
    "         color=['navajowhite', 'limegreen', 'peru'], \n",
    "         label=['EXT', 'In Cryo MC', 'Dirt'])[0]\n",
    "\n",
    "\n",
    "ax1.errorbar(data_bins, n_data, yerr=np.sqrt(n_data), xerr=x_err, \n",
    "             color=\"black\", fmt='o', markersize=3, label='DATA')\n",
    "\n",
    "ax1.legend(fontsize=13)\n",
    "\n",
    "#ax2.yaxis.grid(linestyle=\"--\", color='black', alpha=0.7)\n",
    "ax2.axhline(1.0, color='black', lw=1, linestyle='--')\n",
    "ax2.set_ylim(0.8, 1.2)\n",
    "\n",
    "ax2.errorbar(data_bins, n_data/n[-1], \n",
    "             yerr=get_ratio_err(n_data, n[-1]), xerr=x_err, \n",
    "             color=\"black\", fmt='.', label='Average = '+str(round((n_data/n[-1])[0], 2)))\n",
    "\n",
    "if ISRUN3: \n",
    "    ax1.set_ylabel('Events / 5$\\\\times10^{20}$ POT', fontsize=15)\n",
    "else: \n",
    "    ax1.set_ylabel('Events / 2$\\\\times10^{20}$ POT', fontsize=15)\n",
    "    \n",
    "ax2.set_xlabel('Flash Time [$\\\\mu$s]', fontsize=15)\n",
    "\n",
    "\n",
    "ax2.legend(fontsize=13)\n",
    "ax1.set_xlim(5.64, 15.44)\n",
    "ax2.set_xlim(5.64, 15.44)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "if ISRUN3: \n",
    "    if tune: \n",
    "        ax1.set_title(\"RHC Run 3: Flash Time (98% EXT Tune and 45% Dirt Tune)\", fontsize=15)\n",
    "        plt.savefig(parameters(ISRUN3)['plots_path']+\"RHCRUN3_window_tune.pdf\", transparent=True, bbox_inches='tight')\n",
    "    else: \n",
    "        ax1.set_title(\"RHC Run 3: Flash Time (No Dirt/EXT Tune)\", fontsize=15)\n",
    "        plt.savefig(parameters(ISRUN3)['plots_path']+\"RHCRUN3_window_notune.pdf\", transparent=True, bbox_inches='tight')       \n",
    "\n",
    "else: \n",
    "    if tune: \n",
    "        ax1.set_title(\"FHC Run 1: Flash Time (98% EXT Tune and 65% Dirt Tune)\", fontsize=15)\n",
    "        plt.savefig(parameters(ISRUN3)['plots_path']+\"FHCRUN1_window_tune.pdf\", transparent=True, bbox_inches='tight')\n",
    "    else: \n",
    "        ax1.set_title(\"FHC Run 1: Flash Time (No Dirt/EXT Tune)\", fontsize=15)\n",
    "        plt.savefig(parameters(ISRUN3)['plots_path']+\"FHCRUN1_window_notune.pdf\", transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1.hist([ext['flash_time'], \n",
    "              overlay['flash_time'], \n",
    "              dirt['flash_time']], 50, range=[0, 25], stacked=True, \n",
    "            weights=[ ext['pot_scale'], \n",
    "                 overlay['ppfx_cv']*overlay['weightSplineTimesTune']*overlay['pot_scale'], \n",
    "                 dirt['ppfx_cv']*dirt['weightSplineTimesTune']*dirt['pot_scale']], \n",
    "         color=['navajowhite', 'limegreen', 'peru'], \n",
    "         label=['EXT', 'In Cryo MC', 'Dirt'])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Selection Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvar = 'flash_time'\n",
    "bins = [x*0.5 for x in range(51)]\n",
    "x_label = 'Flash Time'\n",
    "\n",
    "#xvar = 'nslice'\n",
    "#bins = [-0.5, 0.5, 1.5]\n",
    "#x_label = \"Pandora Slice ID\"\n",
    "\n",
    "#xvar = \"reco_nu_vtx_sce_x\"\n",
    "#bins = [x*10 for x in range(27)]\n",
    "#x_label = 'Reconstructed Interaction Vertex (X) [cm]'\n",
    "\n",
    "#xvar = \"reco_nu_vtx_sce_y\"\n",
    "#bins = [-120, -110, -100, -90, -80, -70, -60, -50, -40, -30, -20, -10, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120]\n",
    "#x_label = 'Reconstructed Interaction Vertex (Y) [cm]'\n",
    "\n",
    "#xvar = 'reco_nu_vtx_sce_z'\n",
    "#bins = [x*40 for x in range(27)]\n",
    "#x_label = 'Reconstructed Interaction Vertex (Z) [cm]'\n",
    "\n",
    "#xvar = 'contained_fraction'\n",
    "#bins = [0, .10, .20, .30, .40, .50, .60, .70, .80, .90, 1]\n",
    "#x_label = \"Contained Fraction\"\n",
    "\n",
    "true_var = ''\n",
    "xlow = bins[0]\n",
    "xhigh = bins[-1]\n",
    "\n",
    "q = \"\"#\"nslice==1\"# and \"+reco_in_fv_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv_total = plot_mc(xvar, bins, xlow, xhigh, q, datasets, ISRUN3, norm='data')['CV']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPFX, GENIE, GEANT4 multisims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uncertainty_functions \n",
    "importlib.reload(uncertainty_functions)\n",
    "from uncertainty_functions import plotSysVariations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay['weightsPPFX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ncv, geant4_variations = plotSysVariations(true_var, xvar, bins, xlow, xhigh, '', datasets, 'weightsReint', 1000, \n",
    "#                                         ISRUN3, plot=False, background_subtraction=False)\n",
    "\n",
    "print(\"start:\",datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "ncv, ppfx_variations = plotSysVariations(xvar, true_var, bins, xlow, xhigh, q, datasets, \n",
    "                                           'weightsPPFX', 600, ISRUN3, plot=True)\n",
    "\n",
    "#ncv, genie_variations = plotSysVariations(xvar, true_var, bins, xlow, xhigh, q, datasets, 'weightsGenie', 600, \n",
    "#                                         ISRUN3, plot=True, background_subtraction=False)\n",
    "\n",
    "#ncv, geant4_variations = plotSysVariations(xvar, true_var, bins, xlow, xhigh, q, datasets, 'weightsReint', 1000, \n",
    "#                                         ISRUN3, plot=True, background_subtraction=False)\n",
    "\n",
    "print(\"end:\",datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = calcCov(xvar, bins, ncv, ncv_total, ppfx_variations, plot=False, save=False,isrun3=ISRUN3)['fractional_uncertainty']\n",
    "frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = calcCov(xvar, bins, ncv, ncv_total, ppfx_variations, plot=False, save=False,isrun3=ISRUN3)['cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENIE unisims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the tune weight out of everything except SCC variations\n",
    "# don't divide the tune weight out of SCC variations \n",
    "\n",
    "genie_unisim_variations = ['RPA', \n",
    "                           'CCMEC', 'AxFFCCQE', 'VecFFCCQE', 'DecayAngMEC', 'ThetaDelta2Npi', 'ThetaDelta2NRad', \n",
    "                          'NormCCCOH', 'NormNCCOH', \n",
    "                          'xsr_scc_Fv3', 'xsr_scc_Fa3']\n",
    "\n",
    "\n",
    "genie_unisim_cov = {}\n",
    "\n",
    "\n",
    "for knob in genie_unisim_variations: \n",
    "    \n",
    "    if knob == 'RPA': \n",
    "        idx = [sys_genie_unisim.index('knobRPAup'), sys_genie_unisim.index('knobRPAdn')]\n",
    "    \n",
    "    else: \n",
    "        idx = [sys_genie_unisim.index('knob'+knob+'up')]\n",
    "    \n",
    "    ncv_nu, variations = plotSysVariations(xvar, true_var, bins, xlow, xhigh, q, datasets, 'weightsGenieUnisim', \n",
    "                                        idx, ISRUN3, plot=True, axis_label='Reco '+x_label, \n",
    "                                        pot=str(beamon_pot)+\" POT\", \n",
    "                                        background_subtraction=False, title=knob)\n",
    "    \n",
    "    # calc covariance \n",
    "    genie_unisim_cov[knob] = calcCov(xvar, bins, ncv_nu, ncv_total, variations, save=False, \n",
    "                    axis_label='Reco '+x_label, pot=str(beamon_pot)+\" POT\", isrun3=ISRUN3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total covariance, correlation, & uncertainty \n",
    "\n",
    "cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "cor = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for variation in genie_unisim_cov.keys(): \n",
    "    \n",
    "    for i in range(len(bins)-1): \n",
    "        for j in range(len(bins)-1):\n",
    "            \n",
    "            cov[i][j] += genie_unisim_cov[variation]['cov'][i][j]\n",
    "            frac_cov[i][j] += genie_unisim_cov[variation]['frac_cov'][i][j] \n",
    "\n",
    "            \n",
    "for i in range(len(bins)-1): \n",
    "    for j in range(len(bins)-1):\n",
    "        \n",
    "        if np.sqrt(cov[i][i])*np.sqrt(cov[j][j]) != 0: \n",
    "                cor[i][j] = cov[i][j] / (np.sqrt(cov[i][i])*np.sqrt(cov[j][j]))\n",
    "        \n",
    "            \n",
    "genie_unisim_dict = {\n",
    "    'cov' : cov, \n",
    "    'frac_cov' : frac_cov,\n",
    "    'cor' : cor,\n",
    "    'fractional_uncertainty' : np.sqrt(np.diag(frac_cov))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### beamline geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered by beamline variation run number\n",
    "# [+1sigma run #, -1sigma run #]\n",
    "\n",
    "beamline_runs = {\n",
    "    'HornCurrent' : [1, 2], \n",
    "    'xHorn1' : [3, 4], \n",
    "    'yHorn1' : [5, 6], \n",
    "    'BeamSpotSize' : [7, 8], \n",
    "    'xHorn2' : [9, 10], \n",
    "    'yHorn2' : [11, 12], \n",
    "    'WaterOnHorns' : [13, 14], \n",
    "    'xBeamShift' : [15, 16], \n",
    "    'yBeamShift' : [17, 18], \n",
    "    'zTargetPosition' : [19, 20]    \n",
    "}\n",
    "\n",
    "beamline_cov = {}\n",
    "\n",
    "# index in weightsNuMIGeo are offset by -1\n",
    "\n",
    "for variation in beamline_runs.keys(): \n",
    "    \n",
    "    idx = [i-1 for i in beamline_runs[variation]]\n",
    "    print(idx)\n",
    "    \n",
    "    ncv_nu, beamline_variations = plotSysVariations(xvar, true_var, bins, xlow, xhigh, q, datasets, 'weightsNuMIGeo', \n",
    "                                                 idx, ISRUN3, plot=True, \n",
    "                                                 axis_label='Reco '+x_label, pot=str(beamon_pot)+\" POT\", \n",
    "                                                  background_subtraction=False)\n",
    "    \n",
    "    # calc covariance \n",
    "    beamline_cov[variation] = calcCov(xvar, bins, ncv_nu, ncv_total, \n",
    "                                      beamline_variations, save=False, isrun3=ISRUN3)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total covariance, correlation, & uncertainty \n",
    "\n",
    "cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "cor = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for variation in beamline_cov.keys(): \n",
    "    \n",
    "    for i in range(len(bins)-1): \n",
    "        for j in range(len(bins)-1):\n",
    "            \n",
    "            cov[i][j] += beamline_cov[variation]['cov'][i][j]\n",
    "            frac_cov[i][j] += beamline_cov[variation]['frac_cov'][i][j] \n",
    "\n",
    "            \n",
    "for i in range(len(bins)-1): \n",
    "    for j in range(len(bins)-1):\n",
    "        \n",
    "        if np.sqrt(cov[i][i])*np.sqrt(cov[j][j]) != 0: \n",
    "                cor[i][j] = cov[i][j] / (np.sqrt(cov[i][i])*np.sqrt(cov[j][j]))\n",
    "            \n",
    "beamline_dict = {\n",
    "    'cov' : cov, \n",
    "    'frac_cov' : frac_cov,\n",
    "    'cor' : cor,\n",
    "    'fractional_uncertainty' : np.sqrt(np.diag(frac_cov))\n",
    "} \n",
    "\n",
    "beamline_dict['fractional_uncertainty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stat uncertainty, POT counting, dirt uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Make sure to update query!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't include EXT uncertainty \n",
    "\n",
    "print(\"Make sure to update query!\")\n",
    "\n",
    "mc_stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "mc_frac_stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "\n",
    "ncv = pd.concat([datasets['infv'].copy().query(q), \n",
    "                         datasets['outfv'].copy().query(q)], \n",
    "                ignore_index=True) \n",
    "\n",
    "    \n",
    "for i in range(len(bins)-1):\n",
    "\n",
    "    if i==len(bins)-2: \n",
    "        bin_query = xvar+' >= '+str(bins[i])+' and '+xvar+' <= '+str(bins[i+1])\n",
    "    else: \n",
    "        bin_query = xvar+' >= '+str(bins[i])+' and '+xvar+' < '+str(bins[i+1])\n",
    "        \n",
    "    mc_stat_cov[i][i] = sum(ncv.query(bin_query).totweight_data ** 2) \n",
    "    mc_frac_stat_cov[i][i] = mc_stat_cov[i][i]/ ncv_total[i]**2 \n",
    "    \n",
    "    bin_query = ''\n",
    "    \n",
    "mc_stat_percent_error = np.sqrt(np.diag(mc_frac_stat_cov))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAKE SURE TO UPDATE QUERY!\")\n",
    "\n",
    "# selected EXT uncertainty \n",
    "selected_ext = plt.hist(datasets['ext'].copy()[xvar].query(q)[xvar], \n",
    "                        bins, \n",
    "                        weights=datasets['ext'].copy().query(q)['pot_scale'], \n",
    "                        color='gainsboro')[0]\n",
    "plt.show()\n",
    "#selected_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take fractional with respect to the full event rate\n",
    "\n",
    "ext_frac_stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "ext_stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for i in range(len(bins)-1): \n",
    "    \n",
    "    if selected_ext[i] != 0: \n",
    "        ext_stat_cov[i][i] = selected_ext[i]\n",
    "        ext_frac_stat_cov[i][i] = selected_ext[i]/(ncv_total[i]**2)\n",
    "\n",
    "ext_stat_percent_error = np.sqrt(np.diag(ext_frac_stat_cov))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" make sure to update query !! \")\n",
    "\n",
    "\n",
    "selected_dirt = plt.hist(datasets['outfv'].copy().query('isDirt==1')[xvar], \n",
    "                         bins, \n",
    "                        weights=datasets['outfv'].copy().query('isDirt==1')['pot_scale'], \n",
    "                         color='orchid')[0]\n",
    "\n",
    "dirt_uncertainty = dirt_unisim(xvar, bins, ncv_total, selected_dirt, 1.0, ISRUN3, plot=True, \n",
    "                               x_label=None, title=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pot counting\n",
    "pot_dict = pot_unisims(xvar, ncv_total, bins, 0.02, ISRUN3, plot=True, x_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sf)\n",
    "from selection_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance dictionary\n",
    "\n",
    "if ISRUN3: \n",
    "    with open('covariances/rhc_'+xvar+'_123022.json') as f_cov:\n",
    "        cov_dict = json.load(f_cov)\n",
    "    \n",
    "else: \n",
    "    with open('covariances/fhc_'+xvar+'_123022.json') as f_cov:\n",
    "        cov_dict = json.load(f_cov)\n",
    "        \n",
    "# need to compute beam-on stat uncertainty \n",
    "\n",
    "cov_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## total covariance \n",
    "tot_sim_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for key in cov_dict.keys(): \n",
    "    tot_sim_cov = [ [x+y for x,y in zip(a,b)] for a,b in zip(tot_sim_cov, cov_dict[key]) ]\n",
    "    \n",
    "cov_dict['total'] = tot_sim_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total fractional covariance \n",
    "tot_sim_frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for i in range(len(bins)-1): \n",
    "    for j in range(len(bins)-1): \n",
    "        tot_sim_frac_cov[i][j] = tot_sim_cov[i][j]/(ncv_total[i]*ncv_total[j])\n",
    "        \n",
    "np.sqrt(np.diagonal(tot_sim_frac_cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_unc_dict = {} # on the simulation only \n",
    "\n",
    "for key in cov_dict.keys():\n",
    "    frac_unc_dict[key] = [ 0 for x in range(len(bins)-1) ]\n",
    "    \n",
    "    for i in range(len(bins)-1): \n",
    "        frac_unc_dict[key][i] = np.sqrt(cov_dict[key][i][i]/(ncv_total[i]*ncv_total[i]))\n",
    "    \n",
    "np.array(frac_unc_dict['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the chi2 \n",
    "\n",
    "if q == \"\":\n",
    "    selected_data = plt.hist(datasets['data'].copy()[xvar], bins)[0]\n",
    "    plt.close()\n",
    "    \n",
    "else: \n",
    "    selected_data = plt.hist(datasets['data'].copy().query(q)[xvar], bins)[0]\n",
    "    plt.close()\n",
    "\n",
    "#  make sure to include the beam on stat covariance! \n",
    "\n",
    "beamon_frac_stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "for i in range(len(bins)-1): \n",
    "    if selected_data[i] != 0: \n",
    "        beamon_frac_stat_cov[i][i] = selected_data[i]/(selected_data[i]**2)\n",
    "\n",
    "tot_cov = np.array(tot_sim_frac_cov)+np.array(beamon_frac_stat_cov)\n",
    "\n",
    "for i in range(len(bins)-1): \n",
    "     for j in range(len(bins)-1): \n",
    "            tot_cov[i][j] = tot_cov[i][j] * (ncv_total[i] * ncv_total[j])\n",
    "\n",
    "tot_inverse_cov = np.linalg.inv(tot_cov)\n",
    "\n",
    "## check \n",
    "plt.pcolor(bins, bins, np.matmul(tot_cov, tot_inverse_cov), cmap='OrRd', edgecolors='k')\n",
    "plt.xlim(xlow,xhigh)\n",
    "plt.ylim(xlow,xhigh)\n",
    "cbar = plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "chi2 = 0\n",
    "\n",
    "for i in range(len(bins)-1):  \n",
    "    for j in range(len(bins)-1):  \n",
    "            chi2 = chi2  + ( (ncv_total[i]-selected_data[i])*tot_inverse_cov[i][j]*(ncv_total[j]-selected_data[j]) )\n",
    "chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"make sure to update save label!\")\n",
    "print(\"save label = \", save_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if xvar==\"nslice\": \n",
    "    d = plot_data(xvar, bins, xlow, xhigh, q, datasets, ISRUN3,\n",
    "                  save=True, save_label=save_label, x_label=x_label, ncol=3, \n",
    "                  y_label=beamon_pot_str, x_ticks=[0,1],ymax=900000, \n",
    "                  sys=frac_unc_dict['total'], \n",
    "                  text=chi2_label+\"\\n$\\\\chi^{2}$/n = \"+str(round(chi2, 1))+\"/\"+str(len(bins)-1),  \n",
    "                  xtext=1.4, ytext=400000)\n",
    "\n",
    "\n",
    "else: \n",
    "    \n",
    "    d = plot_data(xvar, bins, xlow, xhigh, q, datasets, ISRUN3,\n",
    "                      save=True, save_label=save_label, x_label=x_label, ncol=3, \n",
    "                      y_label=beamon_pot_str, \n",
    "                      sys=frac_unc_dict['total'], \n",
    "                      text=chi2_label+\"\\n$\\\\chi^{2}$/n = \"+str(round(chi2, 1))+\"/\"+str(len(bins)-1),  \n",
    "                      xtext=245, ytext=7000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ISRUN3: \n",
    "    with open('covariances/fhc_'+xvar+\"_\"+date.today().strftime(\"%m%d%y\")+\".json\", 'w') as f:\n",
    "        json.dump(cov_dict, f)\n",
    "\n",
    "elif ISRUN3: \n",
    "    with open('covariances/rhc_'+xvar+\"_\"+date.today().strftime(\"%m%d%y\")+\".json\", 'w') as f:\n",
    "        json.dump(cov_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
