{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot event rate variations, fractional uncertainties, & data/MC comparisons \n",
    "# for all sources of systematic error\n",
    "# also consider potential NuMI oscillations on the event rate \n",
    "# make sure to update the plots_path here & in backend function scripts before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, 'backend_functions')\n",
    "\n",
    "import selection_functions as sf\n",
    "\n",
    "import importlib\n",
    "\n",
    "import uproot\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import awkward\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import ROOT\n",
    "\n",
    "import top \n",
    "from top import *\n",
    "\n",
    "import uncertainty_functions \n",
    "from uncertainty_functions import *\n",
    "\n",
    "import xsec_functions \n",
    "from xsec_functions import smear_matrix\n",
    "\n",
    "from ROOT import TH1D, TH2D, TDirectory, TH1F, TH2F\n",
    "\n",
    "from selection_functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"date and time:\",date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NuMIGeoWeights\n",
    "importlib.reload(NuMIGeoWeights)\n",
    "\n",
    "# the default option is FHC, RHC needs different arguments\n",
    "numiBeamlineGeoWeights = NuMIGeoWeights.NuMIGeoWeights() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NuMIDetSys\n",
    "importlib.reload(NuMIDetSys)\n",
    "\n",
    "NuMIDetSysWeights = NuMIDetSys.NuMIDetSys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing Run3??\n",
    "ISRUN3 = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nue intrinsic? \n",
    "NUE_INTRINSIC = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_path = parameters(ISRUN3)['plots_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POT normalization factors\n",
    "\n",
    "\n",
    "overlay_pot =  parameters(ISRUN3)['overlay_pot'] #2.33652E21  # v7       \n",
    "dirt_pot = parameters(ISRUN3)['dirt_pot'] # david's file\n",
    "beamon_pot = parameters(ISRUN3)['beamon_pot'] # v5\n",
    "    \n",
    "#proj_pot = parameters(ISRUN3)['proj_pot'] # FHC Runs 1-5: 9.23E20, FHC Runs 1-3: 4.125E20 \n",
    "\n",
    "beamon_ntrig =  parameters(ISRUN3)['beamon_ntrig'] # v5 (EA9CNT_wcut)\n",
    "beamoff_ntrig = parameters(ISRUN3)['beamoff_ntrig']  # v5 (EXT_NUMIwin_FEMBeamTriggerAlgo)\n",
    "    \n",
    "if NUE_INTRINSIC: \n",
    "    nue_intrinsic_pot = parameters(ISRUN3)['intrinsic_pot'] # v7\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = \"nuselection\"\n",
    "tree = \"NeutrinoSelectionFilter\"\n",
    "\n",
    "DATA = \"\"\n",
    "EXT = \"\"\n",
    "OVRLY  = \"\"\n",
    "DRT = \"\"\n",
    "NUE = \"\"\n",
    "\n",
    "\n",
    "# slimmed with opening angle \n",
    "path = parameters(ISRUN3)['cv_ntuple_path']\n",
    "print('path = ', path)\n",
    "\n",
    "if not ISRUN3: \n",
    "    \n",
    "    # Run 1 FHC \n",
    "    OVRLY = 'neutrinoselection_filt_run1_overlay_v7'\n",
    "    EXT = 'neutrinoselection_filt_run1_beamoff_v5'\n",
    "    DATA = 'neutrinoselection_filt_run1_beamon_beamgood_v5'\n",
    "    DRT = 'prodgenie_numi_uboone_overlay_dirt_fhc_mcc9_run1_v28_all_snapshot'\n",
    "    \n",
    "    if NUE_INTRINSIC: \n",
    "        NUE = 'neutrinoselection_filt_run1_overlay_intrinsic_v7'\n",
    "\n",
    "else: \n",
    "    \n",
    "    # Run 3 RHC\n",
    "    OVRLY = 'neutrinoselection_filt_run3b_overlay_v7'\n",
    "    DATA = 'neutrinoselection_filt_run3b_beamon_beamgood_v5'\n",
    "    EXT = 'neutrinoselection_filt_run3b_beamoff_v5'\n",
    "    DRT = 'neutrinoselection_filt_run3b_dirt_overlay_v6'\n",
    "    \n",
    "    if NUE_INTRINSIC: \n",
    "        NUE = 'neutrinoselection_filt_run3b_overlay_intrinsic_v7'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = uproot.open(path+OVRLY+\".root\")[fold][tree]\n",
    "data = uproot.open(path+DATA+\".root\")[fold][tree]\n",
    "ext = uproot.open(path+EXT+\".root\")[fold][tree]\n",
    "dirt = uproot.open(path+DRT+\".root\")[fold][tree]  \n",
    "\n",
    "uproot_v = [overlay,data,ext,dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue = uproot.open(path+NUE+\".root\")[fold][tree]\n",
    "    uproot_v.append(nue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    \"trk_score_v\", \n",
    "    \"shr_tkfit_dedx_Y\", \n",
    "    \"n_tracks_contained\", \n",
    "    \"NeutrinoEnergy2\",\n",
    "    \"run\",\"sub\",\"evt\",\n",
    "    \"reco_nu_vtx_sce_x\",\"reco_nu_vtx_sce_y\",\"reco_nu_vtx_sce_z\",\n",
    "    \"shrsubclusters0\",\"shrsubclusters1\",\"shrsubclusters2\",\n",
    "    \"trkshrhitdist2\",\n",
    "    \"n_showers_contained\", \n",
    "    \"shr_phi\", \"trk_phi\", \"trk_theta\",\n",
    "    \"shr_score\", \n",
    "    \"trk_energy\", \n",
    "    \"tksh_distance\", \"tksh_angle\",\n",
    "    \"shr_energy_tot_cali\", \"shr_energy_cali\", \n",
    "    \"nslice\", \n",
    "    \"contained_fraction\",\n",
    "    \"shrmoliereavg\", \"shr_px\", \"shr_py\", \"shr_pz\"\n",
    "]\n",
    "\n",
    "# MC only variables\n",
    "mc_var = [\"nu_pdg\", \"shr_theta\", \"true_e_visible\", \"ccnc\", \n",
    "          \"nproton\", \"nu_purity_from_pfp\", \"nu_e\", \"npi0\", \"npion\",\n",
    "          \"true_nu_vtx_x\", \"true_nu_vtx_y\" , \"true_nu_vtx_z\", \n",
    "          \"weightTune\", \"weightSpline\", \"weightSplineTimesTune\", \n",
    "          \"true_nu_px\", \"true_nu_py\", \"true_nu_pz\", \n",
    "          \"elec_e\", \"proton_e\", \"mc_px\", \"mc_py\", \"mc_pz\", \"elec_px\", \"elec_py\", \"elec_pz\", \n",
    "          \"swtrig_pre\", \"ppfx_cv\", \"mc_pdg\", \"opening_angle\"]\n",
    "\n",
    "sys_genie = [\"weightsGenie\", \"weightsReint\", \n",
    "             \"knobRPAup\", \"knobRPAdn\", \n",
    "             \"knobCCMECup\", \"knobCCMECdn\", \n",
    "             \"knobAxFFCCQEup\", \"knobAxFFCCQEdn\", \n",
    "             \"knobVecFFCCQEup\", \"knobVecFFCCQEdn\", \n",
    "             \"knobDecayAngMECup\", \"knobDecayAngMECdn\", \n",
    "             \"knobThetaDelta2Npiup\", \"knobThetaDelta2Npidn\", \n",
    "             \"knobThetaDelta2NRadup\", \"knobThetaDelta2NRaddn\", \n",
    "             #\"knobRPA_CCQE_Reducedup\", \"knobRPA_CCQE_Reduceddn\", \n",
    "             \"knobNormCCCOHup\", \"knobNormCCCOHdn\", \n",
    "             \"knobNormNCCOHup\", \"knobNormNCCOHdn\",    \n",
    "             \"knobxsr_scc_Fv3up\", \"knobxsr_scc_Fv3dn\", \n",
    "             \"knobxsr_scc_Fa3up\", \"knobxsr_scc_Fa3dn\"]\n",
    "\n",
    "sys_flux = ['weightsPPFX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = overlay.pandas.df(variables + mc_var + sys_genie + sys_flux, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt = dirt.pandas.df(variables + mc_var + sys_genie[:-4] + sys_flux, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUE_INTRINSIC: \n",
    "    nue = nue.pandas.df(variables + mc_var + sys_genie + sys_flux, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.pandas.df(variables, flatten=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = ext.pandas.df(variables, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in mc_var+sys_genie+sys_flux: \n",
    "    data[var] = np.nan\n",
    "    ext[var] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is dirt bool\n",
    "\n",
    "overlay['isDirt'] = False\n",
    "dirt['isDirt'] = True\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue['isDirt'] = False\n",
    "    \n",
    "data['isDirt'] = np.nan\n",
    "ext['isDirt'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to get the LLR-PID value for the \"track candidate\" \n",
    "# (proton for nue selection, muon for numu)\n",
    "# can be done for any variable\n",
    "# code from Giuseppe!\n",
    "#LLR-PID : log likelihood ratio particle ID \n",
    "\n",
    "df_v = [overlay,data,ext,dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    df_v.append(nue)\n",
    "    \n",
    "for i,df in enumerate(df_v):\n",
    "    up = uproot_v[i]\n",
    "    trk_llr_pid_v = up.array('trk_llr_pid_score_v')\n",
    "    trk_id = up.array('trk_id')-1 # I think we need this -1 to get the right result\n",
    "    trk_llr_pid_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_llr_pid_v,trk_id)])\n",
    "    df['trkpid'] = trk_llr_pid_v_sel\n",
    "    df['subcluster'] = df['shrsubclusters0'] + df['shrsubclusters1'] + df['shrsubclusters2']\n",
    "    \n",
    "    df['NeutrinoEnergy2_GeV'] = df['NeutrinoEnergy2']/1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_df = [overlay, dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    mc_df.append(nue)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(mc_df):\n",
    "    \n",
    "    # is signal bool \n",
    "    df['is_signal'] = np.where((df.swtrig_pre == 1) & (df.nu_purity_from_pfp>0.5)\n",
    "                             & (df.nu_pdg==12) & (df.ccnc==0) & (df.nproton>0) & (df.npion==0) & (df.npi0==0)\n",
    "                             & (10 <= df.true_nu_vtx_x) & (df.true_nu_vtx_x <= 246)\n",
    "                             & (-106 <= df.true_nu_vtx_y) & (df.true_nu_vtx_y <= 106)\n",
    "                             & (10 <= df.true_nu_vtx_z) & (df.true_nu_vtx_z <= 1026), True, False)\n",
    "    \n",
    "    # Add truth level theta & phi angles (detector & beam coordinates)\n",
    "    df = addAngles(df)\n",
    "    \n",
    "    \n",
    "    df['weightsPPFX'] = df['weightsPPFX']/1000\n",
    "    df['weightsReint'] = df['weightsReint']/1000\n",
    "    df['weightsGenie'] = df['weightsGenie']/1000\n",
    "    \n",
    "    \n",
    "    # add beamline geometry weights\n",
    "    df['weightsNuMIGeo'] = df.apply( lambda x: numiBeamlineGeoWeights.calculateGeoWeight(x['nu_pdg'],x['nu_e'],x['thbeam']) , axis=1)\n",
    "    \n",
    "    \n",
    "    # add genie unisim weights \n",
    "    if i==1: \n",
    "        universes = []\n",
    "        for evt in df[sys_genie[2:-4]].values: \n",
    "            if np.all(evt == 1): \n",
    "                universes.append( [0 for j in range(len(sys_genie[2:]))] )\n",
    "                \n",
    "            else: \n",
    "                universes.append( list(evt) + [0, 0, 0, 0] ) # dirt doesn't have variations for the last 4 knobs \n",
    "        \n",
    "    else: \n",
    "        universes = []\n",
    "        for evt in df[sys_genie[2:]].values: \n",
    "            if np.all(evt == 1): \n",
    "                universes.append( [0 for j in range(len(sys_genie[2:]))] )  # don't include CV neutrinos \n",
    "\n",
    "            else: \n",
    "                universes.append( evt )\n",
    "\n",
    "        \n",
    "    df['weightsGenieUnisim'] = universes\n",
    "    \n",
    "    # for easier handling \n",
    "    df['weightsGenieUnisim'] = df['weightsGenieUnisim'].apply(lambda x: np.array(x))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframes equal # of columns \n",
    "\n",
    "data['is_signal'] = np.nan\n",
    "ext['is_signal'] = False\n",
    "\n",
    "nan_var = ['thdet', 'phidet', 'true_nu_px_beam', 'true_nu_py_beam', 'true_nu_pz_beam', \n",
    "           'thbeam', 'phibeam','weightsNuMIGeo', 'weightsGenieUnisim']\n",
    "\n",
    "for var in mc_var+sys_genie+sys_flux+nan_var: \n",
    "    data[var] = np.nan\n",
    "    ext[var] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.setdiff1d(ext.columns,overlay.columns)\n",
    "# ext.columns == overlay.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check is_signal boolean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some checks \n",
    "print(len(nue.query('is_signal==True'))==len(nue.query(signal)))\n",
    "print(len(nue.query('is_signal==False'))==len(nue.query(not_signal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean bad weights & values \n",
    "\n",
    "for i,df in enumerate(mc_df):\n",
    "    \n",
    "    # bad weights \n",
    "    df.loc[ df['weightSplineTimesTune'] <= 0, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] == np.inf, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] > 100, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightSplineTimesTune']) == True, 'weightSplineTimesTune' ] = 1.\n",
    "    \n",
    "    df.loc[ df['weightTune'] <= 0, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] == np.inf, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] > 100, 'weightTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightTune']) == True, 'weightTune' ] = 1.  \n",
    "\n",
    "    \n",
    "    for ievt in range(df.shape[0]):\n",
    "        \n",
    "        # GENIE MULTISIMS\n",
    "        \n",
    "        # check for NaNs separately        \n",
    "        if np.isnan(df['weightsGenie'].iloc[ievt]).any() == True: \n",
    "            df['weightsGenie'].iloc[ievt][ np.isnan(df['weightsGenie'].iloc[ievt]) ] = 1.\n",
    "            \n",
    "        reweightCondition = ((df['weightsGenie'].iloc[ievt] > 60) | (df['weightsGenie'].iloc[ievt] < 0)  | \n",
    "                             (df['weightsGenie'].iloc[ievt] == np.inf) | (df['weightsGenie'].iloc[ievt] == np.nan))\n",
    "        df['weightsGenie'].iloc[ievt][ reweightCondition ] = 1.\n",
    "        \n",
    "        # if no variations exist for the event\n",
    "        if not list(df['weightsGenie'].iloc[ievt]): \n",
    "            df['weightsGenie'].iloc[ievt] = [1.0 for k in range(600)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # RE-INTERACTION WEIGHTS\n",
    "        \n",
    "        # check for NaNs separately        \n",
    "        if np.isnan(df['weightsReint'].iloc[ievt]).any() == True: \n",
    "            df['weightsReint'].iloc[ievt][ np.isnan(df['weightsReint'].iloc[ievt]) ] = 1.\n",
    "        \n",
    "        reweightCondition2 = ((df['weightsReint'].iloc[ievt] > 60) | (df['weightsReint'].iloc[ievt] < 0)   |\n",
    "                             (df['weightsReint'].iloc[ievt] == np.inf))\n",
    "        df['weightsReint'].iloc[ievt][ reweightCondition2 ] = 1.\n",
    "        \n",
    "        # if no variations exist for the event\n",
    "        if not list(df['weightsReint'].iloc[ievt]): \n",
    "            df['weightsReint'].iloc[ievt] = [1.0 for k in range(1000)]\n",
    "            \n",
    "            \n",
    "            \n",
    "        # GENIE UNISIMS \n",
    "        \n",
    "        # check for NaNs separately\n",
    "        if np.isnan(df['weightsGenieUnisim'].iloc[ievt]).any() == True: \n",
    "            df['weightsGenieUnisim'].iloc[ievt][ np.isnan(df['weightsGenieUnisim'].iloc[ievt]) ] = 1.\n",
    "        \n",
    "        reweightCondition3 = ((df['weightsGenieUnisim'].iloc[ievt] == np.inf) | (df['weightsGenieUnisim'].iloc[ievt] > 60) | \n",
    "                              (df['weightsGenieUnisim'].iloc[ievt] < 0))\n",
    "        df['weightsGenieUnisim'].iloc[ievt][ reweightCondition3 ] = 1.\n",
    "        \n",
    "        # if no variations exist for the event\n",
    "        if not list(df['weightsGenieUnisim'].iloc[ievt]): \n",
    "            df['weightsGenieUnisim'].iloc[ievt] = [1.0 for k in range(len(sys_genie[2:]))]\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pot scaling weights \n",
    "\n",
    "dirt_tune = parameters(ISRUN3)['dirt_tune']\n",
    "ext_tune = parameters(ISRUN3)['ext_tune']\n",
    "    \n",
    "##############################################\n",
    "# SCALE TO BEAM ON POT\n",
    "overlay_scale_to_data = beamon_pot/overlay_pot\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue_scale_to_data = beamon_pot/nue_intrinsic_pot\n",
    "\n",
    "dirt_scale_to_data = dirt_tune*(beamon_pot/dirt_pot)\n",
    "beamoff_scale_to_data = ext_tune*(beamon_ntrig/beamoff_ntrig) # scale factor to beam on POT\n",
    "\n",
    "overlay['pot_scale'] = overlay_scale_to_data\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue['pot_scale'] = nue_scale_to_data\n",
    "    \n",
    "dirt['pot_scale'] = dirt_scale_to_data\n",
    "ext['pot_scale'] = beamoff_scale_to_data\n",
    "data['pot_scale'] = [1 for x in range(len(data))]\n",
    "\n",
    "##############################################\n",
    "# SCALE TO OVERLAY\n",
    "\n",
    "dirt_scale_to_overlay = dirt_tune*(overlay_pot/dirt_pot)\n",
    "beamoff_scale_to_overlay = ext_tune*((overlay_pot/beamon_pot)*(beamon_ntrig/beamoff_ntrig))\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue_scale_to_overlay = overlay_pot/nue_intrinsic_pot\n",
    "\n",
    "overlay['pot_scale_overlay'] = [1 for x in range(len(overlay))]\n",
    "if NUE_INTRINSIC: \n",
    "    nue['pot_scale_overlay'] = nue_scale_to_overlay\n",
    "    \n",
    "dirt['pot_scale_overlay'] = dirt_scale_to_overlay\n",
    "ext['pot_scale_overlay'] = beamoff_scale_to_overlay\n",
    "data['pot_scale_overlay'] = [1 for x in range(len(data))]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total weights \n",
    "\n",
    "# combined genie * POT weight * flux weight \n",
    "# ext gets POT weight only \n",
    "\n",
    "################################################################\n",
    "# totweight_data scales to BEAMON\n",
    "\n",
    "# tuned\n",
    "overlay['totweight_data'] = overlay['pot_scale']*overlay['ppfx_cv']*overlay['weightSplineTimesTune']\n",
    "dirt['totweight_data'] = dirt['pot_scale']*dirt['ppfx_cv']*dirt['weightSplineTimesTune']\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue['totweight_data'] = nue['pot_scale']*nue['ppfx_cv']*nue['weightSplineTimesTune']\n",
    "\n",
    "\n",
    "################################################################\n",
    "# totweight_overlay scales to STANDARD OVERLAY\n",
    "\n",
    "# tuned\n",
    "overlay['totweight_overlay'] = overlay['ppfx_cv']*overlay['weightSplineTimesTune']\n",
    "dirt['totweight_overlay'] = dirt['pot_scale_overlay']*dirt['ppfx_cv']*dirt['weightSplineTimesTune']\n",
    "\n",
    "if NUE_INTRINSIC:\n",
    "    nue['totweight_overlay'] = nue['pot_scale_overlay']*nue['ppfx_cv']*nue['weightSplineTimesTune']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to keep the number of columns the same \n",
    "new_var = ['totweight', 'totweight_overlay']\n",
    "\n",
    "for var in new_var: \n",
    "    for df in [data, ext]: \n",
    "        df[var] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace overlay nue CC events with nue intrinsic sample\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    \n",
    "    # intrinsic sample contains in AV TPC events ONLY, & only CC events (overlay is entire cryo)\n",
    "    in_AV_query = \"-1.55<=true_nu_vtx_x<=254.8 and -116.5<=true_nu_vtx_y<=116.5 and 0<=true_nu_vtx_z<=1036.8\"\n",
    "    \n",
    "    nueCC_query = 'abs(nu_pdg)==12 and ccnc==0 and '+in_AV_query\n",
    "    print(\"# of nueCC in AV in overlay sample = \"+str(len(overlay.query(nueCC_query))))\n",
    "    len1 = len(overlay)\n",
    "    \n",
    "    idx = overlay.query(nueCC_query).index\n",
    "    overlay.drop(idx, inplace=True)\n",
    "    len2 = len(overlay) \n",
    "    print(\"# of nueCC in AV dropped in overlay = \"+str(len1-len2))\n",
    "    \n",
    "    overlay = pd.concat([overlay,nue], ignore_index=True)\n",
    "\n",
    "    # from here on out everything else should be the same. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SW trigger, combine overlay + dirt as MC \n",
    "mc = pd.concat([overlay.query('swtrig_pre==1'),dirt.query('swtrig_pre==1')], ignore_index=True, sort=True)\n",
    "\n",
    "# separate by in/out FV & cosmic\n",
    "infv = mc.query(in_fv_query+' and nu_purity_from_pfp>0.5')\n",
    "#cosmic = mc.query(in_fv_query+' and nu_purity_from_pfp<=0.5')\n",
    "outfv = mc.query(out_fv_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check that everything is accounted for \n",
    "print(len(mc)==len(infv)+len(outfv))#+len(cosmic))\n",
    "\n",
    "if not (len(mc)==len(infv)+len(outfv)):#+len(cosmic)): \n",
    "    d = len(mc) - (len(infv)+len(outfv))#+len(cosmic))\n",
    "    print(d)\n",
    "    \n",
    "     \n",
    "    m = pd.concat([infv, outfv]) #pd.concat([infv, cosmic, outfv])\n",
    "    diff = np.setdiff1d(list(mc.index),list(m.index))\n",
    "\n",
    "    #for i in range(d):\n",
    "        #print(mc.loc[diff[i], 'nu_purity_from_pfp'])\n",
    "        #print(mc.loc[diff[i], 'nslice'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_signal_weighted = np.nansum(mc.query(signal+' and '+in_fv_query)['totweight_data'])\n",
    "print('total signal events in FV = '+ str(tot_signal_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 main categories: \n",
    "\n",
    "# infv - overlay & dirt events with truth vtx in FV \n",
    "# outfv - overlay & dirt events with truth vtx in FV that are classified as neutrinos\n",
    "# cosmic - overlay & dirt events with true vtx in FV that get misclassified as cosmic \n",
    "# ext - beam OFF data\n",
    "# data - beam ON data \n",
    "\n",
    "datasets = {\n",
    "    \"infv\": infv, \n",
    "    \"outfv\": outfv, \n",
    "    #\"cosmic\": cosmic, \n",
    "    \"ext\": ext, \n",
    "    \"data\": data\n",
    "}\n",
    "\n",
    "# [infv, outfv, cosmic, ext, data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply BDT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality cuts\n",
    "BDT_PRE_QUERY = 'nslice==1'\n",
    "BDT_PRE_QUERY += ' and ' + reco_in_fv_query\n",
    "BDT_PRE_QUERY +=' and contained_fraction>0.9'\n",
    "\n",
    "# signal definition - shower constraints\n",
    "BDT_PRE_QUERY += ' and n_showers_contained==1'\n",
    "BDT_PRE_QUERY += ' and shr_energy_tot_cali>0.07'\n",
    "\n",
    "# signal definition - track constraints\n",
    "BDT_PRE_QUERY += ' and n_tracks_contained>0'\n",
    "BDT_PRE_QUERY += ' and trk_energy>0.04' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT_LOOSE_CUTS = BDT_PRE_QUERY\n",
    "\n",
    "# loose shower constraints\n",
    "BDT_LOOSE_CUTS +=' and shr_score<0.3'\n",
    "BDT_LOOSE_CUTS += ' and shrmoliereavg<15'\n",
    "BDT_LOOSE_CUTS += ' and shr_tkfit_dedx_Y<7'\n",
    "\n",
    "# loose track constraints\n",
    "BDT_LOOSE_CUTS += ' and trkpid<0.35'\n",
    "BDT_LOOSE_CUTS += ' and tksh_distance<12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = parameters(ISRUN3)['bdt_training_parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bdt model \n",
    "bdt_model = xgb.Booster({'nthread': 4})\n",
    "bdt_model.load_model(parameters(ISRUN3)['bdt_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_bdt = {}\n",
    "\n",
    "for i in range(len(datasets)): \n",
    "    \n",
    "    df = list(datasets.values())[i].copy()\n",
    "    df = df.query(BDT_LOOSE_CUTS)\n",
    "    \n",
    "    # clean datasets \n",
    "    for column in training_parameters:\n",
    "        df.loc[(df[column] < -1.0e37) | (df[column] > 1.0e37), column] = np.nan\n",
    "\n",
    "    # create testing dmatrix \n",
    "    df_test = xgb.DMatrix(data=df[training_parameters])\n",
    "\n",
    "    # apply the bdt selection\n",
    "    preds = bdt_model.predict(df_test)\n",
    "\n",
    "    # add columns for plotting \n",
    "    df['BDT_score'] = preds\n",
    "    \n",
    "    datasets_bdt[list(datasets.keys())[i]] = df\n",
    "\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_score_cut = parameters(ISRUN3)['bdt_score_cut']\n",
    "    \n",
    "print(\"BDT SCORE THRESHOLD = \"+str(bdt_score_cut))\n",
    "\n",
    "selected_query = BDT_LOOSE_CUTS + ' and BDT_score>'+str(bdt_score_cut)\n",
    "selected_signal_query = selected_query + ' and is_signal==True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GENERATED SIGNAL (DATA NORMALIZED) = \"+str(sum(generated_signal(ISRUN3, 'nu_e', 1, 0, 20))*overlay_scale_to_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat only errors \n",
    "\n",
    "x = plot_mc('NeutrinoEnergy2_GeV', 20, 0, 5, 'BDT_score>'+str(bdt_score_cut), datasets_bdt, \n",
    "            ISRUN3, x_label=\"Reco $\\\\nu$ Energy [GeV]\", norm='data', pot='$2.0\\\\times10^{20}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max selected neutrino energy [true, GeV] =', max(datasets_bdt['infv'].query(selected_signal_query).nu_e))\n",
    "print('min selected neutrino energy [true, GeV] =', min(datasets_bdt['infv'].query(selected_signal_query).nu_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvar = 'shr_energy_cali'\n",
    "\n",
    "xvar_dict = xsec_variables(xvar, ISRUN3)\n",
    "\n",
    "bins = xvar_dict['bins']\n",
    "true_var = xvar_dict['true_var']\n",
    "x_label = xvar_dict['x_label']\n",
    "beamon_pot = xvar_dict['beamon_pot']\n",
    "xlow = xvar_dict['xlow']\n",
    "xhigh = xvar_dict['xhigh']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsec_units = False\n",
    "plot = False\n",
    "\n",
    "n_target = parameters(ISRUN3)['n_target']\n",
    "flux = parameters(ISRUN3)['integrated_flux_per_pot'] * parameters(ISRUN3)['beamon_pot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat only -- SCALES TO DATA\n",
    "\n",
    "x = plot_mc(xvar, bins, xlow, xhigh, 'BDT_score>'+str(bdt_score_cut), datasets_bdt, ISRUN3, \n",
    "            norm='data', sys=False, x_label='Reco '+x_label, save=False, save_label=\"wide\", \n",
    "            pot=\"$2.0 x 10^{20}$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(uncertainty_functions)\n",
    "from uncertainty_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv, ppfx_variations = plotSysVariations(true_var, xvar, bins, xlow, xhigh, selected_query, datasets_bdt, 'weightsPPFX',600, \n",
    "                                         ISRUN3, plot=plot, axis_label='Reco '+x_label, pot='$2.0 x 10^{20}$ POT', \n",
    "                                              background_subtraction=True)\n",
    "\n",
    "ppfx_dict = calcCov(xvar, bins, ncv, ppfx_variations, 'weightsPPFX', plot=plot, save=False, \n",
    "                    axis_label='Reco '+x_label, pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beamline Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered by beamline variation run number\n",
    "# [+1sigma run #, -1sigma run #]\n",
    "\n",
    "beamline_runs = {\n",
    "    'HornCurrent' : [1, 2], \n",
    "    'xHorn1' : [3, 4], \n",
    "    'yHorn1' : [5, 6], \n",
    "    'BeamSpotSize' : [7, 8], \n",
    "    'xHorn2' : [9, 10], \n",
    "    'yHorn2' : [11, 12], \n",
    "    'WaterOnHorns' : [13, 14], \n",
    "    'xBeamShift' : [15, 16], \n",
    "    'yBeamShift' : [17, 18], \n",
    "    'zTargetPosition' : [19, 20]    \n",
    "}\n",
    "\n",
    "beamline_cov = {}\n",
    "\n",
    "# index in weightsNuMIGeo are offset by -1\n",
    "\n",
    "for variation in beamline_runs.keys(): \n",
    "    \n",
    "    idx = [i-1 for i in beamline_runs[variation]]\n",
    "    \n",
    "    ncv, beamline_variations = plotSysVariations(true_var, xvar, bins, xlow, xhigh, selected_query, datasets_bdt, 'weightsNuMIGeo', \n",
    "                                                 idx, ISRUN3, plot=plot, axis_label='Reco '+x_label, pot='$2.0 x 10^{20}$ POT', \n",
    "                                                  background_subtraction=True, title=variation)\n",
    "    \n",
    "    # calc covariance \n",
    "    beamline_cov[variation] = calcCov(xvar, bins, ncv, beamline_variations, 'weightsNuMIGeo', plot=plot, save=False, \n",
    "                    axis_label='Reco '+x_label, pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total covariance, correlation, & uncertainty \n",
    "\n",
    "cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "cor = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for variation in beamline_cov.keys(): \n",
    "    \n",
    "    for i in range(len(bins)-1): \n",
    "        for j in range(len(bins)-1):\n",
    "            \n",
    "            cov[i][j] = sum([beamline_cov[x]['cov'][i][j] for x in beamline_cov.keys()])\n",
    "            \n",
    "            if ncv[i]*ncv[j] != 0: \n",
    "                frac_cov[i][j] = cov[i][j]/(ncv[i]*ncv[j])\n",
    "\n",
    "            \n",
    "for i in range(len(bins)-1): \n",
    "    for j in range(len(bins)-1):\n",
    "        \n",
    "        if np.sqrt(cov[i][i])*np.sqrt(cov[j][j]) != 0: \n",
    "                cor[i][j] = cov[i][j] / (np.sqrt(cov[i][i])*np.sqrt(cov[j][j]))\n",
    "            \n",
    "beamline_dict = {\n",
    "    'cov' : cov, \n",
    "    'frac_cov' : frac_cov,\n",
    "    'cor' : cor,\n",
    "    'fractional_uncertainty' : np.sqrt(np.diag(frac_cov))\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENIE multisims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv, genie_variations = plotSysVariations(true_var, xvar, bins, xlow, xhigh, selected_query, datasets_bdt, 'weightsGenie', 600, \n",
    "                                         ISRUN3, plot=plot, axis_label='Reco '+x_label, pot='$2.0 x 10^{20}$ POT', \n",
    "                                              background_subtraction=True)\n",
    "\n",
    "genie_dict = calcCov(xvar, bins, ncv, genie_variations, 'weightsGenie', plot=plot, save=False, \n",
    "                    axis_label='Reco '+x_label, pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENIE unisims -- need to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO FIX GENIE UNISIM VARIATIONS 4 BACKGROUND "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = plotSysVariations(true_var, xvar, bins, xlow, xhigh, selected_signal_query, datasets_bdt, 'weightsGenieUnisim', 22, \n",
    "                                      ISRUN3, plot=True, axis_label='Reco '+x_label, pot='$2.0 x 10^{20}$ POT', \n",
    "                                              background_subtraction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = plotSysVariations(true_var, xvar, bins, xlow, xhigh, selected_query+' and is_signal==False', \n",
    "                               datasets_bdt, 'weightsGenieUnisim', 22, \n",
    "                                      ISRUN3, plot=True, axis_label='Reco '+x_label, pot='$2.0 x 10^{20}$ POT', \n",
    "                                              background_subtraction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEANT4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv, geant4_variations = plotSysVariations(true_var, xvar, bins, xlow, xhigh, selected_query, datasets_bdt, 'weightsReint', 1000, \n",
    "                                         ISRUN3, plot=plot, axis_label='Reco '+x_label, pot='$2.0 x 10^{20}$ POT', \n",
    "                                              background_subtraction=True)\n",
    "\n",
    "geant4_dict = calcCov(xvar, bins, ncv, geant4_variations, 'weightsReint', plot=plot, save=False, \n",
    "                    axis_label='Reco '+x_label, pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector Systematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ROOT file with BDT-selected detector variations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this step if it is already created\n",
    "# should manually delete the file first \n",
    "# (located here: /uboone/data/users/kmiller/uBNuMI_CCNp/ntuples/runX/systematics/detvar/)\n",
    "\n",
    "# scales to the det sys CV POT (standard overlay)\n",
    "\n",
    "\n",
    "if recreate_file: \n",
    "    for v in list(detvar_run1_fhc.keys()): \n",
    "        NuMIDetSysWeights.makehist_detsys(v, ISRUN3, \"NuMI_FHC_BDT_DetectorVariations_MARCH2022.root\", xvar, \n",
    "                                          bins, cut=selected_query, useBDT=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_variations = NuMIDetSysWeights.plot_variations(xvar, bins, \"NuMI_FHC_BDT_DetectorVariations_MARCH2022.root\", ISRUN3, \n",
    "                                                        axis_label='Reco '+x_label, plot=True, background_subtraction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute covariance (N=1 for each variation)\n",
    "\n",
    "detsys_cov = {}\n",
    "\n",
    "# index in weightsNuMIGeo are offset by -1\n",
    "\n",
    "for variation in detector_variations.keys(): \n",
    "    \n",
    "    if variation=='CV': \n",
    "        continue\n",
    "    \n",
    "    # calc covariance for each unisim \n",
    "    detsys_cov[variation] = calcCov(xvar, bins, detector_variations['CV'], [detector_variations[variation]], 'Detector', \n",
    "                                    plot=False, save=False, axis_label='Reco '+x_label, pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3,\n",
    "                                   title=variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total covariance, correlation, & uncertainty \n",
    "\n",
    "cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "cor = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for variation in detsys_cov.keys(): \n",
    "    \n",
    "    for i in range(len(bins)-1): \n",
    "        for j in range(len(bins)-1):\n",
    "            \n",
    "            cov[i][j] = sum([detsys_cov[x]['cov'][i][j] for x in detsys_cov.keys()])\n",
    "            \n",
    "            if detector_variations['CV'][i]*detector_variations['CV'][j] != 0: \n",
    "                frac_cov[i][j] = cov[i][j]/(detector_variations['CV'][i]*detector_variations['CV'][j])\n",
    "\n",
    "            \n",
    "for i in range(len(bins)-1): \n",
    "    for j in range(len(bins)-1):\n",
    "        \n",
    "        if np.sqrt(cov[i][i])*np.sqrt(cov[j][j]) != 0: \n",
    "                cor[i][j] = cov[i][j] / (np.sqrt(cov[i][i])*np.sqrt(cov[j][j]))\n",
    "            \n",
    "detsys_dict = {\n",
    "    'cov' : cov, \n",
    "    'frac_cov' : frac_cov,\n",
    "    'cor' : cor,\n",
    "    'fractional_uncertainty' : np.sqrt(np.diag(frac_cov))\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stat Uncertainty (MC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GENIE closure test : uncertainty on the full estimated event rate \n",
    "# if fake/real data : uncertainty on the MC background only  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = plot_mc(xvar, bins, xlow, xhigh, selected_query, datasets_bdt, ISRUN3, \n",
    "            norm='data', sys=False, x_label='Reco '+x_label, pot=\"$2.0 x 10^{20}$\")\n",
    "\n",
    "ncv_total = d['CV']\n",
    "ncv_bkgd = d['background_counts']\n",
    "ncv_bkgd_subtracted = [a-b for a,b in zip(ncv_total, ncv_bkgd)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not include EXT \n",
    "\n",
    "ext_counts = plt.hist(datasets_bdt['ext'].query(selected_query)[xvar], bins, color='lightgrey', \n",
    "                      weights=datasets_bdt['ext'].query(selected_query).pot_scale)[0]\n",
    "plt.close()\n",
    "\n",
    "ncv_mc = [a-b for a,b in zip(ncv_total,ext_counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the sum of the weights squared for MC counting error -- on the full event rate (for closure test)\n",
    "\n",
    "print('Make sure to update for full event rate, background-subtracted, or background only !')\n",
    "\n",
    "mc_stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "mc_frac_stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "s = 0 \n",
    "for i in range(len(bins)-1):\n",
    "\n",
    "    if i==len(bins)-2: \n",
    "        bin_query = xvar+' >= '+str(bins[i])+' and '+xvar+' <= '+str(bins[i+1])\n",
    "    else: \n",
    "        bin_query = xvar+' >= '+str(bins[i])+' and '+xvar+' < '+str(bins[i+1])\n",
    "        \n",
    "    infv = datasets_bdt['infv'].copy().query(selected_query)\n",
    "    outfv = datasets_bdt['outfv'].copy().query(selected_query)\n",
    "    #cosmic = datasets_bdt['cosmic'].copy().query(selected_query)\n",
    "    \n",
    "    ncv_df = pd.concat([infv, outfv], ignore_index=True) #pd.concat([infv, outfv, cosmic], ignore_index=True)\n",
    "    \n",
    "    mc_stat_cov[i][i] = sum(ncv_df.query(bin_query).totweight_data ** 2) \n",
    "    mc_frac_stat_cov[i][i] = mc_stat_cov[i][i]/(ncv_mc[i]*ncv_mc[i])\n",
    "    \n",
    "mc_stat_percent_error = np.sqrt(np.diag(mc_frac_stat_cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot: \n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "        \n",
    "    plt.pcolor(bins, bins, mc_stat_cov, cmap='OrRd', edgecolors='k' )\n",
    "\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    plt.xlabel(\"Reco \"+x_label, fontsize=15)\n",
    "    plt.ylabel(\"Reco \"+x_label, fontsize=15)\n",
    "\n",
    "    plt.title('MC Statistical Covariance', fontsize=15)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "        \n",
    "    plt.pcolor(bins, bins, mc_frac_stat_cov, cmap='OrRd', edgecolors='k' )\n",
    "\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    plt.xlabel(\"Reco \"+x_label, fontsize=15)\n",
    "    plt.ylabel(\"Reco \"+x_label, fontsize=15)\n",
    "\n",
    "    plt.title('MC Fractional Statistical Covariance', fontsize=15)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stat Uncertainty (DATA) -- beam on & EXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('need to add in DATA & EXT uncertainty!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagonals are sqrt(N), everything else is 0\n",
    "# this will be for DATA later on \n",
    "\n",
    "# number of events fed into the signal channel \n",
    "# n, b, p = plt.hist(datasets_bdt['infv'].query(selected_signal_query)['shr_energy_cali'], bins, histtype='bar', range=[xlow, xhigh], \n",
    "#                  weights=datasets_bdt['infv'].query(selected_signal_query).totweight_data)\n",
    "# plt.close()\n",
    "\n",
    "# stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "# for i in range(len(bins)-1): \n",
    "\n",
    "    #stat_cov[i][i] = \n",
    "    \n",
    "    #if xsec_units: \n",
    "    #    stat_cov[i][i] = (n[i]/(n_target*flux))#**2\n",
    "    \n",
    "    #else: \n",
    "    #    stat_cov[i][i] = n[i]#**2\n",
    "    \n",
    "#stat_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC Response Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_signal_df = datasets_bdt['infv'].query(selected_signal_query).copy()\n",
    "selected_signal_df['seed'] = selected_signal_df.apply( lambda x: ConcatRunSubRunEvent(x['run'], x['sub'], x['evt']), axis=1 )\n",
    "selected_signal_df['weightsPoisson'] = selected_signal_df.apply( lambda x: PoissonRandomNumber(x['seed'], mean=1.0, size=1000), axis=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "# histogram bin counts for all universes\n",
    "uni_counts = []\n",
    "\n",
    "for u in range(1000): \n",
    "\n",
    "    # multiply in with sys weight of universe u \n",
    "    sys_weight = list(selected_signal_df['weightsPoisson'].str.get(u))\n",
    "    \n",
    "    total_weight = [ x*y for x, y in zip(sys_weight, selected_signal_df['totweight_data']) ]\n",
    "        \n",
    "    # for i in range(len(sys_weight)): \n",
    "        #if np.isnan(sys_weight[i])==True: \n",
    "        #    print('NaN in ' + sys_var)\n",
    "                \n",
    "            # clean the dataset - get rid of nans\n",
    "        #    sys_weight[i] = 1.0\n",
    "\n",
    "    \n",
    "    selected_signal_df['weight_sys'] = total_weight\n",
    "\n",
    "    # plot variation\n",
    "    if xsec_units: \n",
    "\n",
    "        w_uv = [ (1E39) * wgt/(flux*n_target) for wgt in selected_signal_df['weight_sys'] ] \n",
    "        \n",
    "        n, b, p = plt.hist(selected_signal_df[xvar], bins, histtype='step', weights=w_uv, \n",
    "                            linewidth=0.5, color='cornflowerblue')\n",
    "        \n",
    "        \n",
    "        \n",
    "        uni_counts.append(n/1E39)\n",
    "                           \n",
    "    else: \n",
    "        n, b, p = plt.hist(selected_signal_df[xvar], bins, histtype='step', weights=selected_signal_df['weight_sys'], \n",
    "                            linewidth=0.5, color='cornflowerblue')  \n",
    "            \n",
    "        uni_counts.append(n)\n",
    "\n",
    "\n",
    "if xsec_units: \n",
    "    w_cv = [ (1E39) * wgt/(flux*n_target) for wgt in selected_signal_df['totweight_data']] \n",
    "    ncv, bcv, pcv = plt.hist(selected_signal_df[xvar], bins, histtype='step', \n",
    "                         weights=w_cv, linewidth=2, color='black') \n",
    "    \n",
    "    ncv = ncv/1E39\n",
    "    \n",
    "else: \n",
    "    ncv, bcv, pcv = plt.hist(selected_signal_df[xvar], bins, histtype='step', \n",
    "                         weights=selected_signal_df['totweight_data'], linewidth=2, color='black')      \n",
    "        \n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "        \n",
    "plt.xlabel('Reco '+x_label, fontsize=15)\n",
    "\n",
    "        \n",
    "if xsec_units==False: \n",
    "    plt.ylabel(\"$\\\\nu$ / $2 \\\\times 10^{20}$ POT\" , fontsize=15)\n",
    "\n",
    "else: \n",
    "    plt.ylabel(\"$\\\\tilde{\\\\sigma}$ [$10^{-39}$cm$^{2}$/nucleon]\", fontsize=15)\n",
    "\n",
    "plt.title('weightsPoisson', fontsize=16)    \n",
    "\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_matrix_uncertainty = calcCov(xvar, bins, ncv, uni_counts, 'weightsPoisson', isrun3=ISRUN3, plot=True, \n",
    "                   save=False, axis_label='Reco '+x_label, pot=parameters(ISRUN3)['beamon_pot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POT Counting (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncertainty on the background-subtracted event rate, like the other systematics\n",
    "\n",
    "ncv_bkgd = plot_mc(xvar, bins, xlow, xhigh, selected_query, datasets_bdt, ISRUN3, \n",
    "            norm='data', sys=False, x_label='Reco '+x_label, pot=\"$2.0 x 10^{20}$\")['background_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot_counting = pot_unisims(xvar, ncv_total, bins, 0.02, ISRUN3, plot=True, x_label=None, bkgd_cv_counts=ncv_bkgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirt (100%)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100%\n",
    "# vary the dirt interactions by 100% (1 unisim) on the background-subtracted selected event rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dirt = plot_mc(xvar, bins, xlow, xhigh, 'isDirt==1 and BDT_score>'+str(bdt_score_cut), datasets_bdt, ISRUN3, \n",
    "            norm='data', sys=False, x_label='Reco '+x_label, pot=\"$2.0 x 10^{20}$\")['CV']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt_uncertainty = dirt_unisim(xvar, bins, ncv_total, selected_dirt, 1.0, ISRUN3, plot=True, \n",
    "                               x_label=None, title=None, bkgd_cv_counts=ncv_bkgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Sources of Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frac_cov_dict = {\n",
    "    'ppfx' : ppfx_dict['frac_cov'], \n",
    "    'beamline' : beamline_dict['frac_cov'], \n",
    "    'genie_ms' : genie_dict['frac_cov'], \n",
    "    # 'genie_us': genie_unisim_dict['frac_cov'], \n",
    "    'geant4' : geant4_dict['frac_cov'],\n",
    "    'detector' : detsys_dict['frac_cov'], \n",
    "    'pot_counting' : pot_counting['frac_cov'], \n",
    "    'dirt' : dirt_uncertainty['frac_cov'],\n",
    "    'mc_stat' : mc_frac_stat_cov, \n",
    "    'response_matrix' : response_matrix_uncertainty['frac_cov']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_frac_cov, tot_abs_cov = plotFullCov(frac_cov_dict, xvar, ncv_bkgd_subtracted, bins, xlow, xhigh, save=False, \n",
    "                      axis_label='Reco '+x_label, isrun3=ISRUN3, pot='$2.0 x 10^{20}$ POT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ppfx & beamline geometry in quadrature\n",
    "frac_cov_dict['flux'] = [ [x+y for x,y in zip(a,b)] for a,b in zip(frac_cov_dict['ppfx'], frac_cov_dict['beamline'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add genie in quadrature\n",
    "# frac_cov_dict['genie_all'] = [ [x+y for x,y in zip(a,b)] for a,b in zip(frac_cov_dict['genie_ms'], frac_cov_dict['genie_us'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bincenters = 0.5*(np.array(bins)[1:]+np.array(bins)[:-1])\n",
    "#colors = ['#6699CC', '#004488', '#EECC66', '#994455', '#997700', '#EE99AA', 'lightskyblue']\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))  \n",
    "\n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"Total\",\n",
    "        weights=np.sqrt(np.diagonal(tot_frac_cov)), linewidth=1.5, color='black')\n",
    "\n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"Flux\", \n",
    "         weights=np.sqrt(np.diagonal(frac_cov_dict['flux'])))\n",
    "\n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"GENIE\", \n",
    "         weights=np.sqrt(np.diagonal(frac_cov_dict['genie_ms'])))\n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"GEANT4\", \n",
    "         weights=np.sqrt(np.diagonal(frac_cov_dict['geant4'])))\n",
    "\n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"Detector\", \n",
    "         weights=np.sqrt(np.diagonal(frac_cov_dict['detector'])))\n",
    "\n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"POT counting\",\n",
    "        weights=np.sqrt(np.diagonal(frac_cov_dict['pot_counting'])))\n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"Dirt\",\n",
    "        weights=np.sqrt(np.diagonal(frac_cov_dict['dirt'])))\n",
    "\n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"Response matrix\",\n",
    "        weights=np.sqrt(np.diagonal(frac_cov_dict['response_matrix'])))\n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"MC stat\",\n",
    "        weights=np.sqrt(np.diagonal(frac_cov_dict['mc_stat'])))\n",
    "\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.xlabel('Reco '+x_label, fontsize=15)\n",
    "plt.ylabel(\"Fractional Uncertainty\", fontsize=15)\n",
    "\n",
    "plt.xlim(bins[0], xhigh)\n",
    "plt.ylim(0, .75)\n",
    "\n",
    "plt.legend(fontsize=13, frameon=False, ncol=2)\n",
    "\n",
    "#plt.savefig(plots_path+xvar+\"_FracUncertainty.pdf\", transparent=True, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save covariance to unfolding file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcov = TH2D(\"hcov_tot\", \"Covariance Matrix vs. Reco \"+x_label, \n",
    "            len(bins)-1, np.array(bins), len(bins)-1, np.array(bins))\n",
    "\n",
    "for i in range(len(bincenters)): # i = row (y)\n",
    "    for j in range(len(bincenters)): # j = column (x) \n",
    "\n",
    "        hcov.Fill(bincenters[j], bincenters[i], tot_abs_cov[i][j]) \n",
    "        \n",
    "        #print('x = '+str(j), 'y = '+str(i), 'counts = '+ str(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ROOT.TFile.Open(\"/uboone/data/users/kmiller/unfolding/WSVD_\"+xvar+\"_FHCRUN1_MARCH12.root\", \"UPDATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.cd()\n",
    "hcov.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data/MC Comparisons -- TK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NuMI Oscillations (3+1 Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdated \n",
    "\n",
    "x = plot_mc(xvar, [round(0.01*x, 2) for x in range(0, 75, 5)], 0, 0.7, 'BDT_score>0.575', datasets_bdt, ISRUN3, \n",
    "        plt_norm='proj', pot='$9.23\\\\times10^{20}$', ymax=30, x_label='True Neutrino Energy [GeV]', osc='machado_bestfit.csv')\n",
    "\n",
    "# osc='biggest_variation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create projected oscillation dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load a stored dictionary \n",
    "with open('outdated/FHC_Projected_TrueNeutrinoEnergy.json') as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = d['bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = plot_mc('nu_e', bins, 0, 5, selected_query, datasets_bdt, \n",
    "            ISRUN3, x_label=\"Reco $\\\\nu$ Energy [GeV]\", norm='data', pot='$2.0\\\\times10^{20}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['bins'] = bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot_scale = 9.23E20/parameters(ISRUN3)['beamon_pot']\n",
    "print(pot_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['CV'] = [k*pot_scale for k in x['CV']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv, ppfx_variations = plotSysVariations('nu_e', 'nu_e', bins, bins[0], bins[-1], selected_query, datasets_bdt, 'weightsPPFX',600, \n",
    "                                         ISRUN3, plot=True, axis_label='True Neutrino Energy [GeV]', pot='$2.0 x 10^{20}$ POT', \n",
    "                                              background_subtraction=False)\n",
    "\n",
    "ppfx_dict = calcCov('nu_e', bins, ncv, ppfx_variations, 'weightsPPFX', plot=True, save=False, \n",
    "                    axis_label='True Neutrino Energy [GeV] ', pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3, title='Hadron Production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['ppfx_cov_frac'] = ppfx_dict['frac_cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv, genie_variations = plotSysVariations('nu_e', 'nu_e', bins, bins[0], bins[-1], selected_query, datasets_bdt, 'weightsGenie',600, \n",
    "                                         ISRUN3, plot=True, axis_label='True Neutrino Energy [GeV]', pot='$2.0 x 10^{20}$ POT', \n",
    "                                              background_subtraction=False)\n",
    "\n",
    "genie_dict = calcCov('nu_e', bins, ncv, genie_variations, 'weightsGenie', plot=False, save=False, \n",
    "                    axis_label='True Neutrino Energy [GeV] ', pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3, title='Hadron Production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['genie_cov_frac'] = genie_dict['frac_cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv, geant4_variations = plotSysVariations('nu_e', 'nu_e', bins, bins[0], bins[-1], selected_query, datasets_bdt, 'weightsReint',1000, \n",
    "                                         ISRUN3, plot=True, axis_label='True Neutrino Energy [GeV]', pot='$2.0 x 10^{20}$ POT', \n",
    "                                              background_subtraction=False)\n",
    "\n",
    "\n",
    "geant4_dict = calcCov('nu_e', bins, ncv, geant4_variations, 'weightsReint', plot=False, save=False, \n",
    "                    axis_label='True Neutrino Energy [GeV] ', pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3, title='Hadron Production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['reint_cov_frac'] = geant4_dict['frac_cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## detector variations -- make new file \n",
    "recreate_file=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if recreate_file: \n",
    "    for v in list(detvar_run1_fhc.keys()): \n",
    "        NuMIDetSysWeights.makehist_detsys(v, ISRUN3, \"NuMI_FHC_BDT_DetectorVariations_OscillationAnalysis.root\", 'nu_e', \n",
    "                                          bins, cut=selected_query, useBDT=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_variations = NuMIDetSysWeights.plot_variations('nu_e', bins, \"NuMI_FHC_BDT_DetectorVariations_OscillationAnalysis.root\", \n",
    "                                                        ISRUN3, axis_label='True Neutrino Energy', plot=True, background_subtraction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute covariance (N=1 for each variation)\n",
    "\n",
    "detsys_cov = {}\n",
    "\n",
    "# index in weightsNuMIGeo are offset by -1\n",
    "\n",
    "for variation in detector_variations.keys(): \n",
    "    \n",
    "    if variation=='CV': \n",
    "        continue\n",
    "    \n",
    "    # calc covariance for each unisim \n",
    "    detsys_cov[variation] = calcCov('nu_e', bins, detector_variations['CV'], [detector_variations[variation]], 'Detector', \n",
    "                                    plot=False, save=False, pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3,\n",
    "                                   title=variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total covariance, correlation, & uncertainty \n",
    "\n",
    "cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "cor = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for variation in detsys_cov.keys(): \n",
    "    \n",
    "    for i in range(len(bins)-1): \n",
    "        for j in range(len(bins)-1):\n",
    "            \n",
    "            cov[i][j] = sum([detsys_cov[x]['cov'][i][j] for x in detsys_cov.keys()])\n",
    "            \n",
    "            if detector_variations['CV'][i]*detector_variations['CV'][j] != 0: \n",
    "                frac_cov[i][j] = cov[i][j]/(detector_variations['CV'][i]*detector_variations['CV'][j])\n",
    "\n",
    "            \n",
    "for i in range(len(bins)-1): \n",
    "    for j in range(len(bins)-1):\n",
    "        \n",
    "        if np.sqrt(cov[i][i])*np.sqrt(cov[j][j]) != 0: \n",
    "                cor[i][j] = cov[i][j] / (np.sqrt(cov[i][i])*np.sqrt(cov[j][j]))\n",
    "            \n",
    "detsys_dict = {\n",
    "    'cov' : cov, \n",
    "    'frac_cov' : frac_cov,\n",
    "    'cor' : cor,\n",
    "    'fractional_uncertainty' : np.sqrt(np.diag(frac_cov))\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['det_cov_frac'] = detsys_dict['frac_cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for source in list(oscillation_dict.keys())[2:]: \n",
    "    tot_frac_cov = [ [x+y for x,y in zip(a,b)] for a,b in zip(tot_frac_cov, oscillation_dict[source])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['tot_cov_frac'] = tot_frac_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this dictionary \n",
    "\n",
    "with open('mun/FHC_Projected_TrueNeutrinoEnergy_March2022.json', 'w') as f:\n",
    "    json.dump(oscillation_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
