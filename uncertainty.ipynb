{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3 = True\n",
    "isData = True # use for NuWro or NuMI data \n",
    "NUE_INTRINSIC = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot event rate variations, fractional uncertainties, & data/MC comparisons \n",
    "# for all sources of systematic error\n",
    "# also consider potential NuMI oscillations on the event rate \n",
    "# make sure to update the plots_path here & in backend function scripts before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, 'backend_functions')\n",
    "\n",
    "import selection_functions as sf\n",
    "\n",
    "import importlib\n",
    "\n",
    "import uproot\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import awkward\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import ROOT\n",
    "\n",
    "import top \n",
    "from top import *\n",
    "\n",
    "import uncertainty_functions \n",
    "from uncertainty_functions import *\n",
    "\n",
    "import xsec_functions \n",
    "from xsec_functions import smear_matrix\n",
    "\n",
    "from ROOT import TH1D, TH2D, TDirectory, TH1F, TH2F\n",
    "\n",
    "from selection_functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"date and time:\",date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NuMIGeoWeights\n",
    "importlib.reload(NuMIGeoWeights)\n",
    "\n",
    "if ISRUN3: \n",
    "    current = \"RHC\"\n",
    "    \n",
    "else: \n",
    "    current = \"FHC\"\n",
    "\n",
    "numiBeamlineGeoWeights = NuMIGeoWeights.NuMIGeoWeights(current=current) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NuMIDetSys\n",
    "importlib.reload(NuMIDetSys)\n",
    "\n",
    "NuMIDetSysWeights = NuMIDetSys.NuMIDetSys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_path = parameters(ISRUN3)['plots_path']\n",
    "plots_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = \"nuselection\"\n",
    "tree = \"NeutrinoSelectionFilter\"\n",
    "\n",
    "DATA = \"\"\n",
    "EXT = \"\"\n",
    "OVRLY  = \"\"\n",
    "DRT = \"\"\n",
    "NUE = \"\"\n",
    "\n",
    "\n",
    "# slimmed with opening angle \n",
    "path = parameters(ISRUN3)['cv_ntuple_path']\n",
    "print('path = ', path)\n",
    "\n",
    "if not ISRUN3: \n",
    "    \n",
    "    # Run 1 FHC \n",
    "    OVRLY = 'neutrinoselection_filt_run1_overlay_v7'\n",
    "    EXT = 'neutrinoselection_filt_run1_beamoff_v5'\n",
    "    DATA = 'neutrinoselection_filt_run1_beamon_beamgood_v5'\n",
    "    DRT = 'prodgenie_numi_uboone_overlay_dirt_fhc_mcc9_run1_v28_all_snapshot'\n",
    "    \n",
    "    if NUE_INTRINSIC: \n",
    "        NUE = 'neutrinoselection_filt_run1_overlay_intrinsic_v7'\n",
    "\n",
    "else: \n",
    "    \n",
    "    # Run 3 RHC\n",
    "    OVRLY = 'neutrinoselection_filt_run3b_overlay_v7'\n",
    "    DATA = 'neutrinoselection_filt_run3b_beamon_beamgood_v5'\n",
    "    EXT = 'neutrinoselection_filt_run3b_beamoff_v5'\n",
    "    DRT = 'neutrinoselection_filt_run3b_dirt_overlay_v6'\n",
    "    \n",
    "    if NUE_INTRINSIC: \n",
    "        NUE = 'neutrinoselection_filt_run3b_overlay_intrinsic_v7'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overlay = uproot.open(\"/uboone/data/users/kmiller/uBNuMI_CCNp/ntuples/run1/cv/test/neutrinoselection_filt_run1_overlay_v64_eventweight_v2_slim.root\")[fold][tree]\n",
    "overlay = uproot.open(path+OVRLY+\".root\")[fold][tree]\n",
    "data = uproot.open(path+DATA+\".root\")[fold][tree]\n",
    "ext = uproot.open(path+EXT+\".root\")[fold][tree]\n",
    "dirt = uproot.open(path+DRT+\".root\")[fold][tree]  \n",
    "\n",
    "uproot_v = [overlay,data,ext,dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue = uproot.open(path+NUE+\".root\")[fold][tree]\n",
    "    uproot_v.append(nue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    \"trk_score_v\", \n",
    "    \"shr_tkfit_dedx_Y\", \n",
    "    \"n_tracks_contained\", \n",
    "    \"NeutrinoEnergy2\",\n",
    "    \"run\",\"sub\",\"evt\",\n",
    "    \"reco_nu_vtx_sce_x\",\"reco_nu_vtx_sce_y\",\"reco_nu_vtx_sce_z\",\n",
    "    \"shrsubclusters0\",\"shrsubclusters1\",\"shrsubclusters2\",\n",
    "    \"trkshrhitdist2\",\n",
    "    \"n_showers_contained\", \n",
    "    \"shr_phi\", \"trk_phi\", \"trk_theta\",\n",
    "    \"shr_score\", \n",
    "    \"trk_energy\", \n",
    "    \"tksh_distance\", \"tksh_angle\",\n",
    "    \"shr_energy_tot_cali\", \"shr_energy_cali\", \n",
    "    \"nslice\", \n",
    "    \"contained_fraction\",\n",
    "    \"shrmoliereavg\", \"shr_px\", \"shr_py\", \"shr_pz\", \"swtrig_pre\"\n",
    "]\n",
    "\n",
    "# MC only variables\n",
    "mc_var = [\"nu_pdg\", \"shr_theta\", \"true_e_visible\", \"ccnc\", \n",
    "          \"nproton\", \"nu_e\", \"npi0\", \"npion\",\n",
    "          \"true_nu_vtx_x\", \"true_nu_vtx_y\" , \"true_nu_vtx_z\", \n",
    "          \"weightTune\", \"weightSpline\", \"weightSplineTimesTune\", \n",
    "          \"true_nu_px\", \"true_nu_py\", \"true_nu_pz\", \n",
    "          \"elec_e\", \"proton_e\", \"mc_px\", \"mc_py\", \"mc_pz\", \"elec_px\", \"elec_py\", \"elec_pz\", \n",
    "          \"ppfx_cv\", \"mc_pdg\", \"opening_angle\"]\n",
    "\n",
    "sys_genie = [\"weightsGenie\", \"weightsReint\", \n",
    "             \"knobRPAup\", \"knobRPAdn\", \n",
    "             \"knobCCMECup\", \n",
    "             \"knobAxFFCCQEup\", \n",
    "             \"knobVecFFCCQEup\", \n",
    "             \"knobDecayAngMECup\", \n",
    "             \"knobThetaDelta2Npiup\", \n",
    "             \"knobThetaDelta2NRadup\", \n",
    "             #\"knobRPA_CCQE_Reducedup\", \"knobRPA_CCQE_Reduceddn\", # obsolete\n",
    "             \"knobNormCCCOHup\", \n",
    "             \"knobNormNCCOHup\",   \n",
    "             \"knobxsr_scc_Fv3up\",  # these are supposed to be multisims - 10 universes each -- map to pull out\n",
    "             \"knobxsr_scc_Fa3up\"\n",
    "            ]\n",
    "\n",
    "sys_flux = ['weightsPPFX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = overlay.pandas.df(variables + mc_var + sys_genie + sys_flux, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt = dirt.pandas.df(variables + mc_var + sys_genie[:-2] + sys_flux, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt['knobxsr_scc_Fv3up'] = 1\n",
    "dirt['knobxsr_scc_Fa3up'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUE_INTRINSIC: \n",
    "    nue = nue.pandas.df(variables + mc_var + sys_genie + sys_flux, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.pandas.df(variables, flatten=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = ext.pandas.df(variables, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in mc_var+sys_genie+sys_flux: \n",
    "    data[var] = np.nan\n",
    "    ext[var] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is dirt bool\n",
    "\n",
    "overlay['isDirt'] = False\n",
    "dirt['isDirt'] = True\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue['isDirt'] = False\n",
    "    \n",
    "data['isDirt'] = np.nan\n",
    "ext['isDirt'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to get the LLR-PID value for the \"track candidate\" \n",
    "# (proton for nue selection, muon for numu)\n",
    "# can be done for any variable\n",
    "# code from Giuseppe!\n",
    "#LLR-PID : log likelihood ratio particle ID \n",
    "\n",
    "df_v = [overlay,data,ext,dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    df_v.append(nue)\n",
    "    \n",
    "for i,df in enumerate(df_v):\n",
    "    up = uproot_v[i]\n",
    "    trk_llr_pid_v = up.array('trk_llr_pid_score_v')\n",
    "    trk_id = up.array('trk_id')-1 # I think we need this -1 to get the right result\n",
    "    trk_llr_pid_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_llr_pid_v,trk_id)])\n",
    "    df['trkpid'] = trk_llr_pid_v_sel\n",
    "    df['subcluster'] = df['shrsubclusters0'] + df['shrsubclusters1'] + df['shrsubclusters2']\n",
    "    \n",
    "    df['NeutrinoEnergy2_GeV'] = df['NeutrinoEnergy2']/1000\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = overlay.query('swtrig_pre==1')\n",
    "dirt = dirt.query('swtrig_pre==1')\n",
    "nue = nue.query('swtrig_pre==1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_df = [overlay, dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    mc_df.append(nue)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add truth level theta & phi angles (detector & beam coordinates)\n",
    "overlay = addAngles(overlay)\n",
    "dirt = addAngles(dirt)\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue = addAngles(nue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(mc_df):\n",
    "    # is signal bool \n",
    "    df['is_signal'] = np.where((df.swtrig_pre == 1) \n",
    "                             & (df.nu_pdg==12) & (df.ccnc==0) & (df.nproton>0) & (df.npion==0) & (df.npi0==0)\n",
    "                             & (10 <= df.true_nu_vtx_x) & (df.true_nu_vtx_x <= 246)\n",
    "                             & (-106 <= df.true_nu_vtx_y) & (df.true_nu_vtx_y <= 106)\n",
    "                             & (10 <= df.true_nu_vtx_z) & (df.true_nu_vtx_z <= 1026), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(mc_df):\n",
    "  \n",
    "    # get right order of magnitude for multiverses\n",
    "    df['weightsPPFX'] = df['weightsPPFX']/1000\n",
    "    df['weightsReint'] = df['weightsReint']/1000\n",
    "    df['weightsGenie'] = df['weightsGenie']/1000\n",
    "    \n",
    "    # add beamline geometry weights\n",
    "    df['weightsNuMIGeo'] = df.apply( lambda x: numiBeamlineGeoWeights.calculateGeoWeight(x['nu_pdg'],x['nu_e'],x['thbeam']) , axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframes equal # of columns \n",
    "\n",
    "data['is_signal'] = np.nan\n",
    "ext['is_signal'] = False\n",
    "\n",
    "nan_var = ['thdet', 'phidet', 'true_nu_px_beam', 'true_nu_py_beam', 'true_nu_pz_beam', \n",
    "           'thbeam', 'phibeam','weightsNuMIGeo']\n",
    "\n",
    "for var in mc_var+sys_genie+sys_flux+nan_var: \n",
    "    data[var] = np.nan\n",
    "    ext[var] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.setdiff1d(ext.columns,overlay.columns)\n",
    "#ext.columns == overlay.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some checks \n",
    "print(len(nue.query('is_signal==True'))==len(nue.query(signal)))\n",
    "print(len(nue.query('is_signal==False'))==len(nue.query(not_signal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean bad weights & values \n",
    "\n",
    "for i,df in enumerate(mc_df):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    # bad weights \n",
    "    df.loc[ df['weightSplineTimesTune'] <= 0, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] == np.inf, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] > 60, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightSplineTimesTune']) == True, 'weightSplineTimesTune' ] = 1.\n",
    "    \n",
    "    df.loc[ df['weightTune'] <= 0, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] == np.inf, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] > 60, 'weightTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightTune']) == True, 'weightTune' ] = 1.  \n",
    "    \n",
    "                \n",
    "    # CLEAN GENIE UNISIM WEIGHTS & CREATE WEIGHTSGENIEUNISIM LIST \n",
    "    for v in sys_genie[2:]: \n",
    "        df.loc[ df[v] <= 0, v ] = 1.\n",
    "        df.loc[ df[v] == np.inf, v ] = 1.\n",
    "        df.loc[ df[v] > 60, v ] = 1.\n",
    "        df.loc[ np.isnan(df[v]) == True, v ] = 1.\n",
    "        \n",
    "    universes = []\n",
    "    for evt in df[sys_genie[2:]].values: \n",
    "        universes.append( evt )\n",
    "            \n",
    "    df['weightsGenieUnisim'] = universes\n",
    "\n",
    "        \n",
    "    # cleaning -- for entries that are arrays \n",
    "    for ievt in range(df.shape[0]):\n",
    "        \n",
    "        # GENIE MULTISIMS\n",
    "        \n",
    "        # check for NaNs separately        \n",
    "        if np.isnan(df['weightsGenie'].iloc[ievt]).any() == True: \n",
    "            df['weightsGenie'].iloc[ievt][ np.isnan(df['weightsGenie'].iloc[ievt]) ] = 1.\n",
    "            \n",
    "        reweightCondition = ((df['weightsGenie'].iloc[ievt] > 60) | (df['weightsGenie'].iloc[ievt] < 0)  | \n",
    "                             (df['weightsGenie'].iloc[ievt] == np.inf) | (df['weightsGenie'].iloc[ievt] == np.nan))\n",
    "        df['weightsGenie'].iloc[ievt][ reweightCondition ] = 1.\n",
    "        \n",
    "        # if no variations exist for the event\n",
    "        if not list(df['weightsGenie'].iloc[ievt]): \n",
    "            df['weightsGenie'].iloc[ievt] = [1.0 for k in range(600)]\n",
    "        \n",
    "        # RE-INTERACTION WEIGHTS\n",
    "        \n",
    "        # check for NaNs separately        \n",
    "        if np.isnan(df['weightsReint'].iloc[ievt]).any() == True: \n",
    "            df['weightsReint'].iloc[ievt][ np.isnan(df['weightsReint'].iloc[ievt]) ] = 1.\n",
    "        \n",
    "        reweightCondition2 = ((df['weightsReint'].iloc[ievt] > 60) | (df['weightsReint'].iloc[ievt] < 0)   |\n",
    "                             (df['weightsReint'].iloc[ievt] == np.inf))\n",
    "        df['weightsReint'].iloc[ievt][ reweightCondition2 ] = 1.\n",
    "        \n",
    "        # if no variations exist for the event\n",
    "        if not list(df['weightsReint'].iloc[ievt]): \n",
    "            df['weightsReint'].iloc[ievt] = [1.0 for k in range(1000)]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POT normalization\n",
    "\n",
    "beamon_pot = parameters(ISRUN3)['beamon_pot'] \n",
    "\n",
    "overlay = pot_scale(overlay, 'overlay', ISRUN3)\n",
    "#overlay['pot_scale'] = beamon_pot/2.33828e+21\n",
    "dirt = pot_scale(dirt, 'dirt', ISRUN3)\n",
    "ext = pot_scale(ext, 'ext', ISRUN3)\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue = pot_scale(nue, 'intrinsic', ISRUN3)\n",
    "\n",
    "data['pot_scale'] = [1 for x in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total weights \n",
    "\n",
    "# combined genie * POT weight * flux weight \n",
    "# ext gets POT weight only \n",
    "\n",
    "################################################################\n",
    "# totweight_data scales to BEAMON\n",
    "\n",
    "# tuned\n",
    "overlay['totweight_data'] = overlay['pot_scale']*overlay['ppfx_cv']*overlay['weightSplineTimesTune']\n",
    "dirt['totweight_data'] = dirt['pot_scale']*dirt['ppfx_cv']*dirt['weightSplineTimesTune']\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue['totweight_data'] = nue['pot_scale']*nue['ppfx_cv']*nue['weightSplineTimesTune']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to keep the number of columns the same \n",
    "\n",
    "new_var = ['weightsGenieUnisim', 'totweight_data']\n",
    "\n",
    "for var in new_var: \n",
    "    ext[var] = np.nan\n",
    "    data[var] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace overlay nue CC events with nue intrinsic sample\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    \n",
    "    # intrinsic sample contains in AV TPC events ONLY, & only CC events (overlay is entire cryo)\n",
    "    print(\"# of nueCC in AV in overlay sample = \"+str(len(overlay.query(nueCC_query))))\n",
    "    len1 = len(overlay)\n",
    "    \n",
    "    idx = overlay.query(nueCC_query).index\n",
    "    overlay.drop(idx, inplace=True)\n",
    "    len2 = len(overlay) \n",
    "    print(\"# of nueCC in AV dropped in overlay = \"+str(len1-len2))\n",
    "    \n",
    "    overlay = pd.concat([overlay,nue], ignore_index=True)\n",
    "\n",
    "    # from here on out everything else should be the same. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SW trigger, combine overlay + dirt as MC \n",
    "mc = pd.concat([overlay.query('swtrig_pre==1'),dirt.query('swtrig_pre==1')], ignore_index=True, sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate by in/out FV & cosmic\n",
    "infv = mc.query(in_fv_query)\n",
    "outfv = mc.query(out_fv_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check that everything is accounted for \n",
    "print(len(mc)==len(infv)+len(outfv))#+len(cosmic))\n",
    "\n",
    "if not (len(mc)==len(infv)+len(outfv)):#+len(cosmic)): \n",
    "    d = len(mc) - (len(infv)+len(outfv))#+len(cosmic))\n",
    "    print(d)\n",
    "    \n",
    "     \n",
    "    m = pd.concat([infv, outfv]) #pd.concat([infv, cosmic, outfv])\n",
    "    diff = np.setdiff1d(list(mc.index),list(m.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_signal_weighted = np.nansum(infv.query('is_signal==True')['pot_scale'])\n",
    "print('total signal events = '+ str(tot_signal_weighted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total generated signal events = ', sum(generated_signal(ISRUN3, 'nu_e', 1, 0, 100)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 main categories: \n",
    "\n",
    "# infv - overlay & dirt events with truth vtx in FV \n",
    "# outfv - overlay & dirt events with truth vtx in FV that are classified as neutrinos\n",
    "# cosmic - overlay & dirt events with true vtx in FV that get misclassified as cosmic - no longer used \n",
    "# ext - beam OFF data\n",
    "# data - beam ON data \n",
    "\n",
    "datasets = {\n",
    "    \"infv\": infv, \n",
    "    \"outfv\": outfv, \n",
    "    \"ext\": ext,\n",
    "    \"data\": data\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply BDT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useBDT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useBDT: # load bdt model \n",
    "    bdt_model = xgb.Booster({'nthread': 4})\n",
    "    bdt_model.load_model(parameters(ISRUN3)['bdt_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useBDT: \n",
    "    \n",
    "    datasets_bdt = {}\n",
    "\n",
    "    for i in range(len(datasets)): \n",
    "\n",
    "        df = list(datasets.values())[i].copy()\n",
    "        df = df.query(BDT_LOOSE_CUTS)\n",
    "\n",
    "        # clean datasets \n",
    "        for column in training_parameters:\n",
    "            df.loc[(df[column] < -1.0e37) | (df[column] > 1.0e37), column] = np.nan\n",
    "\n",
    "        # create testing dmatrix \n",
    "        df_test = xgb.DMatrix(data=df[training_parameters])\n",
    "\n",
    "        # apply the bdt selection\n",
    "        preds = bdt_model.predict(df_test)\n",
    "\n",
    "        # add columns for plotting \n",
    "        df['BDT_score'] = preds\n",
    "\n",
    "        datasets_bdt[list(datasets.keys())[i]] = df\n",
    "\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useBDT: \n",
    "    bdt_score_cut = parameters(ISRUN3)['bdt_score_cut']\n",
    "\n",
    "    print(\"BDT SCORE THRESHOLD = \"+str(bdt_score_cut))\n",
    "\n",
    "    selected_query = BDT_LOOSE_CUTS + ' and BDT_score>'+str(bdt_score_cut)\n",
    "    selected_signal_query = selected_query + ' and is_signal==True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat only errors \n",
    "if useBDT: \n",
    "    x = plot_mc('BDT_score', [x*0.1 for x in range(11)], 0, 1, BDT_LOOSE_CUTS, datasets_bdt, \n",
    "                ISRUN3, x_label=\"BDT Score\", norm='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_subtraction = False\n",
    "detsys = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variations = False\n",
    "plot_cov = False\n",
    "\n",
    "datasets_dict = datasets_bdt\n",
    "print(\"make sure to update the datasets dictionary used!\")\n",
    "\n",
    "q = BDT_LOOSE_CUTS +' and BDT_score>' + str(parameters(ISRUN3)['bdt_score_cut'])\n",
    "\n",
    "#'nslice==1 and '+reco_in_fv_query+' and contained_fraction>0.9 and n_tracks_contained>0'\n",
    "\n",
    "#BDT_LOOSE_CUTS + ' and shr_energy_tot_cali>0.07 and -0.9<tksh_angle<0.9'\n",
    "#BDT_LOOSE_CUTS #+' and BDT_score>' + str(parameters(ISRUN3)['bdt_score_cut'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvar = 'tksh_angle'\n",
    "true_var = 'opening_angle'\n",
    "\n",
    "bins = [-1, -0.5, np.cos(100 * np.pi/180), np.cos(80 * np.pi/180), 0.5, 1]\n",
    "x_ticks = [-1, -0.5, -0.174, 0.174, 0.5, 1]\n",
    "\n",
    "\n",
    "xlow = bins[0]\n",
    "xhigh = bins[-1]\n",
    "\n",
    "x_label = \"cos $\\\\theta_{ep}$\"\n",
    "\n",
    "if ISRUN3: \n",
    "    y_label = '$\\\\nu$ / 5.0 $\\\\times 10^{20}$ POT'\n",
    "else: \n",
    "    y_label = '$\\\\nu$ / 2.0 $\\\\times 10^{20}$ POT'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat only -- SCALES TO DATA\n",
    "\n",
    "full_event_rate = plot_mc(xvar, bins, xlow, xhigh, q, datasets_dict, ISRUN3, \n",
    "            norm='data', save=False, \n",
    "                         x_label=x_label, y_label=y_label)['CV']\n",
    "full_event_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgd_event_rate = plot_mc(xvar, bins, xlow, xhigh, q, datasets_dict, ISRUN3, \n",
    "            norm='data', x_label=x_label, save=False)['background_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if background_subtraction: # use the estimated signal event rate only \n",
    "    evt_rate = [x-y for x,y in zip(full_event_rate,bkgd_event_rate)]\n",
    "    \n",
    "else: \n",
    "    evt_rate = full_event_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evt_rate == full_event_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv_nu, ppfx_variations = plotSysVariations(xvar, true_var, bins, xlow, xhigh, q, datasets_dict, 'weightsPPFX',\n",
    "                                         600,ISRUN3, plot=plot_variations, axis_label='Reco '+x_label, \n",
    "                                         pot=str(beamon_pot)+\" POT\", background_subtraction=background_subtraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppfx_dict = calcCov(xvar, bins, ncv_nu, evt_rate, ppfx_variations, plot=plot_cov, save=False, \n",
    "                    axis_label='Reco '+x_label, pot=str(beamon_pot)+\" POT\", isrun3=ISRUN3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppfx_dict['fractional_uncertainty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beamline Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered by beamline variation run number\n",
    "# [+1sigma run #, -1sigma run #]\n",
    "\n",
    "beamline_variations = []\n",
    "\n",
    "beamline_runs = {\n",
    "    'HornCurrent' : [1, 2], \n",
    "    'xHorn1' : [3, 4], \n",
    "    'yHorn1' : [5, 6], \n",
    "    'BeamSpotSize' : [7, 8], \n",
    "    'xHorn2' : [9, 10], \n",
    "    'yHorn2' : [11, 12], \n",
    "    'WaterOnHorns' : [13, 14], \n",
    "    'xBeamShift' : [15, 16], \n",
    "    'yBeamShift' : [17, 18], \n",
    "    'zTargetPosition' : [19, 20]    \n",
    "}\n",
    "\n",
    "beamline_cov = {}\n",
    "\n",
    "# index in weightsNuMIGeo are offset by -1\n",
    "\n",
    "for variation in beamline_runs.keys(): \n",
    "    \n",
    "    idx = [i-1 for i in beamline_runs[variation]]\n",
    "    print(idx)\n",
    "    \n",
    "    ncv_nu, variations = plotSysVariations(xvar, true_var, bins, xlow, xhigh, q, datasets_dict, 'weightsNuMIGeo', \n",
    "                                                 idx, ISRUN3, plot=plot_variations, \n",
    "                                                 axis_label='Reco '+x_label, pot=str(beamon_pot)+\" POT\", \n",
    "                                                  background_subtraction=background_subtraction, title=variation)\n",
    "    \n",
    "    beamline_variations.append([list(a) for a in variations])\n",
    "    \n",
    "    # calc covariance \n",
    "    beamline_cov[variation] = calcCov(xvar, bins, ncv_nu, evt_rate, \n",
    "                                      variations, plot=plot_cov, save=False, \n",
    "                    axis_label='Reco '+x_label, pot=str(beamon_pot)+\" POT\", isrun3=ISRUN3)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total covariance, correlation, & uncertainty \n",
    "\n",
    "cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "cor = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for variation in beamline_cov.keys(): \n",
    "    \n",
    "    for i in range(len(bins)-1): \n",
    "        for j in range(len(bins)-1):\n",
    "            \n",
    "            cov[i][j] += beamline_cov[variation]['cov'][i][j]\n",
    "            frac_cov[i][j] += beamline_cov[variation]['frac_cov'][i][j] \n",
    "\n",
    "            \n",
    "for i in range(len(bins)-1): \n",
    "    for j in range(len(bins)-1):\n",
    "        \n",
    "        if np.sqrt(cov[i][i])*np.sqrt(cov[j][j]) != 0: \n",
    "                cor[i][j] = cov[i][j] / (np.sqrt(cov[i][i])*np.sqrt(cov[j][j]))\n",
    "            \n",
    "beamline_dict = {\n",
    "    'cov' : cov, \n",
    "    'frac_cov' : frac_cov,\n",
    "    'cor' : cor,\n",
    "    'fractional_uncertainty' : np.sqrt(np.diag(frac_cov))\n",
    "} \n",
    "\n",
    "beamline_dict['fractional_uncertainty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENIE multisims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(overlay.weightsGenie[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv_nu, genie_variations = plotSysVariations(xvar, true_var, bins, xlow, xhigh, q, datasets_dict, 'weightsGenie', 500, \n",
    "                                         ISRUN3, plot=plot_variations, axis_label='Reco '+x_label, \n",
    "                                          pot=str(beamon_pot)+\" POT\", \n",
    "                                              background_subtraction=background_subtraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genie_dict = calcCov(xvar, bins, ncv_nu, evt_rate, genie_variations, plot=plot_cov, save=False, \n",
    "                    axis_label='Reco '+x_label, pot=str(beamon_pot)+\" POT\", isrun3=ISRUN3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genie_dict['fractional_uncertainty']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENIE unisims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# divide the tune weight out of everything except SCC variations\n",
    "# don't divide the tune weight out of SCC variations \n",
    "\n",
    "genie_unisim_variations = ['RPA', \n",
    "                           'CCMEC', 'AxFFCCQE', 'VecFFCCQE', 'DecayAngMEC', 'ThetaDelta2Npi', 'ThetaDelta2NRad', \n",
    "                          'NormCCCOH', 'NormNCCOH', \n",
    "                          'xsr_scc_Fv3', 'xsr_scc_Fa3']\n",
    "\n",
    "\n",
    "genie_unisim_cov = {}\n",
    "\n",
    "genie_us_variations = []\n",
    "\n",
    "for knob in genie_unisim_variations: \n",
    "    \n",
    "    if knob == 'RPA': \n",
    "        idx = [sys_genie[2:].index('knobRPAup'), sys_genie[2:].index('knobRPAdn')]\n",
    "    \n",
    "    else: \n",
    "        idx = [sys_genie[2:].index('knob'+knob+'up')]\n",
    "    \n",
    "    ncv_nu, variations = plotSysVariations(xvar, true_var, bins, xlow, xhigh, q, datasets_dict, 'weightsGenieUnisim', \n",
    "                                        idx, ISRUN3, plot=plot_variations, axis_label='Reco '+x_label, \n",
    "                                        pot=str(beamon_pot)+\" POT\", \n",
    "                                        background_subtraction=background_subtraction, title=knob)\n",
    "    \n",
    "    genie_us_variations.append([list(a) for a in variations])\n",
    "    \n",
    "    # calc covariance \n",
    "    genie_unisim_cov[knob] = calcCov(xvar, bins, ncv_nu, evt_rate, variations, plot=plot_cov, save=False, \n",
    "                    axis_label='Reco '+x_label, pot=str(beamon_pot)+\" POT\", isrun3=ISRUN3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total covariance, correlation, & uncertainty \n",
    "\n",
    "cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "cor = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for variation in genie_unisim_cov.keys(): \n",
    "    \n",
    "    for i in range(len(bins)-1): \n",
    "        for j in range(len(bins)-1):\n",
    "            \n",
    "            cov[i][j] += genie_unisim_cov[variation]['cov'][i][j]\n",
    "            frac_cov[i][j] += genie_unisim_cov[variation]['frac_cov'][i][j] \n",
    "\n",
    "            \n",
    "for i in range(len(bins)-1): \n",
    "    for j in range(len(bins)-1):\n",
    "        \n",
    "        if np.sqrt(cov[i][i])*np.sqrt(cov[j][j]) != 0: \n",
    "                cor[i][j] = cov[i][j] / (np.sqrt(cov[i][i])*np.sqrt(cov[j][j]))\n",
    "        \n",
    "            \n",
    "genie_unisim_dict = {\n",
    "    'cov' : cov, \n",
    "    'frac_cov' : frac_cov,\n",
    "    'cor' : cor,\n",
    "    'fractional_uncertainty' : np.sqrt(np.diag(frac_cov))\n",
    "} \n",
    "\n",
    "genie_unisim_dict['fractional_uncertainty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEANT4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv_nu, geant4_variations = plotSysVariations(xvar, true_var, bins, xlow, xhigh, q, datasets_dict, 'weightsReint', 1000, \n",
    "                                         ISRUN3, plot=plot_variations, axis_label='Reco '+x_label, pot=str(beamon_pot)+\" POT\", \n",
    "                                              background_subtraction=background_subtraction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geant4_dict = calcCov(xvar, bins, ncv_nu, evt_rate, geant4_variations, plot=plot_cov, save=False, \n",
    "                    axis_label='Reco '+x_label, pot=str(beamon_pot)+\" POT\", isrun3=ISRUN3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geant4_dict['fractional_uncertainty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector Systematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ROOT file with BDT-selected detector variations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detsys_flat = True\n",
    "recreate_file = False\n",
    "\n",
    "#detvar_file = \"detsys_08122022.root\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not detsys_flat: \n",
    "\n",
    "    if ISRUN3: \n",
    "        detvar = detvar_run3_rhc.keys()\n",
    "\n",
    "    else: \n",
    "        detvar = detvar_run1_fhc.keys()\n",
    "        \n",
    "    # skip this step if it is already created\n",
    "    # should manually delete the file first \n",
    "    # (located here: /uboone/data/users/kmiller/uBNuMI_CCNp/ntuples/runX/systematics/detvar/)\n",
    "\n",
    "    # scales to the det sys CV POT (standard overlay)\n",
    "\n",
    "    if recreate_file: \n",
    "        for v in list(detvar): \n",
    "            NuMIDetSysWeights.makehist_detsys(v, ISRUN3, detvar_file, xvar, bins, cut=q, useBDT=True, \n",
    "                                             background_subtraction=background_subtraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not detsys_flat:\n",
    "\n",
    "    detector_variations = NuMIDetSysWeights.plot_variations(xvar, bins, \n",
    "                                                            detvar_file,  \n",
    "                                                            ISRUN3, axis_label=x_label, \n",
    "                                                            plot=True, background_subtraction=background_subtraction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total covariance, correlation, & uncertainty \n",
    "\n",
    "cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "cor = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "detsys_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute covariance (N=1 for each variation)\n",
    "# detvar are scaled to beam on POT at this point\n",
    "\n",
    "if not detsys_flat:\n",
    "\n",
    "    detsys_cov = {}\n",
    "\n",
    "    for variation in detector_variations.keys(): \n",
    "\n",
    "        if variation=='CV': \n",
    "            continue\n",
    "\n",
    "        # calc covariance for each unisim \n",
    "        detsys_cov[variation] = calcCov(xvar, bins, detector_variations['CV'], detector_variations['CV'], \n",
    "                                        [detector_variations[variation]], \n",
    "                                        plot=False, save=False, axis_label='Reco '+x_label, pot=str(beamon_pot)+\" POT\", \n",
    "                                        isrun3=ISRUN3)\n",
    "        \n",
    "    for i in range(len(bins)-1): \n",
    "        for j in range(len(bins)-1):\n",
    "            \n",
    "            cov[i][j] = sum([detsys_cov[x]['cov'][i][j] for x in detsys_cov.keys()])\n",
    "            frac_cov[i][j] = sum([detsys_cov[x]['frac_cov'][i][j] for x in detsys_cov.keys()])\n",
    "\n",
    "    for i in range(len(bins)-1): \n",
    "        for j in range(len(bins)-1):\n",
    "\n",
    "            if np.sqrt(cov[i][i])*np.sqrt(cov[j][j]) != 0: \n",
    "                    cor[i][j] = cov[i][j] / (np.sqrt(cov[i][i])*np.sqrt(cov[j][j]))\n",
    "\n",
    "    detsys_dict['cov'] = cov\n",
    "    detsys_dict['frac_cov'] = frac_cov\n",
    "    detsys_dict['cor'] = cor\n",
    "    detsys_dict['fractional_uncertainty'] = np.sqrt(np.diag(frac_cov))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if detsys_flat:\n",
    "    \n",
    "    detsys_dict['fractional_uncertainty'] = []\n",
    "    \n",
    "    for i in range(len(bins)-1): \n",
    "        \n",
    "        if evt_rate[i] != 0: \n",
    "            detsys_dict['fractional_uncertainty'].append(parameters(ISRUN3)['detsys_flat'])\n",
    "            frac_cov[i][i] = parameters(ISRUN3)['detsys_flat']**2\n",
    "        else: \n",
    "            detsys_dict['fractional_uncertainty'].append(0)\n",
    "            \n",
    "                \n",
    "    \n",
    "    print((detsys_dict['fractional_uncertainty']))\n",
    "    \n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detsys_dict['frac_cov'] = frac_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stat Uncertainty of the MC event count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if data/MC comparisons :  uncertainty on the full estimated event rate \n",
    "# if closure test : uncertainty on the MC background only  (?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of the weights squared\n",
    "\n",
    "mc_stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "mc_frac_stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "# also save sumw2 of the estimated signal and neutrino background\n",
    "mc_signal_sumw2 = []\n",
    "mc_bkgd_sumw2 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data/mc comparisons\n",
    "if background_subtraction==False: \n",
    "    \n",
    "    # stat uncertainty for MC - signal + background \n",
    "    ncv = pd.concat([datasets_dict['infv'].copy().query(q), \n",
    "                             datasets_dict['outfv'].copy().query(q)], ignore_index=True)  \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if background_subtraction==True: \n",
    "\n",
    "    # NuWro fake data test: take sum of weights squared for MC bkgd+EXT only (add NuWro event count contribution later...)\n",
    "    # beam on data: take sum of weights squared for MC bkgd+EXT only (add beam-on event count contribution later...)\n",
    "    if isData==True: \n",
    "        ncv = pd.concat([datasets_dict['infv'].copy().query(q+' and is_signal==False'), \n",
    "                                         datasets_dict['outfv'].copy().query(q+' and is_signal==False')], \n",
    "                                ignore_index=True) \n",
    "\n",
    "    # GENIE closure test: take sum of weights squared for the full event count \n",
    "    else: \n",
    "        ncv = pd.concat([datasets_dict['infv'].copy().query(q), \n",
    "                             datasets_dict['outfv'].copy().query(q)], ignore_index=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bins)-1):\n",
    "\n",
    "    if i==len(bins)-2: \n",
    "        bin_query = xvar+'>='+str(bins[i])+' and '+xvar+'<='+str(bins[i+1])\n",
    "    else: \n",
    "        bin_query = xvar+'>='+str(bins[i])+' and '+xvar+'<'+str(bins[i+1])\n",
    "        \n",
    "    mc_stat_cov[i][i] = sum(ncv.query(bin_query).totweight_data ** 2) \n",
    "    \n",
    "    mc_signal_sumw2.append(sum(datasets_dict['infv'].copy().query(q+' and is_signal==True and '+bin_query).totweight_data ** 2) )\n",
    "    mc_bkgd_sumw2.append( sum(pd.concat([datasets_dict['infv'].copy().query(q), \n",
    "                                        datasets_dict['outfv'].copy().query(q)], ignore_index=True).query(\"is_signal==False and \"+bin_query).totweight_data **2 ))\n",
    "\n",
    "    if mc_stat_cov[i][i] != 0: \n",
    "        mc_frac_stat_cov[i][i] = mc_stat_cov[i][i]/ evt_rate[i]**2 \n",
    "    \n",
    "    bin_query = ''\n",
    "    \n",
    "mc_stat_percent_error = np.sqrt(np.diag(mc_frac_stat_cov))\n",
    "mc_stat_percent_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mc_bkgd_sumw2_reweight = [a*b for a,b in zip(mc_bkgd_sumw2,nuwro_to_genie)]\n",
    "#mc_bkgd_sumw2_reweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_cov: \n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "        \n",
    "    plt.pcolor(bins, bins, mc_stat_cov, cmap='OrRd', edgecolors='k' )\n",
    "\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    plt.xlabel(\"Reco \"+x_label, fontsize=15)\n",
    "    plt.ylabel(\"Reco \"+x_label, fontsize=15)\n",
    "\n",
    "    plt.title('MC Statistical Covariance', fontsize=15)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "        \n",
    "    plt.pcolor(bins, bins, mc_frac_stat_cov, cmap='OrRd', edgecolors='k' )\n",
    "\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    plt.xlabel(\"Reco \"+x_label, fontsize=15)\n",
    "    plt.ylabel(\"Reco \"+x_label, fontsize=15)\n",
    "\n",
    "    plt.title('MC Fractional Statistical Covariance', fontsize=15)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stat Uncertainty Beam On & EXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXT uses sum of the weights squared \n",
    "selected_ext = datasets_dict['ext'].copy().query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "ext_frac_stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "    \n",
    "for i in range(len(bins)-1):\n",
    "\n",
    "    if i==len(bins)-2: \n",
    "        bin_query = xvar+' >= '+str(bins[i])+' and '+xvar+' <= '+str(bins[i+1])\n",
    "    else: \n",
    "        bin_query = xvar+' >= '+str(bins[i])+' and '+xvar+' < '+str(bins[i+1])\n",
    "        \n",
    "    ext_stat_cov[i][i] = sum(selected_ext.query(bin_query).pot_scale ** 2) \n",
    "    \n",
    "    if ext_stat_cov[i][i] != 0: \n",
    "        ext_frac_stat_cov[i][i] = ext_stat_cov[i][i]/ evt_rate[i]**2 \n",
    "    \n",
    "    bin_query = ''\n",
    "    \n",
    "ext_stat_percent_error = np.sqrt(np.diag(ext_frac_stat_cov))\n",
    "ext_stat_percent_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_sumw2 = np.diagonal(ext_stat_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ext = plt.hist(selected_ext[xvar], bins, range=[bins[0], bins[-1]],\n",
    "         weights=selected_ext.pot_scale, color='gainsboro')[0]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beamon_frac_stat_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "    \n",
    "selected_data_counts = plt.hist(datasets_dict['data'].query(q)[xvar], bins, range=[bins[0], bins[-1]])[0]\n",
    "plt.xlim(xlow,xhigh)\n",
    "plt.show()\n",
    "    \n",
    "if background_subtraction==True: \n",
    "    selected_data_counts = [a-b for a,b in zip(selected_data_counts,bkgd_event_rate)]\n",
    "    \n",
    "print(sum(selected_data_counts))\n",
    "    \n",
    "for i in range(len(bins)-1): \n",
    "    \n",
    "    if selected_data_counts[i] != 0: \n",
    "        beamon_frac_stat_cov[i][i] = selected_data_counts[i]/(selected_data_counts[i]**2)\n",
    "\n",
    "beamon_stat_percent_error = np.sqrt(np.diag(beamon_frac_stat_cov))\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(datasets_dict['infv'].query(q)[xvar], bins, \n",
    "         range=[bins[0], bins[-1]], weights=datasets_dict['infv'].query(q).totweight_data)[0]\n",
    "plt.xlim(xlow,xhigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC Response Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for the signal channel ONLY \n",
    "\n",
    "if background_subtraction: \n",
    "\n",
    "    selected_signal_df = datasets_dict['infv'].query(selected_signal_query).copy()\n",
    "    selected_signal_df['seed'] = selected_signal_df.apply( lambda x: ConcatRunSubRunEvent(x['run'], x['sub'], x['evt']), axis=1 )\n",
    "    selected_signal_df['weightsPoisson'] = selected_signal_df.apply( lambda x: PoissonRandomNumber(x['seed'], mean=1.0, size=1000), axis=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if background_subtraction: \n",
    "\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "    # histogram bin counts for all universes\n",
    "    rmatrix_uni_counts = []\n",
    "\n",
    "    for u in range(1000): \n",
    "\n",
    "        # multiply in with sys weight of universe u \n",
    "        sys_weight = list(selected_signal_df['weightsPoisson'].str.get(u))\n",
    "        total_weight = [ x*y for x, y in zip(sys_weight, selected_signal_df['totweight_data']) ]\n",
    "\n",
    "        n, b, p = plt.hist(selected_signal_df[xvar], bins, histtype='step', weights=total_weight, \n",
    "                                linewidth=0.5, color='cornflowerblue')  \n",
    "\n",
    "        rmatrix_uni_counts.append(list(n))\n",
    "\n",
    "    ncv, bcv, pcv = plt.hist(selected_signal_df[xvar], bins, histtype='step', \n",
    "                             weights=selected_signal_df['totweight_data'], linewidth=2, color='black')      \n",
    "\n",
    "    plt.xticks(x_ticks, fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    plt.xlim(xlow, xhigh)\n",
    "\n",
    "    plt.xlabel('Reco '+x_label, fontsize=15)\n",
    "    plt.ylabel(\"\", fontsize=15)\n",
    "\n",
    "    plt.title('weightsPoisson', fontsize=16)    \n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if background_subtraction: \n",
    "    response_matrix_uncertainty = calcCov(xvar, bins, ncv, evt_rate, rmatrix_uni_counts, isrun3=ISRUN3, plot=False, \n",
    "                   save=False, axis_label='Reco '+x_label, pot=parameters(ISRUN3)['beamon_pot'])\n",
    "    \n",
    "    response_matrix_uncertainty['fractional_uncertainty']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POT Counting (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters(ISRUN3)['beamon_pot'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot_counting = pot_unisims(xvar, evt_rate, bins, 0.02, ISRUN3, plot=plot_variations, x_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirt (100%)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected dirt uncertainty \n",
    "# vary the dirt interactions by 100% (1 unisim) on the event rate \n",
    "\n",
    "selected_dirt = plt.hist(datasets_dict['outfv'].copy().query(q+' and isDirt==1')[xvar], \n",
    "                         bins, \n",
    "                        weights=datasets_dict['outfv'].copy().query(q+' and isDirt==1')['pot_scale'], \n",
    "                         color='orchid')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt_uncertainty = dirt_unisim(xvar, bins, evt_rate, selected_dirt, 1.0, isrun3=ISRUN3, plot=True, \n",
    "                               x_label=None, title=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt_uncertainty['fractional_uncertainty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dirt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Sources of Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_cov_dict = {\n",
    "    'ppfx' : ppfx_dict['frac_cov'], \n",
    "    'beamline' : beamline_dict['frac_cov'], \n",
    "    'genie_ms' : genie_dict['frac_cov'], \n",
    "    'genie_us': genie_unisim_dict['frac_cov'], \n",
    "    'geant4' : geant4_dict['frac_cov'],\n",
    "    'pot_counting' : pot_counting['frac_cov'], \n",
    "    'dirt' : dirt_uncertainty['frac_cov'],\n",
    "    'mc_stat' : mc_frac_stat_cov, # either the full distribution or just background events \n",
    "    'ext_stat' : ext_frac_stat_cov\n",
    "}\n",
    "\n",
    "\n",
    "if background_subtraction: \n",
    "    frac_cov_dict['response_matrix'] = response_matrix_uncertainty['frac_cov']\n",
    "    frac_cov_dict['beamon_stat'] = beamon_frac_stat_cov\n",
    "    \n",
    "if detsys: \n",
    "    frac_cov_dict['detector'] = detsys_dict['frac_cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = evt_rate\n",
    "\n",
    "tot_frac_cov, tot_abs_cov = plotFullCov(frac_cov_dict, xvar, cv, bins, xlow, xhigh, save=False, \n",
    "                      axis_label='Reco '+x_label, isrun3=ISRUN3, pot=str(beamon_pot)+' POT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ppfx & beamline geometry in quadrature\n",
    "frac_cov_dict['flux'] = [ [x+y for x,y in zip(a,b)] for a,b in zip(frac_cov_dict['ppfx'], frac_cov_dict['beamline'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add genie in quadrature\n",
    "frac_cov_dict['genie_all'] = [ [x+y for x,y in zip(a,b)] for a,b in zip(frac_cov_dict['genie_ms'], frac_cov_dict['genie_us'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stat stuff in quadrature \n",
    "\n",
    "if background_subtraction: \n",
    "    \n",
    "    if isData: # NuMI/NuWro studies\n",
    "        frac_cov_dict['stat_all'] = [ [w+x+y+z for w,x,y,z in zip(a,b,c,d)] for a,b,c,d in zip( frac_cov_dict['beamon_stat'], frac_cov_dict['response_matrix'], frac_cov_dict['mc_stat'], frac_cov_dict['ext_stat'])]\n",
    "    \n",
    "    else:  # GENIE closure studies\n",
    "        frac_cov_dict['stat_all'] = [ [x+y+z for x,y,z in zip(b,c,d)] for b,c,d in zip( frac_cov_dict['response_matrix'], frac_cov_dict['mc_stat'], frac_cov_dict['ext_stat'])]\n",
    "\n",
    "# for mc/data comparisons\n",
    "else: \n",
    "    frac_cov_dict['stat_all'] = [ [x+y for x,y in zip(a,b)] for a,b in zip(frac_cov_dict['mc_stat'], frac_cov_dict['ext_stat'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean away nans\n",
    "v = np.array(tot_frac_cov)\n",
    "v[np.isnan(v)] = 0\n",
    "tot_frac_cov = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean away nans\n",
    "v = np.array(tot_abs_cov)\n",
    "v[np.isnan(v)] = 0\n",
    "tot_abs_cov = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_unc_dict = {\n",
    "    'flux' : np.sqrt(np.diagonal(frac_cov_dict['flux'])), \n",
    "    'genie' : np.sqrt(np.diagonal(frac_cov_dict['genie_all'])), \n",
    "    'geant4' : np.sqrt(np.diagonal(frac_cov_dict['geant4'])),\n",
    "    'pot_counting' : np.sqrt(np.diagonal(frac_cov_dict['pot_counting'])), \n",
    "    'dirt' : np.sqrt(np.diagonal(frac_cov_dict['dirt'])),\n",
    "    'stat' : np.sqrt(np.diagonal(frac_cov_dict['stat_all'])), # does not include beam on STAT \n",
    "    'total' : np.sqrt(np.diagonal(tot_frac_cov))\n",
    "}\n",
    "\n",
    "if detsys: \n",
    "    frac_unc_dict['detector'] = np.sqrt(np.diagonal(frac_cov_dict['detector']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_unc_dict['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_unc = [0 for i in range(len(bins)-1)]\n",
    "\n",
    "for source in frac_unc_dict.keys(): \n",
    "    \n",
    "    if source=='total': \n",
    "        continue\n",
    "    \n",
    "\n",
    "    # square the list \n",
    "    squared = [x**2 for x in frac_unc_dict[source]]\n",
    "    \n",
    "    # add in quadrature \n",
    "    tot_unc = [a+b for a,b in zip(tot_unc, squared)]\n",
    "   \n",
    "tot_unc = np.sqrt(np.array(tot_unc))\n",
    "tot_unc\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bincenters = 0.5*(np.array(bins)[1:]+np.array(bins)[:-1])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))  \n",
    "\n",
    "# TOTAL \n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"Total\",\n",
    "        weights=frac_unc_dict['total'], linewidth=1.5, color='black')\n",
    "\n",
    "# FLUX\n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"Flux\", \n",
    "         weights=frac_unc_dict['flux'], color='royalblue')\n",
    "\n",
    "# CROSS SECTION MODELS \n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"GENIE\", \n",
    "         weights=frac_unc_dict['genie'], color='goldenrod')\n",
    "\n",
    "#plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"GENIE (us)\", \n",
    "#         weights=np.sqrt(np.diag(frac_cov_dict['genie_us'])), color='goldenrod')\n",
    "\n",
    "#plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"GENIE (ms)\", \n",
    "#         weights=np.sqrt(np.diag(frac_cov_dict['genie_ms'])), color='goldenrod', linestyle='--')\n",
    "\n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"GEANT4\", \n",
    "         weights=frac_unc_dict['geant4'], color='green')\n",
    "\n",
    "# DETECTOR \n",
    "if detsys: \n",
    "    plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"Detector\", \n",
    "         weights=frac_unc_dict['detector'], color='crimson')\n",
    "\n",
    "# POT COUNTING \n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"POT counting\",\n",
    "        weights=frac_unc_dict['pot_counting'], color='purple')\n",
    "\n",
    "# DIRT \n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"Dirt\",\n",
    "        weights=frac_unc_dict['dirt'], color='brown')\n",
    "\n",
    "# STATISTICAL \n",
    "plt.hist(bincenters, bins, histtype='step', range=[bins[0], bins[-1]], label=\"Stat\",\n",
    "        weights=frac_unc_dict['stat'], color='hotpink')\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.xlabel(\"Reco \" + x_label, fontsize=15)\n",
    "plt.ylabel(\"Fractional Uncertainty\", fontsize=15)\n",
    "\n",
    "plt.xlim(bins[0], xhigh)\n",
    "#plt.ylim(0, .4)\n",
    "\n",
    "plt.legend(fontsize=13, frameon=False, ncol=3)\n",
    "\n",
    "\n",
    "if background_subtraction: \n",
    "    if ISRUN3: \n",
    "        plt.title(\"Uncertainty on the RHC Background-Selected Event Rate\", fontsize=16)\n",
    "    else: \n",
    "        plt.title(\"Uncertainty on the FHC Background-Selected Event Rate\", fontsize=16)\n",
    "        \n",
    "else: \n",
    "    if ISRUN3: \n",
    "        plt.title(\"Uncertainty on the RHC Selected Event Rate (MC+EXT)\", fontsize=16)\n",
    "    else: \n",
    "        plt.title(\"Uncertainty on the FHC Selected Event Rate (MC+EXT)\", fontsize=16)   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save covariance to unfolding file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if background_subtraction: \n",
    "    print(\"make sure to change file name!\")\n",
    "    \n",
    "    variations_dict = {\n",
    "        'evt_rate' : evt_rate, \n",
    "        'beamon_counts' : list(selected_data_counts), \n",
    "        'ppfx' : ppfx_variations, \n",
    "        'beamline' : beamline_variations, \n",
    "        'genie_ms' : genie_variations, \n",
    "        'genie_us' : genie_us_variations, \n",
    "        'geant4' : geant4_variations,\n",
    "        'pot_counting' : pot_counting['variations'], \n",
    "        'dirt' : dirt_uncertainty['variations'], \n",
    "        'response_matrix' : rmatrix_uni_counts, \n",
    "        'cv_dirt' : list(selected_dirt), \n",
    "        'cv_bkgd' : bkgd_event_rate, # total background event rate (MC+EXT)\n",
    "        'cv_ext' : list(cv_ext),\n",
    "        'mc_bkgd_sumw2' : list(mc_bkgd_sumw2), # need to take the square root for fractional uncertainty \n",
    "        'ext_sumw2' : list(ext_sumw2), # need to take the square root for fractional uncertainty \n",
    "        'mc_signal_sumw2' : mc_signal_sumw2\n",
    "    }\n",
    "    \n",
    "\n",
    "else: \n",
    "    print(\"make sure to change file name!\")\n",
    "    \n",
    "    variations_dict = {\n",
    "        'full_evt_rate' : evt_rate, \n",
    "        'beamon_full_evt_rate' : list(selected_data_counts), \n",
    "        'ppfx' : [list(a) for a in ppfx_variations], \n",
    "        'beamline' : beamline_variations, \n",
    "        'genie_ms' : [list(a) for a in genie_variations], \n",
    "        'genie_us' : genie_us_variations, \n",
    "        'geant4' : [list(a) for a in geant4_variations],\n",
    "        'pot_counting' : pot_counting['variations'], \n",
    "        'dirt' : dirt_uncertainty['variations'], \n",
    "        'cv_dirt' : list(selected_dirt), \n",
    "        'cv_bkgd' : bkgd_event_rate, # total background event rate (MC+EXT)\n",
    "        'cv_ext' : list(cv_ext), # just the EXT background \n",
    "        'mc_bkgd_sumw2' : list(mc_bkgd_sumw2), # need to take the square root for fractional uncertainty \n",
    "        'ext_sumw2' : list(ext_sumw2), # need to take the square root for fractional uncertainty \n",
    "        'mc_signal_sumw2' : mc_signal_sumw2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date  \n",
    "import json\n",
    "import os\n",
    "    \n",
    "path = 'unfolding/variations/full_event_rates/'\n",
    "if ISRUN3: \n",
    "    filename = 'RHCVariations_FullEvtRate_'+xvar+\"_\"+date.today().strftime(\"%m%d%y\")+\".json\"\n",
    "    \n",
    "else: \n",
    "    filename = 'FHCVariations_FullEvtRate_'+xvar+\"_\"+date.today().strftime(\"%m%d%y\")+\".json\"\n",
    "    \n",
    "if os.path.exists(path+filename): \n",
    "    print(filename, \" exists and is readable, need to update file name to save ! \")\n",
    "        \n",
    "else: \n",
    "    with open(path+filename, 'w') as f:\n",
    "        json.dump(variations_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modify existing json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ISRUN3: \n",
    "    with open('unfolding/variations/RHCVariations_'+xvar+'.json') as f:\n",
    "        variations_dict = json.load(f)\n",
    "        \n",
    "else: \n",
    "    with open('unfolding/variations/FHCVariations_'+xvar+'.json') as f:\n",
    "        variations_dict = json.load(f)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variations_dict['beamon_counts'] = list(selected_data_counts)\n",
    "#variations_dict['beamon_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variations_dict['beamon_counts'] = list(selected_data_counts)\n",
    "#variations_dict['beamon_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ISRUN3: \n",
    "    with open('unfolding/variations/RHCVariations_'+xvar+'.json', 'w') as f:\n",
    "        json.dump(variations_dict, f)\n",
    "        \n",
    "else: \n",
    "    with open('unfolding/variations/FHCVariations_'+xvar+'.json', 'w') as f:\n",
    "        json.dump(variations_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data/MC Comparisons -- before background subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sf)\n",
    "from selection_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(top)\n",
    "from top import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = plot_mc(xvar, bins, xlow, xhigh, q, datasets_dict, ISRUN3, \n",
    "            norm='data',\n",
    "            x_label='Reconstructed '+x_label,\n",
    "            save=False, \n",
    "            y_label=y_label,sys=frac_unc_dict['total'], xtext=0.9, ytext=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ISRUN3: \n",
    "    chi2_label = \"RHC RUN 3\"\n",
    "    save_label = \"rhc_bdtcut_data\"\n",
    "    beamon_pot_str = \"5.0 $\\\\times 10^{20}$\"\n",
    "    \n",
    "else: \n",
    "    chi2_label = \"FHC RUN 1\"\n",
    "    save_label = \"fhc_bdtcut_data\"\n",
    "    beamon_pot_str = \"2.0 $\\\\times 10^{20}$\"\n",
    "    \n",
    "print(\"make sure to update save label!\")\n",
    "print(\"save label = \", save_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the chi2 \n",
    "\n",
    "if not background_subtraction: \n",
    "    selected_data = plt.hist(datasets_dict['data'].copy().query(q)[xvar], bins)[0]\n",
    "    plt.close()\n",
    "\n",
    "    # inverse cov -- make sure to include the beam on stat covariance! \n",
    "    tot_cov = np.array(tot_frac_cov)+np.array(beamon_frac_stat_cov)\n",
    "\n",
    "    for i in range(len(bins)-1): \n",
    "         for j in range(len(bins)-1): \n",
    "                tot_cov[i][j] = tot_cov[i][j] * evt_rate[i] * evt_rate[j]\n",
    "\n",
    "    tot_inverse_cov = np.linalg.inv(tot_cov)\n",
    "\n",
    "    ## check \n",
    "    plt.pcolor(bins, bins, np.matmul(tot_cov, tot_inverse_cov), cmap='OrRd', edgecolors='k')\n",
    "    plt.xlim(xlow,xhigh)\n",
    "    plt.ylim(xlow,xhigh)\n",
    "    cbar = plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    chi2 = 0\n",
    "\n",
    "    for i in range(len(bins)-1):  \n",
    "        for j in range(len(bins)-1):  \n",
    "                chi2 = chi2  + ( (evt_rate[i]-selected_data[i])*tot_inverse_cov[i][j]*(evt_rate[j]-selected_data[j]) )\n",
    "    chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys now takes an array of the total uncertainty\n",
    "\n",
    "d = plot_data(xvar, bins, xlow, xhigh, q, datasets_dict, ISRUN3, \n",
    "                  save=False, \n",
    "                  save_label=save_label,  \n",
    "                  x_label=x_label, ncol=3, ymax=50,\n",
    "                  y_label=beamon_pot_str, x_ticks=x_ticks,\n",
    "                  sys=frac_unc_dict['total'], \n",
    "                  text=chi2_label+\"\\n$\\\\chi^{2}$/n = \"+str(round(chi2, 1))+\"/\"+str(len(bins)-1), \n",
    "                  xtext=.85, ytext=23.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NuMI Oscillations (3+1 Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdated \n",
    "\n",
    "x = plot_mc(xvar, [round(0.01*x, 2) for x in range(0, 75, 5)], 0, 0.7, 'BDT_score>0.575', datasets_bdt, ISRUN3, \n",
    "        plt_norm='proj', pot='$9.23\\\\times10^{20}$', ymax=30, x_label='True Neutrino Energy [GeV]', \n",
    "            osc='machado_bestfit.csv')\n",
    "\n",
    "# osc='biggest_variation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create projected oscillation dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load a stored dictionary \n",
    "with open('outdated/FHC_Projected_TrueNeutrinoEnergy.json') as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 4.5, 46) #d['bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = plot_mc('nu_e', bins, 0, 5, selected_query, datasets_bdt, \n",
    "            ISRUN3, x_label=\"Reco $\\\\nu$ Energy [GeV]\", norm='data', pot='$2.0\\\\times10^{20}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['bins'] = bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot_scale = 9.23E20/parameters(ISRUN3)['beamon_pot']\n",
    "print(pot_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['CV'] = [k*pot_scale for k in x['CV']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv, ppfx_variations = plotSysVariations('nu_e', 'nu_e', bins, bins[0], bins[-1], selected_query, datasets_bdt, 'weightsPPFX',600, \n",
    "                                         ISRUN3, plot=False, axis_label='True Neutrino Energy [GeV]', pot='$2.0 x 10^{20}$ POT', \n",
    "                                              background_subtraction=False)\n",
    "\n",
    "ppfx_dict = calcCov('nu_e', bins, ncv, ppfx_variations, plot=False, save=False, \n",
    "                    axis_label='True Neutrino Energy [GeV] ', pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3, title='Hadron Production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['ppfx_cov_frac'] = ppfx_dict['frac_cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv, genie_variations = plotSysVariations('nu_e', 'nu_e', bins, bins[0], bins[-1], selected_query, datasets_bdt, 'weightsGenie',600, \n",
    "                                         ISRUN3, plot=True, axis_label='True Neutrino Energy [GeV]', pot='$2.0 x 10^{20}$ POT', \n",
    "                                              background_subtraction=False)\n",
    "\n",
    "genie_dict = calcCov('nu_e', bins, ncv, genie_variations, plot=False, save=False, \n",
    "                    axis_label='True Neutrino Energy [GeV] ', pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3, title='Hadron Production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['genie_cov_frac'] = genie_dict['frac_cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncv, geant4_variations = plotSysVariations('nu_e', 'nu_e', bins, bins[0], bins[-1], selected_query, datasets_bdt, 'weightsReint',1000, \n",
    "                                         ISRUN3, plot=True, axis_label='True Neutrino Energy [GeV]', pot='$2.0 x 10^{20}$ POT', \n",
    "                                              background_subtraction=False)\n",
    "\n",
    "\n",
    "geant4_dict = calcCov('nu_e', bins, ncv, geant4_variations, plot=False, save=False, \n",
    "                    axis_label='True Neutrino Energy [GeV] ', pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3, title='Hadron Production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['reint_cov_frac'] = geant4_dict['frac_cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## detector variations -- make new file \n",
    "recreate_file=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if recreate_file: \n",
    "    for v in list(detvar_run1_fhc.keys()): \n",
    "        NuMIDetSysWeights.makehist_detsys(v, ISRUN3, \"NuMI_FHC_BDT_DetectorVariations_OscillationAnalysis_v2.root\", 'nu_e', \n",
    "                                          bins, cut=selected_query, useBDT=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_variations = NuMIDetSysWeights.plot_variations('nu_e', bins, \"NuMI_FHC_BDT_DetectorVariations_OscillationAnalysis_v2.root\", \n",
    "                                                        ISRUN3, axis_label='True Neutrino Energy', plot=True, background_subtraction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute covariance (N=1 for each variation)\n",
    "\n",
    "detsys_cov = {}\n",
    "\n",
    "# index in weightsNuMIGeo are offset by -1\n",
    "\n",
    "for variation in detector_variations.keys(): \n",
    "    \n",
    "    if variation=='CV': \n",
    "        continue\n",
    "    \n",
    "    # calc covariance for each unisim \n",
    "    detsys_cov[variation] = calcCov('nu_e', bins, detector_variations['CV'], [detector_variations[variation]], 'Detector', \n",
    "                                    plot=False, save=False, pot='$2.0 x 10^{20}$ POT', isrun3=ISRUN3,\n",
    "                                   title=variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total covariance, correlation, & uncertainty \n",
    "\n",
    "cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "cor = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for variation in detsys_cov.keys(): \n",
    "    \n",
    "    for i in range(len(bins)-1): \n",
    "        for j in range(len(bins)-1):\n",
    "            \n",
    "            cov[i][j] = sum([detsys_cov[x]['cov'][i][j] for x in detsys_cov.keys()])\n",
    "            \n",
    "            if detector_variations['CV'][i]*detector_variations['CV'][j] != 0: \n",
    "                frac_cov[i][j] = cov[i][j]/(detector_variations['CV'][i]*detector_variations['CV'][j])\n",
    "\n",
    "            \n",
    "for i in range(len(bins)-1): \n",
    "    for j in range(len(bins)-1):\n",
    "        \n",
    "        if np.sqrt(cov[i][i])*np.sqrt(cov[j][j]) != 0: \n",
    "                cor[i][j] = cov[i][j] / (np.sqrt(cov[i][i])*np.sqrt(cov[j][j]))\n",
    "            \n",
    "detsys_dict = {\n",
    "    'cov' : cov, \n",
    "    'frac_cov' : frac_cov,\n",
    "    'cor' : cor,\n",
    "    'fractional_uncertainty' : np.sqrt(np.diag(frac_cov))\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['det_cov_frac'] = detsys_dict['frac_cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_frac_cov = [ [0]*(len(bins)-1) for x in range(len(bins)-1) ]\n",
    "\n",
    "for source in list(oscillation_dict.keys())[2:]: \n",
    "    tot_frac_cov = [ [x+y for x,y in zip(a,b)] for a,b in zip(tot_frac_cov, oscillation_dict[source])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['tot_cov_frac'] = tot_frac_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_dict['bins'] = oscillation_dict['bins'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this dictionary \n",
    "\n",
    "with open('mun/FHC_Projected_TrueNeutrinoEnergy_March2022_v2.json', 'w') as f:\n",
    "    json.dump(oscillation_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evt_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_dict['beamon_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## background subtracted event rate comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bincenters = 0.5*(np.array(x_ticks+[xhigh])[1:]+np.array(x_ticks+[xhigh])[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_err = []\n",
    "for x in range(len(bincenters)):\n",
    "    print(x)\n",
    "    x_err.append(round(abs((x_ticks+[3][:-1]+[3])[x+1]-(x_ticks+[3][:-1]+[3])[x])/2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bincenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 7))\n",
    "\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[2, 1])\n",
    "    \n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "    \n",
    "ax1.tick_params(axis = 'both', which = 'major', labelsize = 13)\n",
    "ax2.tick_params(axis = 'both', which = 'major', labelsize = 13)\n",
    "    \n",
    "ax2.yaxis.grid(linestyle=\"--\", color='black', alpha=0.2)\n",
    "ax2.xaxis.grid(linestyle=\"--\", color='black', alpha=0.2)\n",
    "\n",
    "ax1.set_xticks(bins[:-1])\n",
    "ax2.set_xticks(bins[:-1])\n",
    "\n",
    "ax1.set_xlim(bins[0], 3)\n",
    "ax2.set_xlim(bins[0], 3)\n",
    "\n",
    "n, b, p = ax1.hist(bincenters, bins, histtype='bar', weights=variations_dict['evt_rate'], color='orange', alpha=0.5,\n",
    "            label='GENIE: '+str(round(sum(evt_rate), 1)))\n",
    "\n",
    "ax1.errorbar(bincenters, variations_dict['beamon_counts'], yerr=np.sqrt(np.diag(tot_abs_cov)), xerr=x_err, \n",
    "             color=\"black\", fmt='o', markersize=3, label='DATA: '+str(int(sum(variations_dict['beamon_counts']))))\n",
    "\n",
    "\n",
    "ax1.step(list(bins)+[0], [0, n[0], n[-1], 0], \n",
    "         color='saddlebrown', linewidth=1, alpha=0.85)\n",
    "\n",
    "plt.xticks(x_ticks, fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "\n",
    "ax2.set_xlabel(\"Reconstructed Visible Energy [GeV]\", fontsize=15)\n",
    "\n",
    "plt.xlim(bins[0], 3)\n",
    "\n",
    "ax2.errorbar(bincenters, variations_dict['beamon_counts']/n, \n",
    "             yerr=get_ratio_err(variations_dict['beamon_counts'], n), xerr=x_err, color=\"black\", fmt='o')\n",
    "ax2.set_ylim(0, 2)\n",
    "ax2.set_ylabel(\"DATA / GENIE\", fontsize=15)\n",
    "ax2.axhline(1.0, color='black', lw=1, linestyle='--')\n",
    "\n",
    "\n",
    "if ISRUN3==False: \n",
    "    ax1.set_title(\"FHC Background-Subtracted Event Rate\", fontsize=15)\n",
    "    ax1.set_ylabel(\"$\\\\nu$ / 2.0 $\\\\times 10^{20}$ POT\", fontsize=15)\n",
    "else: \n",
    "    ax1.set_title(\"RHC Background-Subtracted Event Rate\", fontsize=15)\n",
    "    ax1.set_ylabel(\"$\\\\nu$ / 5.0 $\\\\times 10^{20}$ POT\", fontsize=15)\n",
    "    \n",
    "ax1.legend(frameon=False, fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(variations_dict['beamon_counts'])/sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
