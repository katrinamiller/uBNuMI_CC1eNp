{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, 'backend_functions')\n",
    "\n",
    "import selection_functions as sf\n",
    "\n",
    "import importlib\n",
    "\n",
    "import uproot\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "import awkward\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import ROOT\n",
    "from ROOT import TH1F, TH2F, TDirectory, TH1D\n",
    "\n",
    "import top \n",
    "from top import *\n",
    "\n",
    "importlib.reload(sf)\n",
    "from selection_functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3 = True\n",
    "NUE_INTRINSIC = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_path = parameters(ISRUN3)['plots_path']\n",
    "plots_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = \"nuselection\"\n",
    "tree = \"NeutrinoSelectionFilter\"\n",
    "\n",
    "DATA = \"\"\n",
    "EXT = \"\"\n",
    "OVRLY  = \"\"\n",
    "DRT = \"\"\n",
    "NUE = \"\"\n",
    "\n",
    "path = parameters(ISRUN3)['cv_ntuple_path']\n",
    "print('path = ', path)\n",
    "\n",
    "if not ISRUN3: \n",
    "    \n",
    "    # Run 1 FHC \n",
    "    OVRLY = 'neutrinoselection_filt_run1_overlay_v7'\n",
    "    EXT = 'neutrinoselection_filt_run1_beamoff_v5'\n",
    "    DATA = 'neutrinoselection_filt_run1_beamon_beamgood_v5'\n",
    "    DRT = 'prodgenie_numi_uboone_overlay_dirt_fhc_mcc9_run1_v28_all_snapshot'\n",
    "    \n",
    "    if NUE_INTRINSIC: \n",
    "        NUE = 'neutrinoselection_filt_run1_overlay_intrinsic_v7' \n",
    "\n",
    "\n",
    "else: \n",
    "    \n",
    "    # Run 3 RHC\n",
    "    OVRLY = 'neutrinoselection_filt_run3b_overlay_v7'\n",
    "    DATA = 'neutrinoselection_filt_run3b_beamon_beamgood_v5'\n",
    "    EXT = 'neutrinoselection_filt_run3b_beamoff_v5'\n",
    "    DRT = 'neutrinoselection_filt_run3b_dirt_overlay_v6'\n",
    "    \n",
    "    if NUE_INTRINSIC: \n",
    "        NUE = 'neutrinoselection_filt_run3b_overlay_intrinsic_v7'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "overlay = uproot.open(path+OVRLY+\".root\")[fold][tree]\n",
    "data = uproot.open(path+DATA+\".root\")[fold][tree]\n",
    "ext = uproot.open(path+EXT+\".root\")[fold][tree]\n",
    "dirt = uproot.open(path+DRT+\".root\")[fold][tree]  \n",
    "\n",
    "uproot_v = [overlay,data,ext,dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue = uproot.open(path+NUE+\".root\")[fold][tree]\n",
    "    uproot_v.append(nue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    \"trk_score_v\", \n",
    "    \"shr_tkfit_dedx_Y\", \n",
    "    \"n_tracks_contained\", \n",
    "    \"NeutrinoEnergy2\",\n",
    "    \"run\",\"sub\",\"evt\",\n",
    "    \"reco_nu_vtx_sce_x\",\"reco_nu_vtx_sce_y\",\"reco_nu_vtx_sce_z\",\n",
    "    \"shrsubclusters0\",\"shrsubclusters1\",\"shrsubclusters2\",\n",
    "    \"trkshrhitdist2\",\n",
    "    \"n_showers_contained\", \n",
    "    \"shr_phi\", \"trk_phi\", \"trk_theta\",\n",
    "    \"shr_score\", \n",
    "    \"trk_energy\", \n",
    "    \"tksh_distance\", \"tksh_angle\",\n",
    "    \"shr_energy_tot_cali\", \"shr_energy_cali\", \n",
    "    \"nslice\", \n",
    "    \"contained_fraction\",\n",
    "    \"shrmoliereavg\", \"shr_px\", \"shr_py\", \"shr_pz\",\"swtrig_pre\"\n",
    "]\n",
    "\n",
    "# MC only variables\n",
    "mc_var = [\"nu_pdg\", \"shr_theta\", \"true_e_visible\", \"ccnc\", \"interaction\", \n",
    "          \"nproton\", \"nu_e\", \"npi0\", \"npion\",\n",
    "          \"true_nu_vtx_x\", \"true_nu_vtx_y\" , \"true_nu_vtx_z\", \n",
    "          \"weightTune\", \"weightSpline\", \"weightSplineTimesTune\", \n",
    "          \"true_nu_px\", \"true_nu_py\", \"true_nu_pz\", \n",
    "          \"elec_e\", \"proton_e\", \"mc_px\", \"mc_py\", \"mc_pz\", \"elec_px\", \"elec_py\", \"elec_pz\", \n",
    "           \"ppfx_cv\", \"mc_pdg\"]#, \"opening_angle\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERLAY \n",
    "overlay = overlay.pandas.df(variables + mc_var, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRT \n",
    "dirt = dirt.pandas.df(variables + mc_var, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTRINSIC \n",
    "if NUE_INTRINSIC: \n",
    "    nue = nue.pandas.df(variables + mc_var, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEAM ON \n",
    "data = data.pandas.df(variables, flatten=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEAM OFF\n",
    "ext = ext.pandas.df(variables, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equalize the columns\n",
    "\n",
    "for var in mc_var: \n",
    "    data[var] = np.nan\n",
    "    ext[var] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is dirt bool\n",
    "\n",
    "overlay['isDirt'] = False\n",
    "dirt['isDirt'] = True\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue['isDirt'] = False\n",
    "    \n",
    "data['isDirt'] = np.nan\n",
    "ext['isDirt'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to get the LLR-PID value for the \"track candidate\" (proton for nue selection, muon for numu)\n",
    "# code from Giuseppe!\n",
    "#LLR-PID : log likelihood ratio particle ID \n",
    "\n",
    "df_v = [overlay,data,ext,dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    df_v.append(nue)\n",
    "    \n",
    "for i,df in enumerate(df_v):\n",
    "    up = uproot_v[i]\n",
    "    trk_llr_pid_v = up.array('trk_llr_pid_score_v')\n",
    "    trk_id = up.array('trk_id')-1 # I think we need this -1 to get the right result\n",
    "    trk_llr_pid_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_llr_pid_v,trk_id)])\n",
    "    df['trkpid'] = trk_llr_pid_v_sel\n",
    "    df['subcluster'] = df['shrsubclusters0'] + df['shrsubclusters1'] + df['shrsubclusters2']\n",
    "\n",
    "    df['NeutrinoEnergy2_GeV'] = df['NeutrinoEnergy2']/1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = overlay.query('swtrig_pre==1')\n",
    "dirt = dirt.query('swtrig_pre==1')\n",
    "nue = nue.query('swtrig_pre==1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_df = [overlay, dirt]\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    mc_df.append(nue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(mc_df):\n",
    "    \n",
    "    # is signal bool \n",
    "    df['is_signal'] = np.where((df.swtrig_pre == 1) \n",
    "                             & (df.nu_pdg==12) & (df.ccnc==0) & (df.nproton>0) & (df.npion==0) & (df.npi0==0)\n",
    "                             & (10 <= df.true_nu_vtx_x) & (df.true_nu_vtx_x <= 246)\n",
    "                             & (-106 <= df.true_nu_vtx_y) & (df.true_nu_vtx_y <= 106)\n",
    "                             & (10 <= df.true_nu_vtx_z) & (df.true_nu_vtx_z <= 1026), True, False)\n",
    "    \n",
    "    # Add truth level theta & phi angles (detector & beam coordinates)\n",
    "    df = addAngles(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframes equal # of columns \n",
    "\n",
    "data['is_signal'] = np.nan\n",
    "ext['is_signal'] = False\n",
    "\n",
    "nan_var = ['thdet', 'phidet', 'true_nu_px_beam', 'true_nu_py_beam', 'true_nu_pz_beam', \n",
    "           'thbeam', 'phibeam']\n",
    "\n",
    "for var in mc_var+nan_var: \n",
    "    data[var] = np.nan\n",
    "    ext[var] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some checks \n",
    "print(len(nue.query('is_signal==True'))==len(nue.query(signal)))\n",
    "print(len(nue.query('is_signal==False'))==len(nue.query(not_signal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean bad weights & values \n",
    "\n",
    "for i,df in enumerate(mc_df):\n",
    "     \n",
    "    df.loc[ df['weightSplineTimesTune'] <= 0, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] == np.inf, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] > 100, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightSplineTimesTune']) == True, 'weightSplineTimesTune' ] = 1.\n",
    "    \n",
    "    df.loc[ df['weightTune'] <= 0, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] == np.inf, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] > 100, 'weightTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightTune']) == True, 'weightTune' ] = 1.  \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POT normalization\n",
    "\n",
    "beamon_pot = parameters(ISRUN3)['beamon_pot'] \n",
    "\n",
    "overlay = pot_scale(overlay, 'overlay', ISRUN3)\n",
    "dirt = pot_scale(dirt, 'dirt', ISRUN3)\n",
    "ext = pot_scale(ext, 'ext', ISRUN3)\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue = pot_scale(nue, 'intrinsic', ISRUN3)\n",
    "\n",
    "data['pot_scale'] = [1 for x in range(len(data))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined genie * POT weight * flux weight , ext gets POT weight only \n",
    "################################################################\n",
    "# totweight_data scales to BEAMON\n",
    "\n",
    "# tuned\n",
    "overlay['totweight_data'] = overlay['pot_scale']*overlay['ppfx_cv']*overlay['weightSplineTimesTune']\n",
    "dirt['totweight_data'] = dirt['pot_scale']*dirt['ppfx_cv']*dirt['weightSplineTimesTune']\n",
    "\n",
    "if NUE_INTRINSIC: \n",
    "    nue['totweight_data'] = nue['pot_scale']*nue['ppfx_cv']*nue['weightSplineTimesTune']\n",
    "\n",
    "ext['totweight_data'] = np.nan\n",
    "data['totweight_data'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUE_INTRINSIC: \n",
    "    \n",
    "    print(\"# of nueCC in AV in overlay sample = \"+str(len(overlay.query(nueCC_query))))\n",
    "    len1 = len(overlay)\n",
    "    \n",
    "    idx = overlay.query(nueCC_query).index\n",
    "    overlay.drop(idx, inplace=True)\n",
    "    len2 = len(overlay) \n",
    "    print(\"# of nueCC in AV dropped in overlay = \"+str(len1-len2))\n",
    "    \n",
    "    # then add in nue_intrinsic \n",
    "    overlay = pd.concat([overlay,nue], ignore_index=True)\n",
    "\n",
    "    # from here on out everything else should be the same. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine overlay + dirt as MC \n",
    "mc = pd.concat([overlay.query('swtrig_pre==1'),dirt.query('swtrig_pre==1')], ignore_index=True, sort=True)\n",
    "\n",
    "# separate by in/out FV & cosmic\n",
    "infv = mc.query(in_fv_query)\n",
    "outfv = mc.query(out_fv_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check that everything is accounted for \n",
    "print(len(mc)==len(infv)+len(outfv))#+len(cosmic))\n",
    "\n",
    "if not (len(mc)==len(infv)+len(outfv)): #+len(cosmic)): \n",
    "    \n",
    "    d = len(mc) - (len(infv)+len(outfv))#+len(cosmic))\n",
    "    print(d)\n",
    "    \n",
    "    m = pd.concat([infv, outfv]) #pd.concat([infv, cosmic, outfv])\n",
    "    diff = np.setdiff1d(list(mc.index),list(m.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_signal_weighted = np.nansum(mc.query('is_signal==True')['totweight_data'])\n",
    "print('total signal events = '+ str(tot_signal_weighted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 main categories: \n",
    "\n",
    "# infv - overlay & dirt events with truth vtx in FV \n",
    "# outfv - overlay & dirt events with truth vtx in FV that are classified as neutrinos\n",
    "# cosmic - overlay & dirt events with true vtx in FV that get misclassified as cosmic \n",
    "# ext - beam OFF data\n",
    "# data - beam ON data \n",
    "\n",
    "\n",
    "datasets = {\n",
    "    \"infv\": infv, \n",
    "    \"outfv\": outfv, \n",
    "    \"ext\": ext, \n",
    "    \"data\": data\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't clean datasets for the linear selection, only for BDT \n",
    "# apply preselection \n",
    "\n",
    "PRE_QUERY = 'swtrig_pre==1 and nslice==1' # \n",
    "PRE_QUERY += ' and ' + reco_in_fv_query \n",
    "PRE_QUERY += ' and contained_fraction>0.9' \n",
    "\n",
    "PRE_QUERY += ' and n_showers_contained == 1'\n",
    "PRE_QUERY += ' and n_tracks_contained>0'\n",
    "PRE_QUERY += ' and trk_energy>0.04' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new optimized selection by Kaushal \n",
    "\n",
    "SEL_QUERY = PRE_QUERY\n",
    "\n",
    "SEL_QUERY += ' and shr_score<0.125'\n",
    "SEL_QUERY += ' and shrmoliereavg<8'\n",
    "SEL_QUERY += ' and trkpid<0'\n",
    "\n",
    "SEL_QUERY += ' and shr_tkfit_dedx_Y<4'\n",
    "\n",
    "if not ISRUN3: \n",
    "    SEL_QUERY += ' and tksh_distance<5'\n",
    "\n",
    "else: \n",
    "    SEL_QUERY += ' and tksh_distance<4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDT Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bdt model \n",
    "split = 1\n",
    "bdt_model = xgb.Booster({'nthread': 4})\n",
    "bdt_model.load_model(parameters(ISRUN3)['bdt_model'])\n",
    "    \n",
    "datasets_bdt = {}\n",
    "\n",
    "for i in range(len(datasets)): \n",
    "\n",
    "    df = list(datasets.values())[i].copy()\n",
    "    df = df.query(BDT_LOOSE_CUTS)\n",
    "\n",
    "    # clean datasets \n",
    "    for column in training_parameters:\n",
    "        df.loc[(df[column] < -1.0e37) | (df[column] > 1.0e37), column] = np.nan\n",
    "\n",
    "    # create testing dmatrix \n",
    "    df_test = xgb.DMatrix(data=df[training_parameters])\n",
    "\n",
    "    # apply the bdt selection\n",
    "    preds = bdt_model.predict(df_test)\n",
    "\n",
    "    # add columns for plotting \n",
    "    df['BDT_score'] = preds\n",
    "\n",
    "    datasets_bdt[list(datasets.keys())[i]] = df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_score_cut = parameters(ISRUN3)['bdt_score_cut']\n",
    "    \n",
    "print(\"BDT SCORE THRESHOLD = \"+str(bdt_score_cut))\n",
    "\n",
    "selected_query = BDT_LOOSE_CUTS + ' and BDT_score>'+str(bdt_score_cut)\n",
    "selected_signal_query = selected_query + ' and is_signal==True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GENERATED SIGNAL (DATA NORMALIZED) = \"+str(sum(generated_signal(ISRUN3, 'nu_e', 1, 0, 20, weight='totweight_data')[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BDT score output \n",
    "\n",
    "x = plot_mc('BDT_score', [round(x*0.05,2) for x in range(21)], 0, 1, BDT_LOOSE_CUTS, \n",
    "            datasets_bdt, ISRUN3, x_label=\"BDT Probability Score\", \n",
    "            norm='data', y_label='$\\\\nu$ / 2$\\\\times10^{20}$ POT', save=False)#, ymax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_labels = {\n",
    "    '0': 'CCQE', \n",
    "    '1': 'CC Res', \n",
    "    '2': 'CC DIS', \n",
    "    '3': 'CC Coh', \n",
    "    '10': 'CC MEC' \n",
    "}\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "xvar = 'nu_e'\n",
    "\n",
    "ccqe = datasets_bdt['infv'].query(selected_signal_query+' and interaction==0')\n",
    "ccres = datasets_bdt['infv'].query(selected_signal_query+' and interaction==1')\n",
    "ccdis = datasets_bdt['infv'].query(selected_signal_query+' and interaction==2')\n",
    "cccoh = datasets_bdt['infv'].query(selected_signal_query+' and interaction==3')\n",
    "ccmec = datasets_bdt['infv'].query(selected_signal_query+' and interaction==10')\n",
    "\n",
    "int_weights = [ccqe['totweight_data'], \n",
    "                 ccres['totweight_data'], \n",
    "                 ccdis['totweight_data'], \n",
    "                 cccoh['totweight_data'], \n",
    "                 ccmec['totweight_data']]\n",
    "\n",
    "int_counts = []\n",
    "for l in int_weights: \n",
    "    int_counts.append( np.nansum(l) )\n",
    "    \n",
    "tot_count = sum(int_counts)\n",
    "int_counts = [str(round((x/tot_count)*100, 1)) for x in int_counts]\n",
    "\n",
    "# signal broken down by interactions after the selection \n",
    "n, b, p = plt.hist([ccqe[xvar], ccres[xvar], ccdis[xvar], cccoh[xvar], ccmec[xvar]],\n",
    "          [x*0.25 for x in range(25)], \n",
    "          weights=int_weights,\n",
    "          label=[int_labels['0']+' ('+int_counts[0]+'%)', \n",
    "                 int_labels['1']+' ('+int_counts[1]+'%)', \n",
    "                 int_labels['2']+' ('+int_counts[2]+'%)', \n",
    "                 int_labels['3']+' ('+int_counts[3]+'%)', \n",
    "                 int_labels['10']+' ('+int_counts[4]+'%)'],\n",
    "          color=[ 'indianred','yellowgreen','moccasin',  'cornflowerblue', 'sandybrown'], \n",
    "          stacked=True)\n",
    "\n",
    "bincenters = 0.5*(b[1:]+b[:-1])\n",
    "\n",
    "y_err = mc_error(xvar, b, b[0], b[-1], [datasets_bdt['infv'].query(selected_signal_query)])\n",
    "#y_err = [x*y for x, y in zip(n[-1], signal_percent_err)]\n",
    "\n",
    "plt.hist(bincenters, b, color='black', linewidth=0.75, weights=n[-1], histtype='step')\n",
    "plt.errorbar(bincenters, n[-1], yerr=y_err,\n",
    "             color='black', linewidth=.75, fmt='none')#, alpha=.7)\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.xlabel(\"True Neutrino Energy [GeV]\", fontsize=15)\n",
    "plt.ylabel('$\\\\nu$ / $2\\\\times10^{20}$ POT', fontsize=15)\n",
    "plt.title('FHC RUN 1 Selected Event Rate', fontsize=16)\n",
    "\n",
    "plt.legend(frameon=False, fontsize=14)\n",
    "\n",
    "plt.xlim(0, 6)\n",
    "\n",
    "#plt.savefig(parameters(ISRUN3)['plots_path']+\"FHCSelectedInteractions.pdf\", transparent=True, bbox_inches='tight') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'swtrig_pre==1 and nslice==1 and '+reco_in_fv_query+' and contained_fraction>0.9' \n",
    "\n",
    "#'swtrig_pre==1 and nslice==1 and '+reco_in_fv_query+' and contained_fraction>0.9' \n",
    "#BDT_LOOSE_CUTS+' and BDT_score>'+str(bdt_score_cut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useBDT = False\n",
    "\n",
    "if useBDT: \n",
    "    datasets_dict = datasets_bdt\n",
    "else: \n",
    "    datasets_dict = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tot_sig_sel = np.nansum(datasets_dict['infv'].query(q+' and is_signal==True').totweight_data)\n",
    "tot_sel = np.nansum(datasets_dict['infv'].query(q).totweight_data) + np.nansum(datasets_dict['outfv'].query(q).totweight_data) + np.nansum(datasets_dict['ext'].query(q).pot_scale)\n",
    "tot_sig = sum(generated_signal(ISRUN3, 'nu_e', 1, 0, 100)[0])\n",
    "\n",
    "print(\"Purity = \", tot_sig_sel/tot_sel)\n",
    "print(\"Efficiency = \", tot_sig_sel/tot_sig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BDT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "save_bdt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TRAIN ON A SUBSET OF THE DISTRIBUTION\n",
    "train_query = BDT_LOOSE_CUTS + ' and -0.9<tksh_angle<0.9 and shr_energy_tot_cali>0.07'\n",
    "test_query = BDT_LOOSE_CUTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset distributions \n",
    "\n",
    "x = plot_mc('tksh_angle', [-1, -.9, -.8, -.7, -.6, -.5, -.4, -.3, -.2, -.1] + list(round(x*0.1,2) for x in range(11)), \n",
    "        -1, 1, test_query, datasets, ISRUN3, norm='data')\n",
    "        #save=False, save_label='RHCRUN3_loosecuts_mconly', log=False, x_label='$cos(\\\\theta_{ep})$',\n",
    "        #text='FHC RUN 1', xtext=0.75, ytext=70)\n",
    "\n",
    "x = plot_mc('shr_energy_tot_cali', list(x*0.01 for x in range(21)), \n",
    "        0, 0.2, test_query, datasets, ISRUN3, norm='data', ymax=65)\n",
    "        #save=False, save_label='RHCRUN3_loosecuts_mconly', log=False, x_label='Total Calibrated Shower Energy [GeV]', \n",
    "        #text='FHC RUN 1', xtext=0.19, ytext=35)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = addRelevantColumns(datasets) # includes both MC and EXT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_signal_generated = sum(generated_signal(ISRUN3, 'nu_e', 1, 0, 100)[0])\n",
    "print(total_signal_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_signal_generated_intrinsic = sum(generated_signal(ISRUN3, 'nu_e', 1, 0, 100, weight='totweight_intrinsic')[0])\n",
    "print(total_signal_generated_intrinsic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre BDT performance as a function of test sample size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficiency of total sample \n",
    "e_total = sum(df_pre.query(test_query+ ' and is_signal==True')['weight']) / total_signal_generated \n",
    "\n",
    "# efficiency error - binomial (use the intrinsic event count)\n",
    "e_total_err = np.sqrt(e_total * (1-e_total) / total_signal_generated_intrinsic) * 100 \n",
    "\n",
    "# purity of total sample \n",
    "p_total = sum(df_pre.query(test_query+ ' and is_signal==True')['weight']) / sum(df_pre.query(test_query)['weight'])\n",
    "\n",
    "# purity error - poissonian \n",
    "p_total_err = (p_total * np.sqrt( sum(df_pre.query(test_query+' and is_signal==True').weight**2)/(sum(df_pre.query(test_query+' and is_signal==True').weight)**2) + sum(df_pre.query(test_query).weight**2)/(sum(df_pre.query(test_query).weight)**2) )) * 100\n",
    "\n",
    "e_total = e_total*100\n",
    "p_total = p_total*100\n",
    "\n",
    "\n",
    "# store efficiency & purity values\n",
    "e = []\n",
    "e_err = []\n",
    "\n",
    "p = []\n",
    "p_err = []\n",
    "\n",
    "xpoints = [round(i*0.05, 2) for i in range(20)]\n",
    "\n",
    "for test_size in xpoints[1:]: \n",
    "\n",
    "    df_pre_train, df_pre_test = train_test_split(df_pre, test_size=test_size, \n",
    "                                             random_state=17, stratify=df_pre['is_signal'])\n",
    "    \n",
    "    total_signal_selected = sum(df_pre_test.query(test_query+ ' and is_signal==True')['weight'])\n",
    "    total_selected = sum(df_pre_test.query(test_query)['weight'])\n",
    "    \n",
    "    eff = total_signal_selected / (total_signal_generated*test_size) \n",
    "    pur = total_signal_selected / total_selected\n",
    "    \n",
    "    e.append(eff*100)\n",
    "    p.append(pur*100)\n",
    "    \n",
    "    e_err.append( np.sqrt( (eff*(1-eff)) / (total_signal_generated_intrinsic*test_size) ) * 100) # use the intrinsic event count\n",
    "    p_err.append( (pur * np.sqrt( sum(df_pre_test.query(test_query+' and is_signal==True').weight**2)/sum(df_pre_test.query(test_query+' and is_signal==True').weight)**2 + sum(df_pre_test.query(test_query).weight**2)/sum(df_pre_test.query(test_query).weight)**2) )*100 ) \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7.5, 5))\n",
    "\n",
    "plt.errorbar(xpoints[1:], p, yerr=p_err, label=\"Purity (Test Sample)\", color='firebrick', marker='.')\n",
    "plt.axhline(y=p_total, label=\"Purity (Full)\", color='firebrick', linewidth=1, linestyle=\"--\")\n",
    "\n",
    "plt.fill_between(xpoints, p_total-p_total_err, p_total+p_total_err, color='firebrick', alpha=0.1)\n",
    "\n",
    "\n",
    "plt.ylim(15, 30)\n",
    "plt.xlim(xpoints[0], xpoints[-1])\n",
    "plt.grid(linestyle=':')\n",
    "plt.legend(fontsize=13, ncol=2)\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.xlabel('Test Sample Size (Relative to Full Sample)', fontsize=14)\n",
    "plt.ylabel('Performance (%)', fontsize=14)\n",
    "\n",
    "if ISRUN3: \n",
    "    plt.title('RHC Run 3 Pre-BDT Purity', fontsize=15)\n",
    "    plt.savefig(parameters(ISRUN3)['plots_path']+\"RHCStratPerf_Purity.pdf\", transparent=True, bbox_inches='tight') \n",
    "else: \n",
    "    plt.title('FHC Run 1 Pre-BDT Efficiency', fontsize=15)\n",
    "    plt.savefig(parameters(ISRUN3)['plots_path']+\"FHCStratPerf_Purity.pdf\", transparent=True, bbox_inches='tight') \n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7.5, 5))\n",
    "\n",
    "plt.errorbar(xpoints[1:], e, yerr=e_err, label=\"Efficiency (Test Sample)\", color='seagreen', marker='.')\n",
    "plt.axhline(y=e_total, label=\"Efficiency (Full)\", color='seagreen', linewidth=1, linestyle=\"--\")\n",
    "\n",
    "plt.fill_between(xpoints, e_total-e_total_err, e_total+e_total_err, color='seagreen', alpha=0.1)\n",
    "\n",
    "\n",
    "plt.ylim(15, 30)\n",
    "plt.xlim(xpoints[0], xpoints[-1])\n",
    "plt.grid(linestyle=':')\n",
    "plt.legend(fontsize=13, ncol=2)\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.xlabel('Test Sample Size (Relative to Full Sample)', fontsize=14)\n",
    "plt.ylabel('Performance (%)', fontsize=14)\n",
    "\n",
    "if ISRUN3: \n",
    "    plt.title('RHC Run 3 Pre-BDT Efficiency', fontsize=15)\n",
    "    plt.savefig(parameters(ISRUN3)['plots_path']+\"RHCStratPerf_Efficiency.pdf\", transparent=True, bbox_inches='tight') \n",
    "else: \n",
    "    plt.title('FHC Run 1 Pre-BDT Efficiency', fontsize=15)\n",
    "    plt.savefig(parameters(ISRUN3)['plots_path']+\"FHCStratPerf_Efficiency.pdf\", transparent=True, bbox_inches='tight') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters(ISRUN3)['plots_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC & AUCPR Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_size = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the boosting rounds \n",
    "\n",
    "\n",
    "\n",
    "# Split arrays or matrices into random train and test subsets\n",
    "df_pre_train, df_pre_test = train_test_split(df_pre, test_size=final_test_size, \n",
    "                                             random_state=17, stratify=df_pre['is_signal'])\n",
    "\n",
    "bdt_metrics(df_pre_train, df_pre_test, train_query, test_query, \n",
    "            training_parameters, ISRUN3, save=False, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new BDT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-determined boosting round numbers\n",
    "\n",
    "if not ISRUN3: # FHC\n",
    "    lc_rounds = 200\n",
    "    \n",
    "else: # RHC\n",
    "    lc_rounds = 150\n",
    "\n",
    "#test train split \n",
    "split = final_test_size\n",
    "print('split', split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISRUN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN: \n",
    "    \n",
    "    # BDT training - backend function cleans the dataset first \n",
    "    bdt_lc = main_BDT(datasets, train_query, test_query, lc_rounds, training_parameters, ISRUN3, test_size=split)\n",
    "\n",
    "    results_df = bdt_lc['bdt_results_df']\n",
    "    bdt_model = bdt_lc['bdt_model']\n",
    "    train_df = bdt_lc['df_pre_train']\n",
    "    test_df = bdt_lc['df_pre_test']\n",
    "    \n",
    "    # split events into different categories \n",
    "    datasets_bdt = split_events(results_df)\n",
    "\n",
    "# note: all of the plots following be made with half the POT this way (test/train split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if save_bdt: \n",
    "#    bdt_model.save_model('BDT_models/test_rhc_may2022.model')\n",
    "#    print('saving BDT...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sf)\n",
    "from selection_functions import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN: \n",
    "\n",
    "    # performance of the linear selection for comparison\n",
    "    gen_split = total_signal_generated*split\n",
    "    eff_box = np.nansum(test_df.query(SEL_QUERY+' and is_signal==1').weight)/gen_split * 100\n",
    "\n",
    "    tot_sel = np.nansum(test_df.query(SEL_QUERY)['weight']) \n",
    "    pur_box = np.nansum(test_df.query(SEL_QUERY+' and is_signal==1').weight) / tot_sel * 100\n",
    "\n",
    "    results_box = [pur_box, eff_box]\n",
    "\n",
    "    #  stat errors on the linear performance \n",
    "    e = eff_box/100\n",
    "\n",
    "    eff_err = math.sqrt( (e*(1-e)) / total_signal_generated_intrinsic*split ) * 100\n",
    "    pur_err = pur_box * np.sqrt( sum(test_df.query(SEL_QUERY+' and is_signal==1').weight**2)/sum(test_df.query(SEL_QUERY+' and is_signal==1').weight)**2 + sum(test_df.query(SEL_QUERY).weight**2)/sum(test_df.query(SEL_QUERY).weight)**2 )\n",
    "\n",
    "    results_box_err = [pur_err, eff_err]\n",
    "\n",
    "    # x values\n",
    "    x = np.arange(0, 0.8, 0.025)\n",
    "\n",
    "    # returns purity, purErr, eff, effErr\n",
    "    perf_dict = bdt_pe(results_df, x, \n",
    "                  total_signal_generated, total_signal_generated_intrinsic, split)\n",
    "    pur, pur_err, eff, eff_err = perf_dict['purity'], perf_dict['purErr'], perf_dict['eff'], perf_dict['effErr']\n",
    "    \n",
    "    bdt_box_plot([pur, pur_err, eff, eff_err], x, ISRUN3, save=False, results_box=results_box, results_box_err=results_box_err)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_svb_plot(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = plot_mc('BDT_score', [round(x*0.05,2) for x in range(21)], 0, 1, '', \n",
    "            datasets_bdt, ISRUN3, x_label=\"BDT Probability Score\", #ymax=100, \n",
    "            norm='data', save=False, bdt_scale=None)\n",
    "\n",
    "# [round(x*0.05,2) for x in range(21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = 2\n",
    "repeats = 20\n",
    "cv = RepeatedStratifiedKFold(n_splits=splits, n_repeats=repeats, random_state=36851234)\n",
    "\n",
    "if ISRUN3: \n",
    "    bdt_score_arr = np.arange(0, 0.75, 0.025)\n",
    "    \n",
    "else: \n",
    "    bdt_score_arr = np.arange(0, 0.8, 0.025)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crossval = addRelevantColumns(datasets)\n",
    "\n",
    "scale_weight = len(df_crossval.query(train_query + ' and is_signal == False')) / len(df_crossval.query(train_query + ' and is_signal == True'))\n",
    "print(\"scale pos weight (ratio of negative to positive) = \"+str(scale_weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model params\n",
    "params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'booster': 'gbtree',\n",
    "        'eta': 0.02,\n",
    "        'tree_method': 'exact',\n",
    "        'max_depth': 3,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 1,\n",
    "        'silent': 1,\n",
    "        'min_child_weight': 1,\n",
    "        'seed': 2002,\n",
    "        'gamma': 1,\n",
    "        'max_delta_step': 0,\n",
    "        'scale_pos_weight': scale_weight,\n",
    "        'eval_metric': ['error', 'auc', 'aucpr']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sf)\n",
    "from selection_functions import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_purity = []\n",
    "final_efficiency=[]\n",
    "\n",
    "fp_err = []\n",
    "fe_err = []\n",
    "\n",
    "box_pur = []\n",
    "box_eff = []\n",
    "\n",
    "boxp_err = []\n",
    "boxe_err = []\n",
    "\n",
    "print('Note: must use 50/50 split for cv.split function!')\n",
    "\n",
    "for train_index, test_index in cv.split(df_crossval, df_crossval['is_signal']):\n",
    "    \n",
    "    train, test = df_crossval.iloc[train_index], df_crossval.iloc[test_index]    \n",
    "    \n",
    "    bdt_cv_lc = bdt_raw_results(train, test, train_query, test_query, training_parameters, params, lc_rounds)\n",
    "    \n",
    "    # saves purity, efficiency and respective errors on current test sample for loose cuts BDT\n",
    "    perf = bdt_pe(bdt_cv_lc[0], bdt_score_arr, total_signal_generated, total_signal_generated_intrinsic, split)\n",
    "    pur, pur_err, eff, eff_err = perf['purity'], perf['purErr'], perf['eff'], perf['effErr']\n",
    "    \n",
    "    final_purity.append(pur)\n",
    "    final_efficiency.append(eff)\n",
    "    \n",
    "    fp_err.append(pur_err)\n",
    "    fe_err.append(eff_err)\n",
    "\n",
    "    # LINEAR SELECTION PERFORMANCE \n",
    "    \n",
    "    sig_sel = sum(test.query(SEL_QUERY+' and is_signal==1').weight)\n",
    "    tot_sel = sum(test.query(SEL_QUERY).weight)\n",
    "    \n",
    "    tot_sig = total_signal_generated*0.5\n",
    "    tot_sig_intrinsic = total_signal_generated_intrinsic*0.5\n",
    "    \n",
    "    p = sig_sel / tot_sel\n",
    "    e = sig_sel / tot_sig\n",
    "    \n",
    "    box_pur.append(p * 100)\n",
    "    box_eff.append(e * 100)\n",
    "        \n",
    "    boxp_err.append( p * np.sqrt( sum(test.query(SEL_QUERY+' and is_signal==True').weight**2)/sum(test.query(SEL_QUERY+' and is_signal==True').weight)**2 + sum(test.query(SEL_QUERY).weight**2)/sum(test.query(SEL_QUERY).weight)**2 ) * 100)\n",
    "    boxe_err.append(math.sqrt( (e * (1-e)) / tot_sig_intrinsic) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averages results column-wise, which is equivalent to averaging results over the same BDT_score cut\n",
    "\n",
    "results_bdt = [np.mean(final_purity, axis=0), np.mean(fp_err, axis=0), \n",
    "               np.mean(final_efficiency, axis=0), np.mean(fe_err, axis=0)]\n",
    "\n",
    "# linear box cut selection is a normal average over each distinct test sample\n",
    "results_box = [np.mean(box_pur), np.mean(box_eff)]\n",
    "results_box_err = [np.mean(boxp_err), np.mean(boxe_err)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bdt_score_arr)): \n",
    "    if bdt_score_arr[i]>=0.45: \n",
    "        print(\"BDT score > \"+str(round(bdt_score_arr[i], 3)))\n",
    "        print(\"efficiency = \"+str(round(results_bdt[2][i], 1)))\n",
    "        print(\"purity = \"+str(round(results_bdt[0][i], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ISRUN3: \n",
    "    title = \"RHC Run 3 Cross Validation\"\n",
    "    save_label = \"RHCRUN3_crossvalidation\"\n",
    "else: \n",
    "    title = \"FHC Run 1 Cross Validation\"\n",
    "    save_label = \"FHCRUN1_crossvalidation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_box_plot(results_bdt, bdt_score_arr, ISRUN3, results_box=results_box, results_box_err=results_box_err, \n",
    "           save=True, save_label=save_label, title=title) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_score_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
